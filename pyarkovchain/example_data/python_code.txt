import os
import sys
import random
import math
import time

class BadInputError(Exception):
    pass

class Player():

    def __init__(self, name):
        self.id = None
        self.name = name
        self.type = 'Human'
        self.hand = Hand()
        self.legalCards = []
        self.wildCards = []
        self.valueChangeCards = []
        self.zeroCards = []
        self.canSkip = False
        self.canReverse = False
        self.canDrawTwo = False
        self.canDrawFour = False
        self.canValueChange = False
        self.drew = False
        self.scrollMax = 0
        self.points = 0
        self.forceDraw = 0

    def addCard(self, card):
        self.drew = True
        if self.forceDraw > 0:
            self.forceDraw -= 1
            self.drew = False
        self.hand.addCard(card)

    def beginTurn(self):
        self.drew = False

    def didDraw(self):
        return self.drew

    def getLegalCards(self, color, value, zeroChange=False):
        self.canSkip = False
        self.canReverse = False
        self.canDrawTwo = False
        self.canDrawFour = False
        self.canValueChange = False
        self.canZeroChange = False
        self.legalCards = []
        self.wildCards = []
        self.valueChangeCards = []
        self.zeroCards = []
        plusFours = []
        for card in self.hand:
            if card.isWild():
                if card.getValue() == '+4':
                    plusFours.append(card)
                else:
                    self.wildCards.append(card)
            elif zeroChange and card.isZero():
                self.canZero = True
                self.zeroCards.append(card)
            elif card.getColor() == color or card.getValue() == value:
                if card.getColor() != color:
                    self.canValueChange = True
                    self.valueChangeCards.append(card)
                if card.getValue() == "+2":
                    self.canDrawTwo = True
                elif card.getValue() == 'R':
                    self.canReverse = True
                elif card.getValue() == 'X':
                    self.canSkip = True
                self.legalCards.append(card)
        if len(self.legalCards) == 0 and len(plusFours) > 0:
            self.canDrawFour = True
            self.wildCards += plusFours

    def getValidCards(self):
        return self.legalCards

    def getAllValidCards(self):
        return self.legalCards + self.wildCards + self.zeroCards

    def hasLegalCard(self):
        return len(self.legalCards) > 0

    def addPoints(self, amount):
        if (self.points + amount) <= 999999999999999999999:
            self.points += amount

    def removeCard(self, index):
        return self.hand.removeCard(index)

    def assignID(self, identity):
        self.id = identity

    def getName(self):
        return self.name

    def getID(self):
        return self.id

    def getPoints(self):
        return self.points

    def getType(self):
        return self.type

    def getCardNum(self):
        return len(self.hand)

    def getHand(self, scrollNum=0, hide=False):
        return self.hand.show(scrollNum, hide)

    def getForceDraws(self):
        return self.forceDraw

    def addForceDraw(self, num):
        self.forceDraw += num

    def decreaseForceDraw(self):
        self.forceDraw -= 1

    def removeForceDraw(self):
        self.forceDraw = 0

    def checkCard(self, index):
        return self.hand.getCard(int(index))

    def discardHand(self):
        self.hand.discard()

    def __str__(self):
        return self.name

    def __repr__(self):
        return '({},{})'.format(self.name, self.points)

class Hand():
    ''''deck' (Deck) : Card's Color (rgby)
       'numberOfCards' (int) : Card's Value (0-9, R, X, W, +2, +4)'''

    def __init__(self, deck=None,numberOfCards=0):
        self.hand = []
        if deck != None:
            self.draw(deck,numberOfCards)

    def __iter__(self):
        return iter(self.hand)

    def __len__(self):
        return len(self.hand)

    def __getitem__(self, item):
        try:
            return self.hand[item]
        except:
            return ''

    def addCard(self, card):
        self.hand.append(card)

    def removeCard(self, index):
        index = int(index)
        if (0 <= index < len(self)):
            return self.hand.pop(index)

    def discard(self):
        self.hand = []

    def show(self, scrollNum=0, hide=False):
        if scrollNum == -1:
            scrollNum = 0
        output = ''
        num = 0
        header, footer, upper, lower = '', '', '', ''
        header +=   ('\033[97m\u2666--\u2666\033[0m ')
        upper +=    ('\033[97m|<-|\033[0m ')
        lower +=    ('\033[97m|<-|\033[0m ')
        footer +=   ('\033[97m\u2666--\u2666\033[0m ')
        for i in range(10):
            indexNum = i+(10*scrollNum)
            if indexNum < len(self):
                header += (self[indexNum].getRow(0,hide)+' ')
                upper += (self[indexNum].getRow(1,hide)+' ')
                lower += (self[indexNum].getRow(2,hide)+' ')
                footer += (self[indexNum].getRow(3,hide)+' ')
                num += 1
        for j in range(10-num):
            j #unused
            header += ('     ')
            footer += ('     ')
            upper += ('     ')
            lower += ('     ')
        header +=   ('\033[97m\u2666--\u2666\033[0m ')
        upper +=    ('\033[97m|->|\033[0m ')
        lower +=    ('\033[97m|->|\033[0m ')
        footer +=   ('\033[97m\u2666--\u2666\033[0m ')
        output += ('  '+header+'\n  '+upper+'\n  '+lower+'\n  '+footer+'\n\033[97m|-(<)--')
        for k in range(num):
            output += '({})'.format(k)
            output += '--'
        for l in range(10-num):
            l #unused
            output += '-----'
        output += '(>)--|\033[0m\n'
        return output

    def getCard(self, index):
        return self.hand[index]

    def indexCard(self, card):
        return self.hand.index(card)

class GameSettings():

    playerIdentities = ('play1','play2','play3','play4')
    computerNames = ('Watson','SkyNet','Hal','Metal Gear')

    def __init__(self):
        self.playerStaging = []                  #    Where Player Objs Are Stored Before Game Starts
        self.players = {}                        #    ID : Player Obj
        self.numPlayers = 0
        self.useColor = True
        self.displayEffects = True
        self.hideComputerHands = True
        self.zeroChange = False
        self.computerSimulation = False
        self.mainMenuError = ''
        self.computerSpeed = 'normal'

    def canAddPlayer(self):
        return (self.numPlayers < 4)

    def canRemovePlayer(self):
        return (self.numPlayers > 0)

    def canBegin(self):
        return (self.numPlayers > 1)

    def addPlayer(self, player):
        self.playerStaging.append(player)
        self.numPlayers += 1

    def removePlayer(self, number):
        number -= 1
        del self.playerStaging[number]
        self.numPlayers -= 1

    def clearStaging(self):
        self.numPlayers = 0
        self.playerStaging = []

    def finalizePlayers(self):
        self.players.clear()
        identity = 0
        for player in self.playerStaging:
            playerID = self.playerIdentities[identity]
            player.assignID(playerID)
            self.players[playerID] = player
            identity += 1

    def getPlayerNum(self):
        return self.numPlayers

    def getComputerName(self):
        complete = False
        index = self.numPlayers
        while not complete:
            name = self.computerNames[index]
            complete = True
            for player in self.playerStaging:
                if player.getName() == name:
                    index += 1
                    if index >= len(self.computerNames):
                        index = 0
                        complete = False

        return self.computerNames[index]

    def getRandomIdentity(self):
        '''For Getting a Random Player for First Turn.'''
        return random.choice(self.players.keys())

    def compileMainMenuElements(self):
        def getBlankSpace(word, total):
            return " "*(total-len(word))

        def getPlayerBox(playerNum, rowNum):
            if rowNum == 1:
                name = self.playerStaging[playerNum-1].getName()
                return '{}{}'.format(name, getBlankSpace(name, 29))
            elif rowNum == 2:
                points = self.playerStaging[playerNum-1].getPoints()
                return 'Points: {}{}'.format(points, getBlankSpace(str(points), 21))

        self.mainMenuElements= {'play1row1':'No Player                    ','play1row2':'                             ',
                                'play2row1':'No Player                    ',
                                'play2row2':'                             ',
                                'play3row1':'No Player                    ','play3row2':'                             ',
                                'play4row1':'No Player                    ',
                                'play4row2':'                             ',
                                'play1box':'\033[90m','play2box':'\033[90m','play3box':'\033[90m','play4box':'\033[90m',
                                'beginBox':'\033[90m','addBox':'\033[97m','removeBox':'\033[90m'
                                }
        playerBoxKey = 'play{}box'
        playerRowKey = 'play{}row{}'
        i = 1
        for j in self.playerStaging:
            j
            colorCode = ['\033[91m','\033[94m','\033[92m','\033[93m']
            key = playerBoxKey.format(i)
            self.mainMenuElements[key] = colorCode[i-1]
            self.mainMenuElements[playerRowKey.format(i,1)] = getPlayerBox(i, 1)
            self.mainMenuElements[playerRowKey.format(i,2)] = getPlayerBox(i, 2)
            i+=1
        if self.canBegin():
            self.mainMenuElements['beginBox'] = '\033[95m'
        if not self.canAddPlayer():
            self.mainMenuElements['addBox'] = '\033[90m'
        if self.canRemovePlayer():
            self.mainMenuElements['removeBox'] = '\033[97m'

    def changeComputerSpeed(self):
        if self.computerSpeed == 'slow':
            self.computerSpeed = 'normal'
        elif self.computerSpeed == 'normal':
            self.computerSpeed = 'fast'
        elif self.computerSpeed == 'fast':
            self.computerSpeed = 'slow'

    def getMainMenuElements(self):
        return self.mainMenuElements

class Deck():
    ''''shuffle' (bool) : shuffle deck.'''

    colors =     ('red','yellow','green','blue')
    values =     ('0','1','2','3','4','5','6','7','8','9','X','R','+2')

    def __init__(self, populate):
        '''Initializes proper deck of 108 Uno Cards.'''
        self.deck = []
        if populate:
            self.populate(True)

    def __getitem__(self, index):
        return self.deck[index]

    def populate(self, shuffle=True):
        for color in self.colors:
            for value in self.values:
                self.deck.append(Card(color, value))
                if value != '0':
                    self.deck.append(Card(color, value))
        for i in range(4):
            i #unused
            self.deck.append(Card('wild', '+4'))
            self.deck.append(Card('wild', 'W'))
        if shuffle:
            self.shuffle()

    def __iter__(self):
        return iter(self.deck)

    def __len__(self):
        return len(self.deck)

    def draw(self):
        return self.deck.pop()

    def place(self, card):
        return self.deck.append(card)

    def insert(self, card):
        self.deck.insert(0, card)

    def shuffle(self):
        random.shuffle(self.deck)

class ComputerPlayer(Player):

    def __init__(self, name):
        super().__init__(name)
        self.type = 'Computer'
        self.begun = False
        self.colorsInHand = {'red':0, 'blue':0, 'green':0, 'yellow':0, 'wild':0}
        self.colorsOutHand = {}
        self.currentColor = ""

    def addCard(self, card):
        Player.addCard(self, card)
        color = card.getColor()
        self.colorsInHand[color] += 1

    def indexCard(self, cardColor, cardValue):
        for card in self.hand:
            if card.getValue() == cardValue:
                if cardValue in ('+4', 'W'):
                    return self.hand.indexCard(card)
                else:
                    if card.getColor() == cardColor:
                        return self.hand.indexCard(card)
        raise ValueError("Card Cannot Be Found")

    def think(self, match):
        card = None
        self.currentColor = match.currentColor
        currentValue = match.currentValue
        zeroChangeRule = match.zeroChange
        twoPlayers = False
        previousTurnID = match.getNextTurn(True)
        nextTurnID = match.getNextTurn(False)
        previousPlayer = match.getPlayer(previousTurnID)
        #nextPlayer = match.getPlayer(nextTurnID)
        if previousTurnID == nextTurnID:
            twoPlayers = True
            if self.canSkip == False and self.canReverse == True:
                self.canSkip = True
            self.canReverse = False

        self.getLegalCards(self.currentColor, currentValue, zeroChangeRule)

        ### DRAW CASE ###

        if len(self.legalCards) == 0 and len(self.wildCards) == 0:
            return "d"

        else:

            ### NO LEGAL CARD, USE WILD CARD ###

            if len(self.legalCards) == 0:

                if zeroChangeRule and self.canZeroChange:
                    bestZeroColor = self.getBestColor(self.zeroCards)
                    card = self.getCardByColor(self.zeroCards, bestZeroColor)

                else:

                    if self.canDrawFour:
                        card = self.getCardByValue(self.wildCards, "+4")
                        print(card)

                    else:
                        card = random.choice(self.wildCards)

            else:

                ### HAS LEGAL CARD ###

                if twoPlayers and self.canSkip: #Always play a skip card in a two player game
                    #print("Shed Skip Strategy")
                    card = self.getCardByValue(self.legalCards,"R", "X")

                if self.canReverse and previousPlayer.didDraw():
                    #print("Reverse Strategy")
                    reverseCards = self.getAllCardsByValue(self.legalCards, "R")
                    for reverseCard in reverseCards:
                        if reverseCard.getColor() == self.currentColor:
                            card = reverseCard

                if self.canValueChange:
                    # Computer Can Value Change, However, Should it?
                    # Computer Checks to See if Value Change Color is Better Than Current
                    currentColorNum = self.colorsInHand[self.currentColor]
                    bestValueChangeColor = self.getBestColor(self.valueChangeCards)
                    if self.colorsInHand[bestValueChangeColor] > currentColorNum or len(self.valueChangeCards) == len(self.legalCards):
                        card = self.getCardByColor(self.valueChangeCards, bestValueChangeColor)


                if card == None:
                    #print("Random Strategy")
                    card = random.choice(list(set(self.legalCards) - set(self.valueChangeCards)))

        color = card.getColor()
        self.colorsInHand[color] -= 1
        return str(self.indexCard(card.getColor(), card.getValue()))

    def getWildColor(self):
        maxKey = max(self.colorsInHand, key=self.colorsInHand.get)
        if maxKey == 'wild':
            return random.choice(('r','g','b','y'))
        else:
            return maxKey

    def getCardByValue(self, cardList, *values):
        for card in cardList:
            if card.getValue() in values:
                return card

    def getAllCardsByValue(self, cardList, *values):
        cards = []
        for card in cardList:
            if card.getValue() in values:
                cards.append(card)
        return cards

    def getCardByColor(self, cardList, *colors):
        for card in cardList:
            if card.getColor() in colors:
                return card

    def getBestColor(self, cardList):
        bestColor = None
        bestColorNum = 0
        for card in cardList:
            color = card.getColor()
            if self.colorsInHand[color] > bestColorNum:
                bestColor = color
                bestColorNum = self.colorsInHand[color]
        return bestColor

class Card():
    '''
    'suit' (string) : Card's Color (rgby)
    'rank' (string) : Card's Value (0-9, R, X, W, +2, +4)
    '''

    colors = {
        'red'       :   '\033[91m',
        'green'     :   '\033[92m',
        'yellow'    :   '\033[93m',
        'blue'      :   '\033[94m',
        'purple'    :   '\033[95m',
        'cyan'      :   '\033[96m',
        'white'     :   '\033[97m',
        'wild'      :   '',
        'dwild'     :   '',
        'dred'       :   '\033[31m',
        'dgreen'     :   '\033[32m',
        'dyellow'    :   '\033[33m',
        'dblue'      :   '\033[34m',
        'dpurple'    :   '\033[35m',
        'dcyan'      :   '\033[36m',
        'dwhite'     :   '\033[37m',
    }

    idMap = {
        'red':'R','blue':'B','green':'G','yellow':'Y','wild':'W',
        '0':'0','1':'1','2':'2','3':'3','4':'4','5':'5','6':'6','7':'7','8':'8','9':'9',
        '+2':'+','R':'R','W':'W','+4':'$','X':'X'
    }

    bigNums = {
        "0" : [" .d888b. ","d88P Y88b","888   888","888   888","888   888","888   888","d88P Y88b"," \"Y888P\" "],
        "1" : ["  d888   "," d8888   ","   888   ","   888   ","   888   ","   888   ","   888   "," 8888888 "],
        "2" : [".d8888b. ","d88P  Y88","d8    888","    .d88P",".od888P\" ","d88P\"    ","888\"     ","888888888"],
        "3" : [" .d8888b.","d88P  Y88","     .d88","   8888\" ","     \"Y8b","888    88","Y88b  d88"," \"Y8888P\""],
        "4" : ["    d88b ","   d8P88 ","  d8  88 "," d8   88 ","d8    88 ","888888888","      88 ","      88 "],
        "5" : ["888888888","888      ","888      ","8888888b ","   \"Y88b ","      888","Y88b d88P","\"Y8888P\" "],
        "6" : [" .d888b. ","d88P Y88b","888      ","888d888b ","888P \"Y8b","888   888","Y88b d88b"," \"Y888P\" "],
        "7" : ["888888888","      d8P","     d8P ","    d8P  "," 8888888 ","  d8P    "," d8P     ","d8P      "],
        "8" : [" .d888b. ","d8P   Y8b","Y8b.  d8P"," \"Y8888\" "," .dP\"Yb. ","888   888","Y88b d88P"," \"Y888P\" "],
        "9" : [" .d888b. ","d8P   Y8b","88     88","Y8b.  d88"," \"Y88P888","      888","Y88b d88P"," \"Y888P\" "],
        "X" : ["Y8b   d8P"," Y8b d8P ","  Y8o8P  ","   Y8P   ","   d8b   ","  d888b  "," d8P Y8b ","d8P   Y8b"],
        "W" : ["88     88","88     88","88  o  88","88 d8b 88","88d888b88","88P   Y88","8P     Y8","P       Y"],
        "+2" : ["  db     ","  88     ","C8888D   ","  88 8888","  VP    8","     8888","     8   ","     8888"],
        "+4" : ["  db     ","  88     ","C8888D   ","  88   d ","  VP  d8 ","     d 8 ","    d8888","       8 "],
        "R9" : ["    d88P ","   d88P  ","  d88P   "," d88P    "," Y88b    ","  Y88b   ","   Y88b  ","    Y88b "],
        "R8" : ["   d88P  ","  d88P   "," d88P    ","d88P     ","Y88b     "," Y88b    ","  Y88b   ","   Y88b  "],
        "R7" : ["  d88P  Y"," d88P    ","d88P     ","88P      ","88b      ","Y88b     "," Y88b    ","  Y88b  d"],
        "R6" : [" d88P  Y8","d88P    Y","88P      ","8P       ","8b       ","88b      ","Y88b    d"," Y88b  d8"],
        "R5" : ["d88P  Y88","88P    Y8","8P      Y","P        ","b        ","8b      d","88b    d8","Y88b  d88"],
        "R4" : ["88P  Y88b","8P    Y88","P      Y8","        Y","        d","b      d8","8b    d88","88b  d88P"],
        "R3" : ["8P  Y88b ","P    Y88b","      Y88","       Y8","       d8","      d88","b    d88P","8b  d88P "],
        "R2" : ["P  Y88b  ","    Y88b ","     Y88b","      Y88","      d88","     d88P","    d88P ","b  d88P  "],
        "R1" : ["  Y88b   ","   Y88b  ","    Y88b ","     Y88b","     d88P","    d88P ","   d88P  ","  d88P   "],
        "R0" : [" Y88b    ","  Y88b   ","   Y88b  ","    Y88b ","    d88P ","   d88P  ","  d88P   "," d88P    "],
    }


    def __init__(self, color, value):
        '''Initializes Uno Card w/ Color and Value.'''
        self.wild = False       #Is wild card?
        self.zero = False
        self.cardID = '{}{}'.format(self.idMap[color],self.idMap[value])
        self.setColor(color)
        self.setValue(value)
        self.setPoints(value)


    #############################################

    ### -\/-  Retrieve Card Information  -\/- ###

    def __repr__(self):
        return "{},{}".format(self.color, self.value)

    def getBigNum(self, reverse, reverseSeed=0):
        '''Returns list of strings to draw card's value on the pile.'''
        bigNums = []
        colorCode = self.colorCode
        colorCodeDark = self.colorCodeDark
        value = self.value
        if value == 'R':
            if not reverse:
                value += str(reverseSeed)
            else:
                value += str(9-reverseSeed)
        for mid in self.bigNums[value]:
            bigNums += ['{}| |{}'.format(colorCode,colorCodeDark)+mid+'{}| |\033[0m\t'.format(colorCode)]

        return bigNums

    def getColor(self):
        '''Returns card's color.'''
        return self.color

    def getColorCode(self):
        '''Returns card's color code.'''
        return self.colorCode

    def getValue(self):
        '''Returns card's value.'''
        return self.value

    def getPoints(self):
        '''Returns card's point value.'''
        return self.points

    def getRow(self,rowNum,hide=False):
        value = self.value
        displaySpace = self.displaySpace
        if hide:
            colorCode = '\033[97m'
            value = '?'
            displaySpace = ' '
        else:
            colorCode = self.colorCode
            if self.isWild():
                if rowNum == 0:
                    colorCode = '\033[91m'
                elif rowNum == 1:
                    colorCode = '\033[93m'
                elif rowNum == 2:
                    colorCode = '\033[92m'
                elif rowNum == 3:
                    colorCode = '\033[94m'

        if rowNum == 0:
            return      '{}\u2666--\u2666\033[0m'.format(colorCode)
        elif rowNum == 1:
            return      '{}|{}{}|\033[0m'.format(colorCode, displaySpace, value)
        elif rowNum == 2:
            if hide:
                return   '{}|? |\033[0m'.format(colorCode)
            else:
                return   '{}|  |\033[0m'.format(colorCode)
        elif rowNum == 3:
            return      '{}\u2666--\u2666\033[0m'.format(colorCode)

    #############################################

    ### -\/-  Set Card Information  -\/- ###

    def setColor(self, color):
        '''Sets Card's color and escape code.'''
        if color == 'blue':
            self.color = 'blue'
            self.colorCode = self.colors['blue']
            self.colorCodeDark = self.colors['dblue']
        elif color == 'red':
            self.color = 'red'
            self.colorCode = self.colors['red']
            self.colorCodeDark = self.colors['dred']
        elif color == 'yellow':
            self.color = 'yellow'
            self.colorCode = self.colors['yellow']
            self.colorCodeDark = self.colors['dyellow']
        elif color == 'green':
            self.color = 'green'
            self.colorCode = self.colors['green']
            self.colorCodeDark = self.colors['dgreen']
        elif color == 'wild':         #No color modification
            self.wild = True
            self.color = 'wild'
            self.colorCodeDark = self.colors['dwild']
            self.colorCode = self.colors['wild']

    def setValue(self, value):
        if value in ('0','1','2','3','4','5','6','7','8','9','X','R','+2','+4','W'):
            self.value = value
            self.displaySpace = ' '
            if len(value) == 2:
                self.displaySpace = ''
            if value == '0':
                self.zero = True

    def setPoints(self, value):
        if value in ('0','1','2','3','4','5','6','7','8','9'):
            self.points = int(value)
        elif value in ("W", "+4"):
            self.points = 50
        else:
            self.points = 20


    #############################################

    ### -\/-  Wild Card Methods  -\/- ###

    def changeColor(self, color):
        '''Changes Card's Color, Intended for Wild Cards.'''
        self.setColor(color)

    def isWild(self):
        '''Returns if card is a wild card.'''
        return self.wild

    def isZero(self):
        return self.zero

class Match():

    elementsInit = {
        ### Names (final) ###
        'P1Name':'           ', 'P2Name':'           ', 'P3Name':'           ', 'P4Name':'           ',
        ### Card Values ###
        'P1Cards':'           ', 'P2Cards':'           ', 'P3Cards':'           ', 'P4Cards':'           ',
        ### Turn Colors / Hand###
        'P1Turn':'', 'P2Turn':'', 'P3Turn':'', 'P4Turn':'',
        'HName':'\t\t', 'HVisual':'' ,'Hand':'',
        ### Deck ###
        'DNum':'', 'Deck':['','','','','','','','',''],
        'PostDNum':'',
        ### Pile ###
        'uHeader':'\t\t\t\t', 'uMiddle':'   ', 'uLower':'   ',
        'oHeader':'\t\t\t', 'oMiddle':['\t\t\t','\t\t\t','\t\t\t','\t\t\t','\t\t\t','\t\t\t','\t\t\t','\t\t\t'],
        ### Messages ###
        'Console':'', 'Error':''
        }

    speeds = {'slow':2,'normal':1,'fast':0}


    def __init__(self, gs):
        ### Decks ###
        self.deck = Deck(True)
        self.pile = Deck(False)

        ### Player Information ###
        self.players = gs.players
        self.turnList = []
        self.handTitles =  {'play1':'','play2':'','play3':'','play4':''}

        ### Carry Information ###
        self.displayEffects = gs.displayEffects
        self.hideComputerHands = gs.hideComputerHands
        self.zeroChange = gs.zeroChange
        self.computerSpeed = self.speeds[gs.computerSpeed]
        self.simulation = gs.computerSimulation

        ### Data ###
        self.handPosition = 0               # For hand displays
        self.drawAmount = 0                 # Used for force draws
        self.passes = 0                     # Keep track of consecutive passes for emergency color change
        self.passMax = 0                    # Max passes before color change
        self.turn = ''                      # Current turn
        self.event = ''                     # Wild, Reverse, Skip, etc
        self.wildColorChange = ''           # Specifies color to change wild card to
        self.currentColor = ''              # Current color
        self.currentValue = ''              # Current value
        self.winnerID = ''                  # ID of Player who Won
        self.reverse = False                # Is turn order reversed
        self.turnComplete = False           # Is turn complete
        self.matchComplete = False          # Is the Game over?
        self.matchAbort = False             # Did the match conclude without a winner?
        self.forcedWild = False             # Force change wild

        ### Initialize Names / Cards / Deck (Assuming New Game) ###
        self.elements = dict(self.elementsInit)

        keyStringName = 'P{}Name'
        keyStringCards = 'P{}Cards'

        for i in self.players:
            self.elements[keyStringName.format(i[-1])] = self.players[i].getName()+(' '*(11-len(self.players[i].getName())))
            self.elements[keyStringCards.format(i[-1])] = '  '+(' '*(3-len(str(self.players[i].getCardNum()))))+str(self.players[i].getCardNum())+' Cards'

        self.elements['DNum'] = len(self.deck)

        if len(str(len(self.deck))) < 2:
            self.elements['PostDNum'] = '\t'

        j = 8
        for i in range(int(math.ceil(len(self.deck)/12))):
            self.elements['Deck'][j] = '='
            j -= 1

        for key in GameSettings.playerIdentities:
            try:
                self.buildHandString(key)
                self.turnList += [key]
            except KeyError:
                pass

        self.passMax = len(self.turnList)

    def clearShell(self):
        os.system('cls' if os.name == 'nt' else 'clear')

    def begin(self):
        self.elements['Console'] = 'Beginning Game, Press Enter.'
        print(self.drawScreen())
        self.enterBreak()
        self.eventDealCards()
        self.turn = random.choice(self.turnList)
        self.elements['Console'] = 'First turn will be {}. Press Enter.'.format(self.players[self.turn].getName())
        print(self.drawScreen(True))
        self.enterBreak()
        self.placeCard()
        self.elements['P{}Turn'.format(self.turn[-1])] = '\033[93m'
        if self.event == 'wild':
            self.eventWildCard()
        elif self.event == 'reverse':
            self.eventReverse()

    def end(self, gs):
        if not self.matchAbort:
            points = 0
            self.elements['P{}Turn'.format(self.turn[-1])] = ''
            self.elements['Console'] = '{} Wins! Press Enter to Begin Point Tally'.format(self.players[self.winnerID].getName())
            print(self.drawScreen())
            self.enterBreak()

            for identity in self.turnList:
                if identity != self.winnerID:
                    self.turn = identity
                    self.elements['HName'] = self.handTitles[self.turn]
                    self.elements['P{}Turn'.format(self.turn[-1])] = '\033[93m'
                    while self.players[identity].getCardNum() > 0:
                        card = self.players[identity].removeCard(0)
                        points += card.getPoints()
                        self.elements['Console'] = '{} Won {} Points!'.format(self.players[self.winnerID].getName(),points)

                        keyStringCards = 'P{}Cards'
                        self.elements[keyStringCards.format(identity[-1])] = '  '+(' '*(3-len(str(self.players[identity].getCardNum()))))+str(self.players[identity].getCardNum())+' Cards'
                        self.players[identity].maxScroll = math.ceil((self.players[identity].getCardNum() / 10)-1)
                        if self.handPosition > self.players[identity].maxScroll:
                            self.handPosition -= 1
                        self.buildHandVisual(identity)

                        if self.displayEffects and not self.simulation:
                            print(self.drawScreen())
                            time.sleep(.1)
                    self.elements['P{}Turn'.format(self.turn[-1])] = ''

            self.players[self.winnerID].addPoints(points)
            self.elements['Console'] = '{} Won {} Points! Press Enter'.format(self.players[self.winnerID].getName(),points)
            print(self.drawScreen())
            self.enterBreak()

        gs.clearStaging()
        for identity in self.turnList:
            self.players[identity].discardHand()
            gs.addPlayer(self.players[identity])
        return gs

    def adjustCardAmount(self, playerID):
        keyStringCards = 'P{}Cards'
        self.elements[keyStringCards.format(playerID[-1])] = '  '+(' '*(3-len(str(self.players[playerID].getCardNum()))))+str(self.players[playerID].getCardNum())+' Cards'
        self.players[playerID].maxScroll = math.ceil((self.players[playerID].getCardNum() / 10)-1)
        if self.handPosition > self.players[playerID].maxScroll:
            self.handPosition -= 1
        self.buildHandVisual(playerID)

    def buildHandString(self, playerID):
        playerName = self.players[playerID].getName()
        if len(playerName) < 9:
            self.handTitles[playerID] = "{}'s Hand\t".format(self.players[playerID].getName())
        else:
            self.handTitles[playerID] = "{}'s Hand".format(self.players[playerID].getName())

    def buildHandVisual(self, playerID):
        string = '['
        for i in range(self.players[playerID].maxScroll+1):
            if i == self.handPosition:
                string += '|'
            else:
                string += '-'
        string += ']'
        self.elements['HVisual'] = string

    def checkInput(self, playerInput):
        if playerInput == '':
            return {'valid':False,'entry':playerInput}
        if playerInput.isnumeric():
            if int(playerInput)+(10*self.handPosition) < self.players[self.turn].getCardNum():
                return {'valid':True,'entry':str(int(playerInput)+(10*self.handPosition)),'type':'card'}
            else:
                self.elements['Error'] = '{} is not a card.'.format(playerInput)
                return {'valid':False,'entry':playerInput}
        else:
            playerInput = playerInput.lower()[0]
            if playerInput in ['<','>','u','d','p','q','s']:
                return {'valid':True,'entry':playerInput}
            else:
                self.elements['Error'] = '{} is not a valid selection.'.format(playerInput)
                return {'valid':False,'entry':playerInput}

    def checkColorInput(self, playerInput):
        if playerInput == '':
            return {'valid':False,'entry':playerInput}
        playerInput = str(playerInput).lower()[0]
        if playerInput[0] == 'b':
            return {'valid':True,'entry':'blue'}
        elif playerInput[0] == 'r':
            return {'valid':True,'entry':'red'}
        elif playerInput[0] == 'g':
            return {'valid':True,'entry':'green'}
        elif playerInput[0] == 'y':
            return {'valid':True,'entry':'yellow'}
        return {'valid':False,'entry':playerInput}

    def eventDealCards(self):
        if self.displayEffects and not self.simulation:
            self.elements['Console'] = 'Dealing Cards...'
        for i in ('play1','play2','play3','play4'):
            if i in self.players:
                for j in range(7):
                    j #unused
                    self.dealCard(i)
                    if self.displayEffects and not self.simulation:
                        print(self.drawScreen(True))
                        time.sleep(.1)

    def eventReverse(self):
        if self.displayEffects and not self.simulation:
            hide = False
            if self.players[self.turn].getType() == "Computer":
                hide = self.hideComputerHands
            self.elements['Console'] = "Reverse Card Played! Reversing Turn Order.".format(self.players[self.turn].getName())
            print(self.drawScreen(hide))
            time.sleep(1)
            for i in range(10):
                cardBigNums = self.pile[0].getBigNum(self.reverse,i)
                self.elements['oMiddle'] = cardBigNums
                print(self.drawScreen(hide))
                if self.displayEffects and not self.simulation:
                    time.sleep(.1)
        cardBigNums = self.pile[0].getBigNum(self.reverse,9)
        self.elements['oMiddle'] = cardBigNums
        self.reverse = not self.reverse
        self.event = ''

    def eventSkip(self):
        if self.displayEffects and not self.simulation:
            hide = False
            if self.players[self.turn].getType() == "Computer":
                hide = self.hideComputerHands
            self.elements['Console'] = "Skip Card Placed! Skipping {}'s Turn.".format(self.players[self.turn].getName())
            print(self.drawScreen(hide))
            time.sleep(1)
            for i in range(2):
                i #unused
                self.elements['P{}Turn'.format(self.turn[-1])] = '\033[91m'
                print(self.drawScreen(hide))
                time.sleep(.3)
                self.elements['P{}Turn'.format(self.turn[-1])] = ''
                print(self.drawScreen(hide))
                time.sleep(.3)
        self.turnComplete = True
        self.event = ''

    def eventWildCard(self):
        hide = False
        if not self.forcedWild:
            if self.players[self.turn].getType() == 'Human':
                self.elements['Console'] = 'Wild Card! Specifiy a Color: (B)lue, (R)ed, (G)reen, (Y)ellow'
                self.elements['Error'] = 'Specifiy A Color'
                print(self.drawScreen())
                playerInput = str(input("Color Change: "))
                checked = self.checkColorInput(playerInput)
                while not checked['valid']:
                    if checked['entry'] == '<':
                        self.handPosition -= 1
                        if self.handPosition == -1:
                            self.handPosition = self.players[self.turn].maxScroll
                        self.buildHandVisual(self.turn)
                    elif checked['entry'] == '>':
                        self.handPosition += 1
                        if self.handPosition > self.players[self.turn].maxScroll:
                            self.handPosition = 0
                        self.buildHandVisual(self.turn)
                    print(self.drawScreen())
                    playerInput = str(input("Color Change: "))
                    checked = self.checkColorInput(playerInput)
            else:
                hide = self.hideComputerHands
                checked = self.checkColorInput(self.players[self.turn].getWildColor())
            self.wildColorChange = checked['entry']
        else:
            self.wildColorChange = self.checkColorInput(random.choice(('r','b','g','y')))['entry']
            self.forcedWild = False
        self.currentColor = self.wildColorChange
        self.elements['Error'] = ""
        if self.displayEffects and not self.simulation:
            self.elements['Console'] = 'Wild Card! Changing Color.'
            seed = 1
            for i in range(10):
                i #unused
                if seed > 4:
                    seed = 1
                print(self.drawScreen(hide,wildSeed=seed))
                time.sleep(.1)
                seed += 1
        self.pile[0].changeColor(self.wildColorChange)
        self.wildColorChange = ''
        cardBigNums = self.pile[0].getBigNum(self.reverse)
        self.elements['oHeader'] = '{}\u2666\u2666\u2666=========\u2666\u2666\u2666\033[0m\t'.format(self.pile[0].getColorCode())
        self.elements['oMiddle'] = cardBigNums
        self.event = ''

    def eventDraw(self):
        self.players[self.turn].addForceDraw(self.drawAmount)
        self.drawAmount = 0
        self.event = ''

    def dealCard(self, playerID):

        card = self.deck.draw()
        self.players[playerID].addCard(card)

        ### Adjust Hand Visual ###
        self.players[playerID].maxScroll = math.ceil((self.players[playerID].getCardNum() / 10)-1)
        self.handPosition = self.players[playerID].maxScroll
        self.buildHandVisual(playerID)

        ### Adjust Player Tile ###
        keyStringCards = 'P{}Cards'
        self.elements[keyStringCards.format(playerID[-1])] = '  '+(' '*(3-len(str(self.players[playerID].getCardNum()))))+str(self.players[playerID].getCardNum())+' Cards'

        ### Adjust Deck ###
        self.elements['DNum'] = len(self.deck)
        if len(str(len(self.deck))) < 2:
            self.elements['PostDNum'] = '\t'
        j = 8
        self.elements['Deck'] = [' ',' ',' ',' ',' ',' ',' ',' ', ' ']
        for i in range(math.ceil(len(self.deck)/12)):
            i #unused
            self.elements['Deck'][j] = '='
            j -= 1

    def placeCard(self, card=None):
        if card == None:
            ### Used At Beginning For First Card ###
            card = self.deck.draw()
            self.elements['DNum'] = len(self.deck)

        cardColor = card.getColorCode()
        cardBigNums = card.getBigNum(self.reverse)

        self.currentColor = card.getColor()
        self.currentValue = card.getValue()

        self.pile.insert(card)
        self.elements['oHeader'] = '{}\u2666\u2666\u2666=========\u2666\u2666\u2666\033[0m\t'.format(cardColor)
        self.elements['oMiddle'] = cardBigNums

        if len(self.pile) > 1:
            previousCard = self.pile[1]
            previousCardColor = previousCard.getColorCode()
            self.elements['uHeader'] = '{}      \u2666\u2666\u2666=========\u2666\u2666\u2666\033[0m\t\t'.format(previousCardColor)
            self.elements['uMiddle'] = '{}| |\033[0m'.format(previousCardColor)
            self.elements['uLower'] = '{}\u2666\u2666\u2666\033[0m'.format(previousCardColor)

        if self.currentColor == 'wild':
            self.event = 'wild'

        if self.currentValue == 'X':
            self.event = 'skip'
        elif self.currentValue == 'R':
            if len(self.players) > 2:
                self.event = 'reverse'
            else:
                self.event = 'skip'
        elif self.currentValue == '+4':
                self.drawAmount = 4
        elif self.currentValue == '+2':
                self.drawAmount = 2
        self.passes = 0

    def extractCard(self, playerID, index):
        card = self.players[playerID].removeCard(index)
        if self.players[playerID].getCardNum() == 0:
            self.matchComplete = True
            self.winnerID = self.turn
        self.adjustCardAmount(playerID)
        return card

    def enterBreak(self):
        if not self.simulation:
            str(input())
        return

    def nextTurn(self):
        self.turnComplete = False
        self.handPosition = 0
        turnType = self.players[self.turn].getType()
        self.players[self.turn].beginTurn()
        ### Prepare Hand Visuals ###

        self.elements['HName'] = self.handTitles[self.turn]
        self.buildHandVisual(self.turn)

        if self.event == 'skip':
            self.eventSkip()
        elif self.drawAmount > 0:
            self.eventDraw()

        while not self.turnComplete:
            if turnType == 'Human':
                self.players[self.turn].getLegalCards(self.currentColor, self.currentValue, self.zeroChange)
                if len(self.deck) > 0:
                    self.elements['Console'] = 'Select a card, (D)raw, or (P)ause.'
                else:
                    self.players[self.turn].removeForceDraw()
                    self.elements['Console'] = 'Select a card, (D)raw, (P)ause, or Pas(s).'
                if self.players[self.turn].getForceDraws() > 0:
                    self.elements['Error'] = 'Draw Card Played! Draw {} cards.'.format(self.players[self.turn].getForceDraws())
                print(self.drawScreen())
                playerInput = str(input("\033[97mSelection: \033[92m"))
                checked = self.checkInput(playerInput)
                while not checked['valid']:
                    print(self.drawScreen())
                    playerInput = str(input("\033[97mSelection: \033[92m"))
                    checked = self.checkInput(playerInput)

                playerInput = checked['entry']

                if playerInput == '<':
                    self.handPosition -= 1
                    if self.handPosition == -1:
                        self.handPosition = self.players[self.turn].maxScroll
                    self.buildHandVisual(self.turn)
                elif playerInput == '>':
                    self.handPosition += 1
                    if self.handPosition > self.players[self.turn].maxScroll:
                        self.handPosition = 0
                    self.buildHandVisual(self.turn)
                elif playerInput == 'd':
                    if len(self.deck) > 0:
                        self.elements['Error'] = ''
                        self.dealCard(self.turn)
                    else:
                        self.elements['Error'] = "Cannot Draw. Deck is Empty"
                elif playerInput == 'p':
                    pauseOutput = self.pauseScreen()
                    if pauseOutput == 'quit':
                        self.matchComplete = True
                        self.turnComplete = True
                        self.winnerID = 'play1'
                        self.matchAbort = True
                elif playerInput == 's':
                    if len(self.deck) > 0:
                        self.elements['Error'] = "Cannot pass until Deck is empty."
                    elif len(self.players[self.turn].getAllValidCards()) > 0:
                        self.elements['Error'] = "Cannot pass while having playable cards."
                    else:
                        self.turnComplete = True
                        self.passes += 1
                        if self.passes == self.passMax:
                            self.forcedWild = True
                            self.event = 'wild'
                            self.passes = 0
                elif playerInput.isnumeric():
                    if self.players[self.turn].getForceDraws() == 0:
                        cardCheck = self.players[self.turn].checkCard(playerInput)
                        if cardCheck in self.players[self.turn].getAllValidCards():
                            card = self.extractCard(self.turn, playerInput)
                            self.placeCard(card)
                            self.elements['Error'] = ""
                            self.turnComplete = True
                        else:
                            self.elements['Error'] = "Card Doesn't Match The Color {} or Value {}!".format(self.currentColor, self.currentValue)
                    else:
                        pass

            elif turnType == 'Computer':
                self.elements['Console'] = '{}\'s Turn'.format(self.players[self.turn].getName())
                print(self.drawScreen(self.hideComputerHands))
                if not self.simulation:
                    time.sleep(self.computerSpeed)
                #str(input())
                while (True):
                    if self.displayEffects and not self.simulation:
                        time.sleep(.2)
                    if self.players[self.turn].getForceDraws() > 0 and len(self.deck) > 0:
                        cardIndex = 'd'
                    else:
                        cardIndex = self.players[self.turn].think(self)
                    if cardIndex.isnumeric():
                        card = self.extractCard(self.turn, int(cardIndex))
                        if card.getColor() != self.currentColor:
                            self.resetDrawBool()
                        self.placeCard(card)
                        self.turnComplete = True
                        break
                    else:
                        if cardIndex == 'd':
                            if len(self.deck) > 0:
                                self.dealCard(self.turn)
                                print(self.drawScreen(self.hideComputerHands))
                            else:
                                self.turnComplete = True
                                self.players[self.turn].removeForceDraw()
                                self.passes += 1
                                if self.passes == self.passMax:
                                    self.forcedWild = True
                                    self.event = 'wild'
                                    self.passes = 0
                                break

            ### DECODE INPUT ###

        if self.event == 'reverse':
            self.eventReverse()
        elif self.event == 'wild':
            self.eventWildCard()

        # Clear Current Turn
        self.elements['P{}Turn'.format(self.turn[-1])] = ''
        # Prepare Next Turn
        self.turn = self.getNextTurn()
        self.elements['P{}Turn'.format(self.turn[-1])] = '\033[93m'

    def drawScreen(self, hide=False, wildSeed=0):
        if self.simulation:
            return ''
        colorCombos = {
            1 : ['\033[91m','\033[93m','\033[92m','\033[94m'],
            2 : ['\033[94m','\033[91m','\033[93m','\033[92m'],
            3 : ['\033[92m','\033[94m','\033[91m','\033[93m'],
            4 : ['\033[93m','\033[92m','\033[94m','\033[91m'] }
        currentTurn = self.turn
        if currentTurn == '':
            currentTurn = self.turnList[-1]
            hide = True
        if wildSeed != 0:
            colorMod = colorCombos[wildSeed]
        else:
            colorMod = ['','','','']

        self.clearShell()
        screenout = ''
        screenout += '\t\t\033[94m      || ||\033[92m ||\ ||  \033[91m// \\\\\n\033[0m'
        screenout += '\t\t\033[94m      || ||\033[92m ||\\\|| \033[91m((   ))\n\033[0m'
        screenout += '\t\t\033[94m      \\\ //\033[92m || \|| \033[91m \\\ //\n\033[0m'
        screenout += '\033[97m===============================================================\n'
        screenout += '\033[93m{}\033[0m\n'.format(self.elements['Console'])
        screenout += '\033[97m===============================================================\n'
        screenout += '\t\t\t\t\t\t'     +        ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P1Turn'])
        screenout += '\033[97mDeck:\t\t'        +       '{}'.format(self.elements['uHeader'])       +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P1Turn'],self.elements['P1Name'])
        screenout += '\033[97m{} Cards'.format(self.elements['DNum'])       +       '{}'.format(self.elements['PostDNum'])+'\t'     +       '{}'.format(self.elements['uHeader'])       +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P1Turn'],self.elements['P1Cards'])
        screenout += '\t\t      '       +      '{}'.format(self.elements['uMiddle'])        +       '\033[97m{}{}'.format(colorMod[0],self.elements['oHeader'])     +      ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P1Turn'])
        screenout += '\033[97m  _+_ \t\t      '     +       '{}'.format(self.elements['uMiddle'])                                                                                                   +       '\033[97m{}{}'.format(colorMod[1],self.elements['oHeader'])         +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P2Turn'])
        screenout += '\033[97m | '      +       '\033[92m{}\033[0m'.format(self.elements['Deck'][0])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[2],self.elements['oMiddle'][0])      +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P2Turn'],self.elements['P2Name'])
        screenout += '\033[97m | '      +       '\033[92m{}\033[0m'.format(self.elements['Deck'][1])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[3],self.elements['oMiddle'][1])      +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P2Turn'],self.elements['P2Cards'])
        screenout += '\033[97m | '      +       '\033[92m{}\033[0m'.format(self.elements['Deck'][2])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[0],self.elements['oMiddle'][2])      +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P2Turn'])
        screenout += '\033[97m | '      +       '\033[93m{}\033[0m'.format(self.elements['Deck'][3])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[1],self.elements['oMiddle'][3])      +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P3Turn'])
        screenout += '\033[97m | '      +       '\033[93m{}\033[0m'.format(self.elements['Deck'][4])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[2],self.elements['oMiddle'][4])      +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P3Turn'],self.elements['P3Name'])
        screenout += '\033[97m | '      +       '\033[93m{}\033[0m'.format(self.elements['Deck'][5])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uMiddle'])       +       '\033[97m{}{}'.format(colorMod[3],self.elements['oMiddle'][5])      +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P3Turn'],self.elements['P3Cards'])
        screenout += '\033[97m | '      +       '\033[91m{}\033[0m'.format(self.elements['Deck'][6])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uLower'])        +       '\033[97m{}{}'.format(colorMod[0],self.elements['oMiddle'][6])      +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P3Turn'])
        screenout += '\033[97m | '      +       '\033[91m{}\033[0m'.format(self.elements['Deck'][7])        +       '\033[97m |\t\t      '      +       '{}'.format(self.elements['uLower'])        +       '\033[97m{}{}'.format(colorMod[1],self.elements['oMiddle'][7])      +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P4Turn'])
        screenout += '\033[97m |_'      +     '\033[91m{}\033[0m'.format(self.elements['Deck'][8])          +        '\033[97m_|\t\t         '                                                      +      '\033[97m{}{}'.format(colorMod[2],self.elements['oHeader'])          +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P4Turn'],self.elements['P4Name'])
        screenout += '\033[97m\t\t         '    +                                                                                                                                                                   '\033[97m{}{}'.format(colorMod[3],self.elements['oHeader'])         +       ' \033[97m{}|{}|\033[0m\n'.format(self.elements['P4Turn'],self.elements['P4Cards'])
        screenout += '\t\t\t\t\t\t'     +       ' \033[97m{}\u2666-----------\u2666\033[0m\n'.format(self.elements['P4Turn'])
        screenout += "\033[97m{}".format(self.elements['HName'])        +       "\t\t\t\t {}\n".format(self.elements['HVisual'])
        screenout += '\033[97m===============================================================\n'
        screenout += self.players[currentTurn].getHand(self.handPosition,hide)
        screenout += '\033[91m{}\033[0m'.format(self.elements['Error'])
        return screenout

    def pauseScreen(self):
        while True:
            self.clearShell()
            print('\n\t\t\tPause')
            print('\n\t\t1. Resume')
            print('\t\t2. Quit')

            selection = str(input('\nSelection: ')).upper()
            while selection not in ['1', '2']:
                print('\nSelection Invalid')
                selection = str(input('\nSelection: ')).upper()

            if selection == '1' or "":
                return ""

            elif selection == '2':
                return "quit"


    def isComplete(self):
        return self.matchComplete

    def next(self):
        self.turn = self.getNextTurn()

    def getNextTurn(self, forceReverse=False):
        if forceReverse:
            reverse = not self.reverse
        else:
            reverse = self.reverse
        currentIndex = self.turnList.index(self.turn)
        if not reverse:
            if (currentIndex + 1) == len(self.turnList):
                return self.turnList[0]
            else:
                return self.turnList[currentIndex+1]
        else:
            if currentIndex == 0:
                return self.turnList[len(self.turnList) - 1]
            else:
                return self.turnList[currentIndex-1]

    def getPlayer(self, playerID):
        return self.players[playerID]

    def resetDrawBool(self):
        for identity in self.players:
            self.players[identity].drew = False

def Uno(debugging=False):

    ###MENUS###

    def clearShell():
        os.system('cls' if os.name == 'nt' else 'clear')

    def mainMenu():
        sys.stdout.write("\x1b[8;32;63t")
        sys.stdout.flush()
        gs = GameSettings()

        while True:

            print(drawMainMenu(gs))

            selection = str(input('\033[97mSelection: \033[92m'))
            while selection not in ['1', '2', '3', '4', '5']:
                gs.mainMenuError = "Invalid Selection"
                print(drawMainMenu(gs))
                selection = str(input('\033[97mSelection: \033[92m'))

            if selection == '1':
                if gs.canBegin():
                    gs.mainMenuError = ""
                    gs.finalizePlayers()
                    gs = playMatch(gs)
                else:
                    gs.mainMenuError = "Two Players Required to Begin"

            elif selection == '2':
                if gs.canAddPlayer():
                    gs.mainMenuError = ""
                    gs = addPlayer(gs)
                else:
                    gs.mainMenuError = "Max Number of Players Reached"

            elif selection == '3':
                if gs.canAddPlayer():
                    gs.mainMenuError = ""
                    gs = addComputer(gs)
                else:
                    gs.mainMenuError = "Max Number of Players Reached"

            elif selection == '4':
                if gs.canRemovePlayer():
                    gs.mainMenuError = ""
                    gs = removePlayer(gs)
                else:
                    gs.mainMenuError = "No Players to Remove"

            elif selection == '5':
                gs.mainMenuError = ""
                gs = settingsMenu(gs)

            else:
                raise BadInputError('Data Provided Has No Function')

    def playMatch(gs):
        for i in range(1):
            i
            m = Match(gs)
            m.begin()
            while (not m.isComplete()):
                m.nextTurn()
            gs = m.end(gs)
        return gs

    def addPlayer(gs):
        colors = ['\033[91m','\033[94m', '\033[92m', '\033[93m']
        nameOkay = False
        playerNum = gs.getPlayerNum() + 1
        colorIndex = playerNum - 1
        message = "\033[97mPlease Enter Player {}'s Name: {}".format(playerNum, colors[colorIndex])

        while not nameOkay:
            print(drawMainMenu(gs))
            name = str(input(message)).title()
            if len(name) > 11:
                gs.mainMenuError = "Name Must Be 11 Characters or Less!"
            elif len(name) == 0:
                gs.mainMenuError = ""
                return gs
            else:
                nameOkay = True
                for player in gs.playerStaging:
                    if player.getName() == name:
                        nameOkay = False
                if nameOkay == False or name in GameSettings.computerNames:
                    gs.mainMenuError = "Name Cannot Match Another Player's Name!"

        p = Player(name)
        gs.addPlayer(p)
        gs.mainMenuError = ""

        return gs

    def addComputer(gs):
        name = gs.getComputerName()
        c = ComputerPlayer(name)
        gs.addPlayer(c)

        return gs

    def removePlayer(gs):
        sys.stdout.write("\x1b[8;{rows};{cols}t".format(rows=32, cols=63))
        sys.stdout.flush()
        clearShell()

        complete = False
        playerNum = gs.getPlayerNum()
        message = "\033[97mPlease Enter Player Number to Remove: \033[91m".format(playerNum)

        while (not complete):
            print(drawMainMenu(gs))
            number = str(input(message))
            if len(number) == 0:
                gs.mainMenuError = ""
                return gs
            try:
                number = int(number)
                if 0 < number <= playerNum:
                    complete = True
                else:
                    gs.mainMenuError = "Invalid Player Number!"
            except:
                gs.mainMenuError = "Please Enter the Player Number, not Name!"

        gs.mainMenuError = ""
        gs.removePlayer(number)
        return gs

    def settingsMenu(gs):
        while True:
            sys.stdout.write("\x1b[8;32;63t")
            sys.stdout.flush()
            clearShell()
            print('\n\t\tSettings')
            print('\n\t1. Draw Effects\t\t\t{}'.format(gs.displayEffects))
            print('\t2. Hide Computer Hands\t\t{}'.format(gs.hideComputerHands))
            print('\t3. Computer Speed\t\t{}'.format(gs.computerSpeed.title()))
            #print('\t4. Zero Card Changes Color\t{}'.format(gs.zeroChange))
            #print('\t5. Run Simulations\t\t{}'.format(gs.computerSimulation))
            print('\n\tA. Exit')

            selection = str(input('\nSelection: ')).upper()
            while selection not in ('1', '2', '3', '4', '5', 'A', ''):
                print('\nSelection Invalid')
                selection = str(input('\nSelection: ')).upper()

            if selection == '1':
                gs.displayEffects = not gs.displayEffects

            elif selection == '2':
                gs.hideComputerHands = not gs.hideComputerHands

            elif selection == '3':
                gs.changeComputerSpeed()
                '''
            elif selection == '4':
                gs.zeroChange = not gs.zeroChange

            elif selection == '5':
                gs.computerSimulation = not gs.computerSimulation
                '''
            elif selection == 'A' or selection == '' or selection in ('4','5'):
                return gs

    def drawMainMenu(gs):
        clearShell()
        gs.compileMainMenuElements()
        menuElements = gs.getMainMenuElements()
        screenout = ''
        screenout += '\t\t\033[94m      || ||\033[92m ||\ ||  \033[91m// \\\\\n\033[0m'
        screenout += '\t\t\033[94m      || ||\033[92m ||\\\|| \033[91m((   ))\n\033[0m'
        screenout += '\t\t\033[94m      \\\ //\033[92m || \|| \033[91m \\\ //\n\033[0m'
        screenout += '\033[97m===============================================================\033[0m\n'
        screenout += "{}1-----------------------------1\033[0m {}2-----------------------------2\033[0m\n".format(menuElements['play1box'],menuElements['play2box'])
        screenout += "{}|{}|\033[0m {}|{}|\033[0m\n".format(menuElements['play1box'],menuElements['play1row1'],menuElements['play2box'],menuElements['play2row1'])
        screenout += "{}|{}|\033[0m {}|{}|\033[0m\n".format(menuElements['play1box'],menuElements['play1row2'],menuElements['play2box'],menuElements['play2row2'])
        screenout += "{}1-----------------------------1\033[0m {}2-----------------------------2\033[0m\n".format(menuElements['play1box'],menuElements['play2box'])
        screenout += "{}3-----------------------------3\033[0m {}4-----------------------------4\033[0m\n".format(menuElements['play3box'],menuElements['play4box'])
        screenout += "{}|{}|\033[0m {}|{}|\033[0m\n".format(menuElements['play3box'],menuElements['play3row1'],menuElements['play4box'],menuElements['play4row1'])
        screenout += "{}|{}|\033[0m {}|{}|\033[0m\n".format(menuElements['play3box'],menuElements['play3row2'],menuElements['play4box'],menuElements['play4row2'])
        screenout += "{}3-----------------------------3\033[0m {}4-----------------------------4\033[0m\n".format(menuElements['play3box'],menuElements['play4box'])
        screenout += "\033[97m===============================================================\033[0m\n"
        screenout += "  {}\u2666---------------------------\u2666\033[0m \u2666===========================\u2666\n".format(menuElements['beginBox'])
        screenout += "  {}|1.       Begin Match       |\033[0m |        High Scores        |\n".format(menuElements['beginBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m \u2666---------------------------\u2666\n".format(menuElements['beginBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}|2.       Add Player        |\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}|3.      Add Computer       |\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['addBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['removeBox'])
        screenout += "  {}|4.      Remove Player      |\033[0m |                           |\n".format(menuElements['removeBox'])
        screenout += "  {}\u2666---------------------------\u2666\033[0m |                           |\n".format(menuElements['removeBox'])
        screenout += "  \033[97m\u2666---------------------------\u2666\033[0m |                           |\n"
        screenout += "  \033[97m|5.        Settings         |\033[0m |                           |\n"
        screenout += "  \033[97m\u2666---------------------------\u2666\033[0m \u2666===========================\u2666\n"
        screenout += "\033[97m===============================================================\033[0m\n"
        screenout += '\033[91m{}\033[0m'.format(gs.mainMenuError)
        return screenout

    mainMenu()

if __name__ == "__main__":
    Uno()

    # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    # ==============================================================================
    """configure script to get build parameters from user."""

    from __future__ import absolute_import
    from __future__ import division
    from __future__ import print_function

    import argparse
    import errno
    import os
    import platform
    import re
    import subprocess
    import sys

    # pylint: disable=g-import-not-at-top
    try:
      from shutil import which
    except ImportError:
      from distutils.spawn import find_executable as which
    # pylint: enable=g-import-not-at-top

    _DEFAULT_CUDA_VERSION = '10.0'
    _DEFAULT_CUDNN_VERSION = '7'
    _DEFAULT_CUDA_COMPUTE_CAPABILITIES = '3.5,7.0'
    _DEFAULT_CUDA_PATH = '/usr/local/cuda'
    _DEFAULT_CUDA_PATH_LINUX = '/opt/cuda'
    _DEFAULT_CUDA_PATH_WIN = ('C:/Program Files/NVIDIA GPU Computing '
                              'Toolkit/CUDA/v%s' % _DEFAULT_CUDA_VERSION)
    _TF_OPENCL_VERSION = '1.2'
    _DEFAULT_COMPUTECPP_TOOLKIT_PATH = '/usr/local/computecpp'
    _DEFAULT_TRISYCL_INCLUDE_DIR = '/usr/local/triSYCL/include'
    _SUPPORTED_ANDROID_NDK_VERSIONS = [10, 11, 12, 13, 14, 15, 16, 17, 18]

    _DEFAULT_PROMPT_ASK_ATTEMPTS = 10

    _TF_BAZELRC_FILENAME = '.tf_configure.bazelrc'
    _TF_WORKSPACE_ROOT = ''
    _TF_BAZELRC = ''
    _TF_CURRENT_BAZEL_VERSION = None

    NCCL_LIB_PATHS = [
        'lib64/', 'lib/powerpc64le-linux-gnu/', 'lib/x86_64-linux-gnu/', ''
    ]

    # List of files to configure when building Bazel on Apple platforms.
    APPLE_BAZEL_FILES = [
        'tensorflow/lite/experimental/objc/BUILD',
        'tensorflow/lite/experimental/swift/BUILD'
    ]

    # List of files to move when building for iOS.
    IOS_FILES = [
        'tensorflow/lite/experimental/objc/TensorFlowLiteObjC.podspec',
        'tensorflow/lite/experimental/swift/TensorFlowLiteSwift.podspec',
    ]

    if platform.machine() == 'ppc64le':
      _DEFAULT_TENSORRT_PATH_LINUX = '/usr/lib/powerpc64le-linux-gnu/'
    else:
      _DEFAULT_TENSORRT_PATH_LINUX = '/usr/lib/%s-linux-gnu' % platform.machine()


    class UserInputError(Exception):
      pass


    def is_windows():
      return platform.system() == 'Windows'


    def is_linux():
      return platform.system() == 'Linux'


    def is_macos():
      return platform.system() == 'Darwin'


    def is_ppc64le():
      return platform.machine() == 'ppc64le'


    def is_cygwin():
      return platform.system().startswith('CYGWIN_NT')


    def get_input(question):
      try:
        try:
          answer = raw_input(question)
        except NameError:
          answer = input(question)  # pylint: disable=bad-builtin
      except EOFError:
        answer = ''
      return answer


    def symlink_force(target, link_name):
      """Force symlink, equivalent of 'ln -sf'.
      Args:
        target: items to link to.
        link_name: name of the link.
      """
      try:
        os.symlink(target, link_name)
      except OSError as e:
        if e.errno == errno.EEXIST:
          os.remove(link_name)
          os.symlink(target, link_name)
        else:
          raise e


    def sed_in_place(filename, old, new):
      """Replace old string with new string in file.
      Args:
        filename: string for filename.
        old: string to replace.
        new: new string to replace to.
      """
      with open(filename, 'r') as f:
        filedata = f.read()
      newdata = filedata.replace(old, new)
      with open(filename, 'w') as f:
        f.write(newdata)


    def write_to_bazelrc(line):
      with open(_TF_BAZELRC, 'a') as f:
        f.write(line + '\n')


    def write_action_env_to_bazelrc(var_name, var):
      write_to_bazelrc('build --action_env %s="%s"' % (var_name, str(var)))


    def run_shell(cmd, allow_non_zero=False):
      if allow_non_zero:
        try:
          output = subprocess.check_output(cmd)
        except subprocess.CalledProcessError as e:
          output = e.output
      else:
        output = subprocess.check_output(cmd)
      return output.decode('UTF-8').strip()


    def cygpath(path):
      """Convert path from posix to windows."""
      return os.path.abspath(path).replace('\\', '/')


    def get_python_path(environ_cp, python_bin_path):
      """Get the python site package paths."""
      python_paths = []
      if environ_cp.get('PYTHONPATH'):
        python_paths = environ_cp.get('PYTHONPATH').split(':')
      try:
        library_paths = run_shell([
            python_bin_path, '-c',
            'import site; print("\\n".join(site.getsitepackages()))'
        ]).split('\n')
      except subprocess.CalledProcessError:
        library_paths = [
            run_shell([
                python_bin_path, '-c',
                'from distutils.sysconfig import get_python_lib;'
                'print(get_python_lib())'
            ])
        ]

      all_paths = set(python_paths + library_paths)

      paths = []
      for path in all_paths:
        if os.path.isdir(path):
          paths.append(path)
      return paths


    def get_python_major_version(python_bin_path):
      """Get the python major version."""
      return run_shell([python_bin_path, '-c', 'import sys; print(sys.version[0])'])


    def setup_python(environ_cp):
      """Setup python related env variables."""
      # Get PYTHON_BIN_PATH, default is the current running python.
      default_python_bin_path = sys.executable
      ask_python_bin_path = ('Please specify the location of python. [Default is '
                             '%s]: ') % default_python_bin_path
      while True:
        python_bin_path = get_from_env_or_user_or_default(environ_cp,
                                                          'PYTHON_BIN_PATH',
                                                          ask_python_bin_path,
                                                          default_python_bin_path)
        # Check if the path is valid
        if os.path.isfile(python_bin_path) and os.access(python_bin_path, os.X_OK):
          break
        elif not os.path.exists(python_bin_path):
          print('Invalid python path: %s cannot be found.' % python_bin_path)
        else:
          print('%s is not executable.  Is it the python binary?' % python_bin_path)
        environ_cp['PYTHON_BIN_PATH'] = ''

      # Convert python path to Windows style before checking lib and version
      if is_windows() or is_cygwin():
        python_bin_path = cygpath(python_bin_path)

      # Get PYTHON_LIB_PATH
      python_lib_path = environ_cp.get('PYTHON_LIB_PATH')
      if not python_lib_path:
        python_lib_paths = get_python_path(environ_cp, python_bin_path)
        if environ_cp.get('USE_DEFAULT_PYTHON_LIB_PATH') == '1':
          python_lib_path = python_lib_paths[0]
        else:
          print('Found possible Python library paths:\n  %s' %
                '\n  '.join(python_lib_paths))
          default_python_lib_path = python_lib_paths[0]
          python_lib_path = get_input(
              'Please input the desired Python library path to use.  '
              'Default is [%s]\n' % python_lib_paths[0])
          if not python_lib_path:
            python_lib_path = default_python_lib_path
        environ_cp['PYTHON_LIB_PATH'] = python_lib_path

      _ = get_python_major_version(python_bin_path)

      # Convert python path to Windows style before writing into bazel.rc
      if is_windows() or is_cygwin():
        python_lib_path = cygpath(python_lib_path)

      # Set-up env variables used by python_configure.bzl
      write_action_env_to_bazelrc('PYTHON_BIN_PATH', python_bin_path)
      write_action_env_to_bazelrc('PYTHON_LIB_PATH', python_lib_path)
      write_to_bazelrc('build --python_path=\"%s"' % python_bin_path)
      environ_cp['PYTHON_BIN_PATH'] = python_bin_path

      # If choosen python_lib_path is from a path specified in the PYTHONPATH
      # variable, need to tell bazel to include PYTHONPATH
      if environ_cp.get('PYTHONPATH'):
        python_paths = environ_cp.get('PYTHONPATH').split(':')
        if python_lib_path in python_paths:
          write_action_env_to_bazelrc('PYTHONPATH', environ_cp.get('PYTHONPATH'))

      # Write tools/python_bin_path.sh
      with open(
          os.path.join(_TF_WORKSPACE_ROOT, 'tools', 'python_bin_path.sh'),
          'w') as f:
        f.write('export PYTHON_BIN_PATH="%s"' % python_bin_path)


    def reset_tf_configure_bazelrc():
      """Reset file that contains customized config settings."""
      open(_TF_BAZELRC, 'w').close()


    def cleanup_makefile():
      """Delete any leftover BUILD files from the Makefile build.
      These files could interfere with Bazel parsing.
      """
      makefile_download_dir = os.path.join(_TF_WORKSPACE_ROOT, 'tensorflow',
                                           'contrib', 'makefile', 'downloads')
      if os.path.isdir(makefile_download_dir):
        for root, _, filenames in os.walk(makefile_download_dir):
          for f in filenames:
            if f.endswith('BUILD'):
              os.remove(os.path.join(root, f))


    def get_var(environ_cp,
                var_name,
                query_item,
                enabled_by_default,
                question=None,
                yes_reply=None,
                no_reply=None):
      """Get boolean input from user.
      If var_name is not set in env, ask user to enable query_item or not. If the
      response is empty, use the default.
      Args:
        environ_cp: copy of the os.environ.
        var_name: string for name of environment variable, e.g. "TF_NEED_HDFS".
        query_item: string for feature related to the variable, e.g. "Hadoop File
          System".
        enabled_by_default: boolean for default behavior.
        question: optional string for how to ask for user input.
        yes_reply: optional string for reply when feature is enabled.
        no_reply: optional string for reply when feature is disabled.
      Returns:
        boolean value of the variable.
      Raises:
        UserInputError: if an environment variable is set, but it cannot be
          interpreted as a boolean indicator, assume that the user has made a
          scripting error, and will continue to provide invalid input.
          Raise the error to avoid infinitely looping.
      """
      if not question:
        question = 'Do you wish to build TensorFlow with %s support?' % query_item
      if not yes_reply:
        yes_reply = '%s support will be enabled for TensorFlow.' % query_item
      if not no_reply:
        no_reply = 'No %s' % yes_reply

      yes_reply += '\n'
      no_reply += '\n'

      if enabled_by_default:
        question += ' [Y/n]: '
      else:
        question += ' [y/N]: '

      var = environ_cp.get(var_name)
      if var is not None:
        var_content = var.strip().lower()
        true_strings = ('1', 't', 'true', 'y', 'yes')
        false_strings = ('0', 'f', 'false', 'n', 'no')
        if var_content in true_strings:
          var = True
        elif var_content in false_strings:
          var = False
        else:
          raise UserInputError(
              'Environment variable %s must be set as a boolean indicator.\n'
              'The following are accepted as TRUE : %s.\n'
              'The following are accepted as FALSE: %s.\n'
              'Current value is %s.' %
              (var_name, ', '.join(true_strings), ', '.join(false_strings), var))

      while var is None:
        user_input_origin = get_input(question)
        user_input = user_input_origin.strip().lower()
        if user_input == 'y':
          print(yes_reply)
          var = True
        elif user_input == 'n':
          print(no_reply)
          var = False
        elif not user_input:
          if enabled_by_default:
            print(yes_reply)
            var = True
          else:
            print(no_reply)
            var = False
        else:
          print('Invalid selection: %s' % user_input_origin)
      return var


    def set_build_var(environ_cp,
                      var_name,
                      query_item,
                      option_name,
                      enabled_by_default,
                      bazel_config_name=None):
      """Set if query_item will be enabled for the build.
      Ask user if query_item will be enabled. Default is used if no input is given.
      Set subprocess environment variable and write to .bazelrc if enabled.
      Args:
        environ_cp: copy of the os.environ.
        var_name: string for name of environment variable, e.g. "TF_NEED_HDFS".
        query_item: string for feature related to the variable, e.g. "Hadoop File
          System".
        option_name: string for option to define in .bazelrc.
        enabled_by_default: boolean for default behavior.
        bazel_config_name: Name for Bazel --config argument to enable build feature.
      """

      var = str(int(get_var(environ_cp, var_name, query_item, enabled_by_default)))
      environ_cp[var_name] = var
      if var == '1':
        write_to_bazelrc('build:%s --define %s=true' %
                         (bazel_config_name, option_name))
        write_to_bazelrc('build --config=%s' % bazel_config_name)
      elif bazel_config_name is not None:
        # TODO(mikecase): Migrate all users of configure.py to use --config Bazel
        # options and not to set build configs through environment variables.
        write_to_bazelrc('build:%s --define %s=true' %
                         (bazel_config_name, option_name))


    def set_action_env_var(environ_cp,
                           var_name,
                           query_item,
                           enabled_by_default,
                           question=None,
                           yes_reply=None,
                           no_reply=None):
      """Set boolean action_env variable.
      Ask user if query_item will be enabled. Default is used if no input is given.
      Set environment variable and write to .bazelrc.
      Args:
        environ_cp: copy of the os.environ.
        var_name: string for name of environment variable, e.g. "TF_NEED_HDFS".
        query_item: string for feature related to the variable, e.g. "Hadoop File
          System".
        enabled_by_default: boolean for default behavior.
        question: optional string for how to ask for user input.
        yes_reply: optional string for reply when feature is enabled.
        no_reply: optional string for reply when feature is disabled.
      """
      var = int(
          get_var(environ_cp, var_name, query_item, enabled_by_default, question,
                  yes_reply, no_reply))

      write_action_env_to_bazelrc(var_name, var)
      environ_cp[var_name] = str(var)


    def convert_version_to_int(version):
      """Convert a version number to a integer that can be used to compare.
      Version strings of the form X.YZ and X.Y.Z-xxxxx are supported. The
      'xxxxx' part, for instance 'homebrew' on OS/X, is ignored.
      Args:
        version: a version to be converted
      Returns:
        An integer if converted successfully, otherwise return None.
      """
      version = version.split('-')[0]
      version_segments = version.split('.')
      # Treat "0.24" as "0.24.0"
      if len(version_segments) == 2:
        version_segments.append('0')
      for seg in version_segments:
        if not seg.isdigit():
          return None

      version_str = ''.join(['%03d' % int(seg) for seg in version_segments])
      return int(version_str)


    def check_bazel_version(min_version, max_version):
      """Check installed bazel version is between min_version and max_version.
      Args:
        min_version: string for minimum bazel version.
        max_version: string for maximum bazel version.
      Returns:
        The bazel version detected.
      """
      if which('bazel') is None:
        print('Cannot find bazel. Please install bazel.')
        sys.exit(0)
      curr_version = run_shell(
          ['bazel', '--batch', '--bazelrc=/dev/null', 'version'])

      for line in curr_version.split('\n'):
        if 'Build label: ' in line:
          curr_version = line.split('Build label: ')[1]
          break

      min_version_int = convert_version_to_int(min_version)
      curr_version_int = convert_version_to_int(curr_version)
      max_version_int = convert_version_to_int(max_version)

      # Check if current bazel version can be detected properly.
      if not curr_version_int:
        print('WARNING: current bazel installation is not a release version.')
        print('Make sure you are running at least bazel %s' % min_version)
        return curr_version

      print('You have bazel %s installed.' % curr_version)

      if curr_version_int < min_version_int:
        print('Please upgrade your bazel installation to version %s or higher to '
              'build TensorFlow!' % min_version)
        sys.exit(1)
      if (curr_version_int > max_version_int and
          'TF_IGNORE_MAX_BAZEL_VERSION' not in os.environ):
        print('Please downgrade your bazel installation to version %s or lower to '
              'build TensorFlow! To downgrade: download the installer for the old '
              'version (from https://github.com/bazelbuild/bazel/releases) then '
              'run the installer.' % max_version)
        sys.exit(1)
      return curr_version


    def set_cc_opt_flags(environ_cp):
      """Set up architecture-dependent optimization flags.
      Also append CC optimization flags to bazel.rc..
      Args:
        environ_cp: copy of the os.environ.
      """
      if is_ppc64le():
        # gcc on ppc64le does not support -march, use mcpu instead
        default_cc_opt_flags = '-mcpu=native'
      elif is_windows():
        default_cc_opt_flags = '/arch:AVX'
      else:
        default_cc_opt_flags = '-march=native -Wno-sign-compare'
      question = ('Please specify optimization flags to use during compilation when'
                  ' bazel option "--config=opt" is specified [Default is %s]: '
                 ) % default_cc_opt_flags
      cc_opt_flags = get_from_env_or_user_or_default(environ_cp, 'CC_OPT_FLAGS',
                                                     question, default_cc_opt_flags)
      for opt in cc_opt_flags.split():
        write_to_bazelrc('build:opt --copt=%s' % opt)
      # It should be safe on the same build host.
      if not is_ppc64le() and not is_windows():
        write_to_bazelrc('build:opt --host_copt=-march=native')
      write_to_bazelrc('build:opt --define with_default_optimizations=true')


    def set_tf_cuda_clang(environ_cp):
      """set TF_CUDA_CLANG action_env.
      Args:
        environ_cp: copy of the os.environ.
      """
      question = 'Do you want to use clang as CUDA compiler?'
      yes_reply = 'Clang will be used as CUDA compiler.'
      no_reply = 'nvcc will be used as CUDA compiler.'
      set_action_env_var(
          environ_cp,
          'TF_CUDA_CLANG',
          None,
          False,
          question=question,
          yes_reply=yes_reply,
          no_reply=no_reply)


    def set_tf_download_clang(environ_cp):
      """Set TF_DOWNLOAD_CLANG action_env."""
      question = 'Do you wish to download a fresh release of clang? (Experimental)'
      yes_reply = 'Clang will be downloaded and used to compile tensorflow.'
      no_reply = 'Clang will not be downloaded.'
      set_action_env_var(
          environ_cp,
          'TF_DOWNLOAD_CLANG',
          None,
          False,
          question=question,
          yes_reply=yes_reply,
          no_reply=no_reply)


    def get_from_env_or_user_or_default(environ_cp, var_name, ask_for_var,
                                        var_default):
      """Get var_name either from env, or user or default.
      If var_name has been set as environment variable, use the preset value, else
      ask for user input. If no input is provided, the default is used.
      Args:
        environ_cp: copy of the os.environ.
        var_name: string for name of environment variable, e.g. "TF_NEED_HDFS".
        ask_for_var: string for how to ask for user input.
        var_default: default value string.
      Returns:
        string value for var_name
      """
      var = environ_cp.get(var_name)
      if not var:
        var = get_input(ask_for_var)
        print('\n')
      if not var:
        var = var_default
      return var


    def set_clang_cuda_compiler_path(environ_cp):
      """Set CLANG_CUDA_COMPILER_PATH."""
      default_clang_path = which('clang') or ''
      ask_clang_path = ('Please specify which clang should be used as device and '
                        'host compiler. [Default is %s]: ') % default_clang_path

      while True:
        clang_cuda_compiler_path = get_from_env_or_user_or_default(
            environ_cp, 'CLANG_CUDA_COMPILER_PATH', ask_clang_path,
            default_clang_path)
        if os.path.exists(clang_cuda_compiler_path):
          break

        # Reset and retry
        print('Invalid clang path: %s cannot be found.' % clang_cuda_compiler_path)
        environ_cp['CLANG_CUDA_COMPILER_PATH'] = ''

      # Set CLANG_CUDA_COMPILER_PATH
      environ_cp['CLANG_CUDA_COMPILER_PATH'] = clang_cuda_compiler_path
      write_action_env_to_bazelrc('CLANG_CUDA_COMPILER_PATH',
                                  clang_cuda_compiler_path)


    def prompt_loop_or_load_from_env(environ_cp,
                                     var_name,
                                     var_default,
                                     ask_for_var,
                                     check_success,
                                     error_msg,
                                     suppress_default_error=False,
                                     n_ask_attempts=_DEFAULT_PROMPT_ASK_ATTEMPTS):
      """Loop over user prompts for an ENV param until receiving a valid response.
      For the env param var_name, read from the environment or verify user input
      until receiving valid input. When done, set var_name in the environ_cp to its
      new value.
      Args:
        environ_cp: (Dict) copy of the os.environ.
        var_name: (String) string for name of environment variable, e.g. "TF_MYVAR".
        var_default: (String) default value string.
        ask_for_var: (String) string for how to ask for user input.
        check_success: (Function) function that takes one argument and returns a
          boolean. Should return True if the value provided is considered valid. May
          contain a complex error message if error_msg does not provide enough
          information. In that case, set suppress_default_error to True.
        error_msg: (String) String with one and only one '%s'. Formatted with each
          invalid response upon check_success(input) failure.
        suppress_default_error: (Bool) Suppress the above error message in favor of
          one from the check_success function.
        n_ask_attempts: (Integer) Number of times to query for valid input before
          raising an error and quitting.
      Returns:
        [String] The value of var_name after querying for input.
      Raises:
        UserInputError: if a query has been attempted n_ask_attempts times without
          success, assume that the user has made a scripting error, and will
          continue to provide invalid input. Raise the error to avoid infinitely
          looping.
      """
      default = environ_cp.get(var_name) or var_default
      full_query = '%s [Default is %s]: ' % (
          ask_for_var,
          default,
      )

      for _ in range(n_ask_attempts):
        val = get_from_env_or_user_or_default(environ_cp, var_name, full_query,
                                              default)
        if check_success(val):
          break
        if not suppress_default_error:
          print(error_msg % val)
        environ_cp[var_name] = ''
      else:
        raise UserInputError('Invalid %s setting was provided %d times in a row. '
                             'Assuming to be a scripting mistake.' %
                             (var_name, n_ask_attempts))

      environ_cp[var_name] = val
      return val


    def create_android_ndk_rule(environ_cp):
      """Set ANDROID_NDK_HOME and write Android NDK WORKSPACE rule."""
      if is_windows() or is_cygwin():
        default_ndk_path = cygpath('%s/Android/Sdk/ndk-bundle' %
                                   environ_cp['APPDATA'])
      elif is_macos():
        default_ndk_path = '%s/library/Android/Sdk/ndk-bundle' % environ_cp['HOME']
      else:
        default_ndk_path = '%s/Android/Sdk/ndk-bundle' % environ_cp['HOME']

      def valid_ndk_path(path):
        return (os.path.exists(path) and
                os.path.exists(os.path.join(path, 'source.properties')))

      android_ndk_home_path = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='ANDROID_NDK_HOME',
          var_default=default_ndk_path,
          ask_for_var='Please specify the home path of the Android NDK to use.',
          check_success=valid_ndk_path,
          error_msg=('The path %s or its child file "source.properties" '
                     'does not exist.'))
      write_action_env_to_bazelrc('ANDROID_NDK_HOME', android_ndk_home_path)
      write_action_env_to_bazelrc(
          'ANDROID_NDK_API_LEVEL',
          get_ndk_api_level(environ_cp, android_ndk_home_path))


    def create_android_sdk_rule(environ_cp):
      """Set Android variables and write Android SDK WORKSPACE rule."""
      if is_windows() or is_cygwin():
        default_sdk_path = cygpath('%s/Android/Sdk' % environ_cp['APPDATA'])
      elif is_macos():
        default_sdk_path = '%s/library/Android/Sdk' % environ_cp['HOME']
      else:
        default_sdk_path = '%s/Android/Sdk' % environ_cp['HOME']

      def valid_sdk_path(path):
        return (os.path.exists(path) and
                os.path.exists(os.path.join(path, 'platforms')) and
                os.path.exists(os.path.join(path, 'build-tools')))

      android_sdk_home_path = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='ANDROID_SDK_HOME',
          var_default=default_sdk_path,
          ask_for_var='Please specify the home path of the Android SDK to use.',
          check_success=valid_sdk_path,
          error_msg=('Either %s does not exist, or it does not contain the '
                     'subdirectories "platforms" and "build-tools".'))

      platforms = os.path.join(android_sdk_home_path, 'platforms')
      api_levels = sorted(os.listdir(platforms))
      api_levels = [x.replace('android-', '') for x in api_levels]

      def valid_api_level(api_level):
        return os.path.exists(
            os.path.join(android_sdk_home_path, 'platforms',
                         'android-' + api_level))

      android_api_level = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='ANDROID_API_LEVEL',
          var_default=api_levels[-1],
          ask_for_var=('Please specify the Android SDK API level to use. '
                       '[Available levels: %s]') % api_levels,
          check_success=valid_api_level,
          error_msg='Android-%s is not present in the SDK path.')

      build_tools = os.path.join(android_sdk_home_path, 'build-tools')
      versions = sorted(os.listdir(build_tools))

      def valid_build_tools(version):
        return os.path.exists(
            os.path.join(android_sdk_home_path, 'build-tools', version))

      android_build_tools_version = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='ANDROID_BUILD_TOOLS_VERSION',
          var_default=versions[-1],
          ask_for_var=('Please specify an Android build tools version to use. '
                       '[Available versions: %s]') % versions,
          check_success=valid_build_tools,
          error_msg=('The selected SDK does not have build-tools version %s '
                     'available.'))

      write_action_env_to_bazelrc('ANDROID_BUILD_TOOLS_VERSION',
                                  android_build_tools_version)
      write_action_env_to_bazelrc('ANDROID_SDK_API_LEVEL', android_api_level)
      write_action_env_to_bazelrc('ANDROID_SDK_HOME', android_sdk_home_path)


    def get_ndk_api_level(environ_cp, android_ndk_home_path):
      """Gets the appropriate NDK API level to use for the provided Android NDK path."""

      # First check to see if we're using a blessed version of the NDK.
      properties_path = '%s/source.properties' % android_ndk_home_path
      if is_windows() or is_cygwin():
        properties_path = cygpath(properties_path)
      with open(properties_path, 'r') as f:
        filedata = f.read()

      revision = re.search(r'Pkg.Revision = (\d+)', filedata)
      if revision:
        ndk_version = revision.group(1)
      else:
        raise Exception('Unable to parse NDK revision.')
      if int(ndk_version) not in _SUPPORTED_ANDROID_NDK_VERSIONS:
        print('WARNING: The NDK version in %s is %s, which is not '
              'supported by Bazel (officially supported versions: %s). Please use '
              'another version. Compiling Android targets may result in confusing '
              'errors.\n' % (android_ndk_home_path, ndk_version,
                             _SUPPORTED_ANDROID_NDK_VERSIONS))

      # Now grab the NDK API level to use. Note that this is different from the
      # SDK API level, as the NDK API level is effectively the *min* target SDK
      # version.
      platforms = os.path.join(android_ndk_home_path, 'platforms')
      api_levels = sorted(os.listdir(platforms))
      api_levels = [
          x.replace('android-', '') for x in api_levels if 'android-' in x
      ]

      def valid_api_level(api_level):
        return os.path.exists(
            os.path.join(android_ndk_home_path, 'platforms',
                         'android-' + api_level))

      android_ndk_api_level = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='ANDROID_NDK_API_LEVEL',
          var_default='18',  # 18 is required for GPU acceleration.
          ask_for_var=('Please specify the (min) Android NDK API level to use. '
                       '[Available levels: %s]') % api_levels,
          check_success=valid_api_level,
          error_msg='Android-%s is not present in the NDK path.')

      return android_ndk_api_level


    def set_gcc_host_compiler_path(environ_cp):
      """Set GCC_HOST_COMPILER_PATH."""
      default_gcc_host_compiler_path = which('gcc') or ''
      cuda_bin_symlink = '%s/bin/gcc' % environ_cp.get('CUDA_TOOLKIT_PATH')

      if os.path.islink(cuda_bin_symlink):
        # os.readlink is only available in linux
        default_gcc_host_compiler_path = os.path.realpath(cuda_bin_symlink)

      gcc_host_compiler_path = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='GCC_HOST_COMPILER_PATH',
          var_default=default_gcc_host_compiler_path,
          ask_for_var='Please specify which gcc should be used by nvcc as the host compiler.',
          check_success=os.path.exists,
          error_msg='Invalid gcc path. %s cannot be found.',
      )

      write_action_env_to_bazelrc('GCC_HOST_COMPILER_PATH', gcc_host_compiler_path)


    def reformat_version_sequence(version_str, sequence_count):
      """Reformat the version string to have the given number of sequences.
      For example:
      Given (7, 2) -> 7.0
            (7.0.1, 2) -> 7.0
            (5, 1) -> 5
            (5.0.3.2, 1) -> 5
      Args:
          version_str: String, the version string.
          sequence_count: int, an integer.
      Returns:
          string, reformatted version string.
      """
      v = version_str.split('.')
      if len(v) < sequence_count:
        v = v + (['0'] * (sequence_count - len(v)))

      return '.'.join(v[:sequence_count])


    def set_tf_cuda_version(environ_cp):
      """Set CUDA_TOOLKIT_PATH and TF_CUDA_VERSION."""
      ask_cuda_version = (
          'Please specify the CUDA SDK version you want to use. '
          '[Leave empty to default to CUDA %s]: ') % _DEFAULT_CUDA_VERSION

      for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
        # Configure the Cuda SDK version to use.
        tf_cuda_version = get_from_env_or_user_or_default(environ_cp,
                                                          'TF_CUDA_VERSION',
                                                          ask_cuda_version,
                                                          _DEFAULT_CUDA_VERSION)
        tf_cuda_version = reformat_version_sequence(str(tf_cuda_version), 2)

        # Find out where the CUDA toolkit is installed
        default_cuda_path = _DEFAULT_CUDA_PATH
        if is_windows() or is_cygwin():
          default_cuda_path = cygpath(
              environ_cp.get('CUDA_PATH', _DEFAULT_CUDA_PATH_WIN))
        elif is_linux():
          # If the default doesn't exist, try an alternative default.
          if (not os.path.exists(default_cuda_path)
             ) and os.path.exists(_DEFAULT_CUDA_PATH_LINUX):
            default_cuda_path = _DEFAULT_CUDA_PATH_LINUX
        ask_cuda_path = ('Please specify the location where CUDA %s toolkit is'
                         ' installed. Refer to README.md for more details. '
                         '[Default is %s]: ') % (tf_cuda_version, default_cuda_path)
        cuda_toolkit_path = get_from_env_or_user_or_default(environ_cp,
                                                            'CUDA_TOOLKIT_PATH',
                                                            ask_cuda_path,
                                                            default_cuda_path)
        if is_windows() or is_cygwin():
          cuda_toolkit_path = cygpath(cuda_toolkit_path)

        if is_windows():
          cuda_rt_lib_paths = ['lib/x64/cudart.lib']
        elif is_linux():
          cuda_rt_lib_paths = [
              '%s/libcudart.so.%s' % (x, tf_cuda_version) for x in [
                  'lib64',
                  'lib/powerpc64le-linux-gnu',
                  'lib/x86_64-linux-gnu',
              ]
          ]
        elif is_macos():
          cuda_rt_lib_paths = ['lib/libcudart.%s.dylib' % tf_cuda_version]

        cuda_toolkit_paths_full = [
            os.path.join(cuda_toolkit_path, x) for x in cuda_rt_lib_paths
        ]
        if any(os.path.exists(x) for x in cuda_toolkit_paths_full):
          break

        # Reset and retry
        print('Invalid path to CUDA %s toolkit. %s cannot be found' %
              (tf_cuda_version, cuda_toolkit_paths_full))
        environ_cp['TF_CUDA_VERSION'] = ''
        environ_cp['CUDA_TOOLKIT_PATH'] = ''

      else:
        raise UserInputError('Invalid TF_CUDA_SETTING setting was provided %d '
                             'times in a row. Assuming to be a scripting mistake.' %
                             _DEFAULT_PROMPT_ASK_ATTEMPTS)

      # Set CUDA_TOOLKIT_PATH and TF_CUDA_VERSION
      environ_cp['CUDA_TOOLKIT_PATH'] = cuda_toolkit_path
      write_action_env_to_bazelrc('CUDA_TOOLKIT_PATH', cuda_toolkit_path)
      environ_cp['TF_CUDA_VERSION'] = tf_cuda_version
      write_action_env_to_bazelrc('TF_CUDA_VERSION', tf_cuda_version)


    def set_tf_cudnn_version(environ_cp):
      """Set CUDNN_INSTALL_PATH and TF_CUDNN_VERSION."""
      ask_cudnn_version = (
          'Please specify the cuDNN version you want to use. '
          '[Leave empty to default to cuDNN %s]: ') % _DEFAULT_CUDNN_VERSION

      for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
        tf_cudnn_version = get_from_env_or_user_or_default(environ_cp,
                                                           'TF_CUDNN_VERSION',
                                                           ask_cudnn_version,
                                                           _DEFAULT_CUDNN_VERSION)
        tf_cudnn_version = reformat_version_sequence(str(tf_cudnn_version), 1)

        default_cudnn_path = environ_cp.get('CUDA_TOOLKIT_PATH')
        ask_cudnn_path = (r'Please specify the location where cuDNN %s library is '
                          'installed. Refer to README.md for more details. [Default'
                          ' is %s]: ') % (tf_cudnn_version, default_cudnn_path)
        cudnn_install_path = get_from_env_or_user_or_default(
            environ_cp, 'CUDNN_INSTALL_PATH', ask_cudnn_path, default_cudnn_path)

        # Result returned from "read" will be used unexpanded. That make "~"
        # unusable. Going through one more level of expansion to handle that.
        cudnn_install_path = os.path.realpath(
            os.path.expanduser(cudnn_install_path))
        if is_windows() or is_cygwin():
          cudnn_install_path = cygpath(cudnn_install_path)

        if is_windows():
          cuda_dnn_lib_path = 'lib/x64/cudnn.lib'
          cuda_dnn_lib_alt_path = 'lib/x64/cudnn.lib'
        elif is_linux():
          cuda_dnn_lib_path = 'lib64/libcudnn.so.%s' % tf_cudnn_version
          cuda_dnn_lib_alt_path = 'libcudnn.so.%s' % tf_cudnn_version
        elif is_macos():
          cuda_dnn_lib_path = 'lib/libcudnn.%s.dylib' % tf_cudnn_version
          cuda_dnn_lib_alt_path = 'libcudnn.%s.dylib' % tf_cudnn_version

        cuda_dnn_lib_path_full = os.path.join(cudnn_install_path, cuda_dnn_lib_path)
        cuda_dnn_lib_alt_path_full = os.path.join(cudnn_install_path,
                                                  cuda_dnn_lib_alt_path)
        if os.path.exists(cuda_dnn_lib_path_full) or os.path.exists(
            cuda_dnn_lib_alt_path_full):
          break

        # Try another alternative for Linux
        if is_linux():
          ldconfig_bin = which('ldconfig') or '/sbin/ldconfig'
          cudnn_path_from_ldconfig = run_shell([ldconfig_bin, '-p'])
          cudnn_path_from_ldconfig = re.search('.*libcudnn.so .* => (.*)',
                                               cudnn_path_from_ldconfig)
          if cudnn_path_from_ldconfig:
            cudnn_path_from_ldconfig = cudnn_path_from_ldconfig.group(1)
            if os.path.exists('%s.%s' %
                              (cudnn_path_from_ldconfig, tf_cudnn_version)):
              cudnn_install_path = os.path.dirname(cudnn_path_from_ldconfig)
              break

        # Reset and Retry
        print(
            'Invalid path to cuDNN %s toolkit. None of the following files can be '
            'found:' % tf_cudnn_version)
        print(cuda_dnn_lib_path_full)
        print(cuda_dnn_lib_alt_path_full)
        if is_linux():
          print('%s.%s' % (cudnn_path_from_ldconfig, tf_cudnn_version))

        environ_cp['TF_CUDNN_VERSION'] = ''
      else:
        raise UserInputError('Invalid TF_CUDNN setting was provided %d '
                             'times in a row. Assuming to be a scripting mistake.' %
                             _DEFAULT_PROMPT_ASK_ATTEMPTS)

      # Set CUDNN_INSTALL_PATH and TF_CUDNN_VERSION
      environ_cp['CUDNN_INSTALL_PATH'] = cudnn_install_path
      write_action_env_to_bazelrc('CUDNN_INSTALL_PATH', cudnn_install_path)
      environ_cp['TF_CUDNN_VERSION'] = tf_cudnn_version
      write_action_env_to_bazelrc('TF_CUDNN_VERSION', tf_cudnn_version)


    def is_cuda_compatible(lib, cuda_ver, cudnn_ver):
      """Check compatibility between given library and cudnn/cudart libraries."""
      ldd_bin = which('ldd') or '/usr/bin/ldd'
      ldd_out = run_shell([ldd_bin, lib], True)
      ldd_out = ldd_out.split(os.linesep)
      cudnn_pattern = re.compile('.*libcudnn.so\\.?(.*) =>.*$')
      cuda_pattern = re.compile('.*libcudart.so\\.?(.*) =>.*$')
      cudnn = None
      cudart = None
      cudnn_ok = True  # assume no cudnn dependency by default
      cuda_ok = True  # assume no cuda dependency by default
      for line in ldd_out:
        if 'libcudnn.so' in line:
          cudnn = cudnn_pattern.search(line)
          cudnn_ok = False
        elif 'libcudart.so' in line:
          cudart = cuda_pattern.search(line)
          cuda_ok = False
      if cudnn and len(cudnn.group(1)):
        cudnn = convert_version_to_int(cudnn.group(1))
      if cudart and len(cudart.group(1)):
        cudart = convert_version_to_int(cudart.group(1))
      if cudnn is not None:
        cudnn_ok = (cudnn == cudnn_ver)
      if cudart is not None:
        cuda_ok = (cudart == cuda_ver)
      return cudnn_ok and cuda_ok


    def set_tf_tensorrt_install_path(environ_cp):
      """Set TENSORRT_INSTALL_PATH and TF_TENSORRT_VERSION.
      Adapted from code contributed by Sami Kama (https://github.com/samikama).
      Args:
        environ_cp: copy of the os.environ.
      Raises:
        ValueError: if this method was called under non-Linux platform.
        UserInputError: if user has provided invalid input multiple times.
      """
      if not is_linux():
        raise ValueError('Currently TensorRT is only supported on Linux platform.')

      # Ask user whether to add TensorRT support.
      if str(int(get_var(environ_cp, 'TF_NEED_TENSORRT', 'TensorRT',
                         False))) != '1':
        return

      for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
        ask_tensorrt_path = (r'Please specify the location where TensorRT is '
                             'installed. [Default is %s]:') % (
                                 _DEFAULT_TENSORRT_PATH_LINUX)
        trt_install_path = get_from_env_or_user_or_default(
            environ_cp, 'TENSORRT_INSTALL_PATH', ask_tensorrt_path,
            _DEFAULT_TENSORRT_PATH_LINUX)

        # Result returned from "read" will be used unexpanded. That make "~"
        # unusable. Going through one more level of expansion to handle that.
        trt_install_path = os.path.realpath(os.path.expanduser(trt_install_path))

        def find_libs(search_path):
          """Search for libnvinfer.so in "search_path"."""
          fl = set()
          if os.path.exists(search_path) and os.path.isdir(search_path):
            fl.update([
                os.path.realpath(os.path.join(search_path, x))
                for x in os.listdir(search_path)
                if 'libnvinfer.so' in x
            ])
          return fl

        possible_files = find_libs(trt_install_path)
        possible_files.update(find_libs(os.path.join(trt_install_path, 'lib')))
        possible_files.update(find_libs(os.path.join(trt_install_path, 'lib64')))
        cuda_ver = convert_version_to_int(environ_cp['TF_CUDA_VERSION'])
        cudnn_ver = convert_version_to_int(environ_cp['TF_CUDNN_VERSION'])
        nvinfer_pattern = re.compile('.*libnvinfer.so.?(.*)$')
        highest_ver = [0, None, None]

        for lib_file in possible_files:
          if is_cuda_compatible(lib_file, cuda_ver, cudnn_ver):
            matches = nvinfer_pattern.search(lib_file)
            if not matches.groups():
              continue
            ver_str = matches.group(1)
            ver = convert_version_to_int(ver_str) if len(ver_str) else 0
            if ver > highest_ver[0]:
              highest_ver = [ver, ver_str, lib_file]
        if highest_ver[1] is not None:
          trt_install_path = os.path.dirname(highest_ver[2])
          tf_tensorrt_version = highest_ver[1]
          break

        # Try another alternative from ldconfig.
        ldconfig_bin = which('ldconfig') or '/sbin/ldconfig'
        ldconfig_output = run_shell([ldconfig_bin, '-p'])
        search_result = re.search('.*libnvinfer.so\\.?([0-9.]*).* => (.*)',
                                  ldconfig_output)
        if search_result:
          libnvinfer_path_from_ldconfig = search_result.group(2)
          if os.path.exists(libnvinfer_path_from_ldconfig):
            if is_cuda_compatible(libnvinfer_path_from_ldconfig, cuda_ver,
                                  cudnn_ver):
              trt_install_path = os.path.dirname(libnvinfer_path_from_ldconfig)
              tf_tensorrt_version = search_result.group(1)
              break

        # Reset and Retry
        if possible_files:
          print('TensorRT libraries found in one the following directories',
                'are not compatible with selected cuda and cudnn installations')
          print(trt_install_path)
          print(os.path.join(trt_install_path, 'lib'))
          print(os.path.join(trt_install_path, 'lib64'))
          if search_result:
            print(libnvinfer_path_from_ldconfig)
        else:
          print(
              'Invalid path to TensorRT. None of the following files can be found:')
          print(trt_install_path)
          print(os.path.join(trt_install_path, 'lib'))
          print(os.path.join(trt_install_path, 'lib64'))
          if search_result:
            print(libnvinfer_path_from_ldconfig)

      else:
        raise UserInputError('Invalid TF_TENSORRT setting was provided %d '
                             'times in a row. Assuming to be a scripting mistake.' %
                             _DEFAULT_PROMPT_ASK_ATTEMPTS)

      # Set TENSORRT_INSTALL_PATH and TF_TENSORRT_VERSION
      environ_cp['TENSORRT_INSTALL_PATH'] = trt_install_path
      write_action_env_to_bazelrc('TENSORRT_INSTALL_PATH', trt_install_path)
      environ_cp['TF_TENSORRT_VERSION'] = tf_tensorrt_version
      write_action_env_to_bazelrc('TF_TENSORRT_VERSION', tf_tensorrt_version)


    def set_tf_nccl_install_path(environ_cp):
      """Set NCCL_INSTALL_PATH, NCCL_HDR_PATH and TF_NCCL_VERSION.
      Args:
        environ_cp: copy of the os.environ.
      Raises:
        ValueError: if this method was called under non-Linux platform.
        UserInputError: if user has provided invalid input multiple times.
      """
      if not is_linux():
        raise ValueError('Currently NCCL is only supported on Linux platforms.')

      ask_nccl_version = (
          'Please specify the locally installed NCCL version you want to use. '
          '[Default is to use https://github.com/nvidia/nccl]: ')

      for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
        tf_nccl_version = get_from_env_or_user_or_default(environ_cp,
                                                          'TF_NCCL_VERSION',
                                                          ask_nccl_version, '')

        if not tf_nccl_version:
          break  # No need to get install path, building the open source code.

        tf_nccl_version = reformat_version_sequence(str(tf_nccl_version), 1)

        # Look with ldconfig first if we can find the library in paths
        # like /usr/lib/x86_64-linux-gnu and the header file in the corresponding
        # include directory. This is where the NCCL .deb packages install them.

        # First check to see if NCCL is in the ldconfig.
        # If its found, use that location.
        if is_linux():
          ldconfig_bin = which('ldconfig') or '/sbin/ldconfig'
          nccl2_path_from_ldconfig = run_shell([ldconfig_bin, '-p'])
          nccl2_path_from_ldconfig = re.search('.*libnccl.so .* => (.*)',
                                               nccl2_path_from_ldconfig)
        if nccl2_path_from_ldconfig:
          nccl2_path_from_ldconfig = nccl2_path_from_ldconfig.group(1)
          if os.path.exists('%s.%s' % (nccl2_path_from_ldconfig, tf_nccl_version)):
            nccl_install_path = os.path.dirname(nccl2_path_from_ldconfig)
            print('NCCL libraries found in ' + nccl2_path_from_ldconfig)

            # Check if this is the main system lib location
            if re.search('.*linux-gnu', nccl_install_path):
              trunc_nccl_install_path = '/usr'
              print('This looks like a system path.')
            else:
              trunc_nccl_install_path = nccl_install_path + '/..'

            # Look for header
            nccl_hdr_path = trunc_nccl_install_path + '/include'
            print('Assuming NCCL header path is ' + nccl_hdr_path)
            if os.path.exists(nccl_hdr_path + '/nccl.h'):
              # Set NCCL_INSTALL_PATH
              environ_cp['NCCL_INSTALL_PATH'] = nccl_install_path
              write_action_env_to_bazelrc('NCCL_INSTALL_PATH', nccl_install_path)

              # Set NCCL_HDR_PATH
              environ_cp['NCCL_HDR_PATH'] = nccl_hdr_path
              write_action_env_to_bazelrc('NCCL_HDR_PATH', nccl_hdr_path)
              break
            else:
              print(
                  'The header for NCCL2 cannot be found. Please install the libnccl-dev package.'
              )
          else:
            print('NCCL2 is listed by ldconfig but the library is not found. '
                  'Your ldconfig is out of date. Please run sudo ldconfig.')
        else:
          # NCCL is not found in ldconfig. Ask the user for the location.
          default_nccl_path = environ_cp.get('CUDA_TOOLKIT_PATH')
          ask_nccl_path = (
              r'Please specify the location where NCCL %s library is '
              'installed. Refer to README.md for more details. [Default '
              'is %s]:') % (tf_nccl_version, default_nccl_path)
          nccl_install_path = get_from_env_or_user_or_default(
              environ_cp, 'NCCL_INSTALL_PATH', ask_nccl_path, default_nccl_path)

          # Result returned from "read" will be used unexpanded. That make "~"
          # unusable. Going through one more level of expansion to handle that.
          nccl_install_path = os.path.realpath(
              os.path.expanduser(nccl_install_path))
          if is_windows() or is_cygwin():
            nccl_install_path = cygpath(nccl_install_path)

          nccl_lib_path = ''
          if is_windows():
            nccl_lib_path = 'lib/x64/nccl.lib'
          elif is_linux():
            nccl_lib_filename = 'libnccl.so.%s' % tf_nccl_version
            nccl_lpath = '%s/lib/%s' % (nccl_install_path, nccl_lib_filename)
            if not os.path.exists(nccl_lpath):
              for relative_path in NCCL_LIB_PATHS:
                path = '%s/%s%s' % (nccl_install_path, relative_path,
                                    nccl_lib_filename)
                if os.path.exists(path):
                  print('NCCL found at ' + path)
                  nccl_lib_path = path
                  break
            else:
              nccl_lib_path = nccl_lpath
          elif is_macos():
            nccl_lib_path = 'lib/libnccl.%s.dylib' % tf_nccl_version

          nccl_lib_path = os.path.join(nccl_install_path, nccl_lib_path)
          nccl_hdr_path = os.path.join(
              os.path.dirname(nccl_lib_path), '../include/nccl.h')
          print('Assuming NCCL header path is ' + nccl_hdr_path)
          if os.path.exists(nccl_lib_path) and os.path.exists(nccl_hdr_path):
            # Set NCCL_INSTALL_PATH
            environ_cp['NCCL_INSTALL_PATH'] = os.path.dirname(nccl_lib_path)
            write_action_env_to_bazelrc('NCCL_INSTALL_PATH',
                                        os.path.dirname(nccl_lib_path))

            # Set NCCL_HDR_PATH
            environ_cp['NCCL_HDR_PATH'] = os.path.dirname(nccl_hdr_path)
            write_action_env_to_bazelrc('NCCL_HDR_PATH',
                                        os.path.dirname(nccl_hdr_path))
            break

          # Reset and Retry
          print(
              'Invalid path to NCCL %s toolkit, %s or %s not found. Please use the '
              'O/S agnostic package of NCCL 2' %
              (tf_nccl_version, nccl_lib_path, nccl_hdr_path))

          environ_cp['TF_NCCL_VERSION'] = ''
      else:
        raise UserInputError('Invalid TF_NCCL setting was provided %d '
                             'times in a row. Assuming to be a scripting mistake.' %
                             _DEFAULT_PROMPT_ASK_ATTEMPTS)

      # Set TF_NCCL_VERSION
      environ_cp['TF_NCCL_VERSION'] = tf_nccl_version
      write_action_env_to_bazelrc('TF_NCCL_VERSION', tf_nccl_version)


    def get_native_cuda_compute_capabilities(environ_cp):
      """Get native cuda compute capabilities.
      Args:
        environ_cp: copy of the os.environ.
      Returns:
        string of native cuda compute capabilities, separated by comma.
      """
      device_query_bin = os.path.join(
          environ_cp.get('CUDA_TOOLKIT_PATH'), 'extras/demo_suite/deviceQuery')
      if os.path.isfile(device_query_bin) and os.access(device_query_bin, os.X_OK):
        try:
          output = run_shell(device_query_bin).split('\n')
          pattern = re.compile('[0-9]*\\.[0-9]*')
          output = [pattern.search(x) for x in output if 'Capability' in x]
          output = ','.join(x.group() for x in output if x is not None)
        except subprocess.CalledProcessError:
          output = ''
      else:
        output = ''
      return output


    def set_tf_cuda_compute_capabilities(environ_cp):
      """Set TF_CUDA_COMPUTE_CAPABILITIES."""
      while True:
        native_cuda_compute_capabilities = get_native_cuda_compute_capabilities(
            environ_cp)
        if not native_cuda_compute_capabilities:
          default_cuda_compute_capabilities = _DEFAULT_CUDA_COMPUTE_CAPABILITIES
        else:
          default_cuda_compute_capabilities = native_cuda_compute_capabilities

        ask_cuda_compute_capabilities = (
            'Please specify a list of comma-separated '
            'CUDA compute capabilities you want to '
            'build with.\nYou can find the compute '
            'capability of your device at: '
            'https://developer.nvidia.com/cuda-gpus.\nPlease'
            ' note that each additional compute '
            'capability significantly increases your '
            'build time and binary size, and that '
            'TensorFlow only supports compute '
            'capabilities >= 3.5 [Default is: %s]: ' %
            default_cuda_compute_capabilities)
        tf_cuda_compute_capabilities = get_from_env_or_user_or_default(
            environ_cp, 'TF_CUDA_COMPUTE_CAPABILITIES',
            ask_cuda_compute_capabilities, default_cuda_compute_capabilities)
        # Check whether all capabilities from the input is valid
        all_valid = True
        # Remove all whitespace characters before splitting the string
        # that users may insert by accident, as this will result in error
        tf_cuda_compute_capabilities = ''.join(tf_cuda_compute_capabilities.split())
        for compute_capability in tf_cuda_compute_capabilities.split(','):
          m = re.match('[0-9]+.[0-9]+', compute_capability)
          if not m:
            print('Invalid compute capability: %s' % compute_capability)
            all_valid = False
          else:
            ver = float(m.group(0))
            if ver < 3.0:
              print('ERROR: TensorFlow only supports CUDA compute capabilities 3.0 '
                    'and higher. Please re-specify the list of compute '
                    'capabilities excluding version %s.' % ver)
              all_valid = False
            if ver < 3.5:
              print('WARNING: XLA does not support CUDA compute capabilities '
                    'lower than 3.5. Disable XLA when running on older GPUs.')

        if all_valid:
          break

        # Reset and Retry
        environ_cp['TF_CUDA_COMPUTE_CAPABILITIES'] = ''

      # Set TF_CUDA_COMPUTE_CAPABILITIES
      environ_cp['TF_CUDA_COMPUTE_CAPABILITIES'] = tf_cuda_compute_capabilities
      write_action_env_to_bazelrc('TF_CUDA_COMPUTE_CAPABILITIES',
                                  tf_cuda_compute_capabilities)


    def set_other_cuda_vars(environ_cp):
      """Set other CUDA related variables."""
      # If CUDA is enabled, always use GPU during build and test.
      if environ_cp.get('TF_CUDA_CLANG') == '1':
        write_to_bazelrc('build --config=cuda_clang')
        write_to_bazelrc('test --config=cuda_clang')
      else:
        write_to_bazelrc('build --config=cuda')
        write_to_bazelrc('test --config=cuda')


    def set_host_cxx_compiler(environ_cp):
      """Set HOST_CXX_COMPILER."""
      default_cxx_host_compiler = which('g++') or ''

      host_cxx_compiler = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='HOST_CXX_COMPILER',
          var_default=default_cxx_host_compiler,
          ask_for_var=('Please specify which C++ compiler should be used as the '
                       'host C++ compiler.'),
          check_success=os.path.exists,
          error_msg='Invalid C++ compiler path. %s cannot be found.',
      )

      write_action_env_to_bazelrc('HOST_CXX_COMPILER', host_cxx_compiler)


    def set_host_c_compiler(environ_cp):
      """Set HOST_C_COMPILER."""
      default_c_host_compiler = which('gcc') or ''

      host_c_compiler = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='HOST_C_COMPILER',
          var_default=default_c_host_compiler,
          ask_for_var=('Please specify which C compiler should be used as the host '
                       'C compiler.'),
          check_success=os.path.exists,
          error_msg='Invalid C compiler path. %s cannot be found.',
      )

      write_action_env_to_bazelrc('HOST_C_COMPILER', host_c_compiler)


    def set_computecpp_toolkit_path(environ_cp):
      """Set COMPUTECPP_TOOLKIT_PATH."""

      def toolkit_exists(toolkit_path):
        """Check if a computecpp toolkit path is valid."""
        if is_linux():
          sycl_rt_lib_path = 'lib/libComputeCpp.so'
        else:
          sycl_rt_lib_path = ''

        sycl_rt_lib_path_full = os.path.join(toolkit_path, sycl_rt_lib_path)
        exists = os.path.exists(sycl_rt_lib_path_full)
        if not exists:
          print('Invalid SYCL %s library path. %s cannot be found' %
                (_TF_OPENCL_VERSION, sycl_rt_lib_path_full))
        return exists

      computecpp_toolkit_path = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='COMPUTECPP_TOOLKIT_PATH',
          var_default=_DEFAULT_COMPUTECPP_TOOLKIT_PATH,
          ask_for_var=(
              'Please specify the location where ComputeCpp for SYCL %s is '
              'installed.' % _TF_OPENCL_VERSION),
          check_success=toolkit_exists,
          error_msg='Invalid SYCL compiler path. %s cannot be found.',
          suppress_default_error=True)

      write_action_env_to_bazelrc('COMPUTECPP_TOOLKIT_PATH',
                                  computecpp_toolkit_path)


    def set_trisycl_include_dir(environ_cp):
      """Set TRISYCL_INCLUDE_DIR."""

      ask_trisycl_include_dir = ('Please specify the location of the triSYCL '
                                 'include directory. (Use --config=sycl_trisycl '
                                 'when building with Bazel) '
                                 '[Default is %s]: ') % (
                                     _DEFAULT_TRISYCL_INCLUDE_DIR)

      while True:
        trisycl_include_dir = get_from_env_or_user_or_default(
            environ_cp, 'TRISYCL_INCLUDE_DIR', ask_trisycl_include_dir,
            _DEFAULT_TRISYCL_INCLUDE_DIR)
        if os.path.exists(trisycl_include_dir):
          break

        print('Invalid triSYCL include directory, %s cannot be found' %
              (trisycl_include_dir))

      # Set TRISYCL_INCLUDE_DIR
      environ_cp['TRISYCL_INCLUDE_DIR'] = trisycl_include_dir
      write_action_env_to_bazelrc('TRISYCL_INCLUDE_DIR', trisycl_include_dir)


    def set_mpi_home(environ_cp):
      """Set MPI_HOME."""

      default_mpi_home = which('mpirun') or which('mpiexec') or ''
      default_mpi_home = os.path.dirname(os.path.dirname(default_mpi_home))

      def valid_mpi_path(mpi_home):
        exists = (
            os.path.exists(os.path.join(mpi_home, 'include')) and
            (os.path.exists(os.path.join(mpi_home, 'lib')) or
             os.path.exists(os.path.join(mpi_home, 'lib64')) or
             os.path.exists(os.path.join(mpi_home, 'lib32'))))
        if not exists:
          print(
              'Invalid path to the MPI Toolkit. %s or %s or %s or %s cannot be found'
              % (os.path.join(mpi_home, 'include'),
                 os.path.exists(os.path.join(mpi_home, 'lib')),
                 os.path.exists(os.path.join(mpi_home, 'lib64')),
                 os.path.exists(os.path.join(mpi_home, 'lib32'))))
        return exists

      _ = prompt_loop_or_load_from_env(
          environ_cp,
          var_name='MPI_HOME',
          var_default=default_mpi_home,
          ask_for_var='Please specify the MPI toolkit folder.',
          check_success=valid_mpi_path,
          error_msg='',
          suppress_default_error=True)


    def set_other_mpi_vars(environ_cp):
      """Set other MPI related variables."""
      # Link the MPI header files
      mpi_home = environ_cp.get('MPI_HOME')
      symlink_force('%s/include/mpi.h' % mpi_home, 'third_party/mpi/mpi.h')

      # Determine if we use OpenMPI or MVAPICH, these require different header files
      # to be included here to make bazel dependency checker happy
      if os.path.exists(os.path.join(mpi_home, 'include/mpi_portable_platform.h')):
        symlink_force(
            os.path.join(mpi_home, 'include/mpi_portable_platform.h'),
            'third_party/mpi/mpi_portable_platform.h')
        # TODO(gunan): avoid editing files in configure
        sed_in_place('third_party/mpi/mpi.bzl', 'MPI_LIB_IS_OPENMPI=False',
                     'MPI_LIB_IS_OPENMPI=True')
      else:
        # MVAPICH / MPICH
        symlink_force(
            os.path.join(mpi_home, 'include/mpio.h'), 'third_party/mpi/mpio.h')
        symlink_force(
            os.path.join(mpi_home, 'include/mpicxx.h'), 'third_party/mpi/mpicxx.h')
        # TODO(gunan): avoid editing files in configure
        sed_in_place('third_party/mpi/mpi.bzl', 'MPI_LIB_IS_OPENMPI=True',
                     'MPI_LIB_IS_OPENMPI=False')

      if os.path.exists(os.path.join(mpi_home, 'lib/libmpi.so')):
        symlink_force(
            os.path.join(mpi_home, 'lib/libmpi.so'), 'third_party/mpi/libmpi.so')
      elif os.path.exists(os.path.join(mpi_home, 'lib64/libmpi.so')):
        symlink_force(
            os.path.join(mpi_home, 'lib64/libmpi.so'), 'third_party/mpi/libmpi.so')
      elif os.path.exists(os.path.join(mpi_home, 'lib32/libmpi.so')):
        symlink_force(
            os.path.join(mpi_home, 'lib32/libmpi.so'), 'third_party/mpi/libmpi.so')

      else:
        raise ValueError(
            'Cannot find the MPI library file in %s/lib or %s/lib64 or %s/lib32' %
            (mpi_home, mpi_home, mpi_home))


    def system_specific_test_config(env):
      """Add default build and test flags required for TF tests to bazelrc."""
      write_to_bazelrc('test --flaky_test_attempts=3')
      write_to_bazelrc('test --test_size_filters=small,medium')
      write_to_bazelrc(
          'test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial')
      write_to_bazelrc('test --build_tag_filters=-benchmark-test,-no_oss')
      if is_windows():
        if env.get('TF_NEED_CUDA', None) == '1':
          write_to_bazelrc(
              'test --test_tag_filters=-no_windows,-no_windows_gpu,-no_gpu')
          write_to_bazelrc(
              'test --build_tag_filters=-no_windows,-no_windows_gpu,-no_gpu')
        else:
          write_to_bazelrc('test --test_tag_filters=-no_windows,-gpu')
          write_to_bazelrc('test --build_tag_filters=-no_windows,-gpu')
      elif is_macos():
        write_to_bazelrc('test --test_tag_filters=-gpu,-nomac,-no_mac')
        write_to_bazelrc('test --build_tag_filters=-gpu,-nomac,-no_mac')
      elif is_linux():
        if env.get('TF_NEED_CUDA', None) == '1':
          write_to_bazelrc('test --test_tag_filters=-no_gpu')
          write_to_bazelrc('test --build_tag_filters=-no_gpu')
          write_to_bazelrc('test --test_env=LD_LIBRARY_PATH')
        else:
          write_to_bazelrc('test --test_tag_filters=-gpu')
          write_to_bazelrc('test --build_tag_filters=-gpu')


    def set_system_libs_flag(environ_cp):
      syslibs = environ_cp.get('TF_SYSTEM_LIBS', '')
      if syslibs:
        if ',' in syslibs:
          syslibs = ','.join(sorted(syslibs.split(',')))
        else:
          syslibs = ','.join(sorted(syslibs.split()))
        write_action_env_to_bazelrc('TF_SYSTEM_LIBS', syslibs)

      if 'PREFIX' in environ_cp:
        write_to_bazelrc('build --define=PREFIX=%s' % environ_cp['PREFIX'])
      if 'LIBDIR' in environ_cp:
        write_to_bazelrc('build --define=LIBDIR=%s' % environ_cp['LIBDIR'])
      if 'INCLUDEDIR' in environ_cp:
        write_to_bazelrc('build --define=INCLUDEDIR=%s' % environ_cp['INCLUDEDIR'])


    def set_windows_build_flags(environ_cp):
      """Set Windows specific build options."""
      # The non-monolithic build is not supported yet
      write_to_bazelrc('build --config monolithic')
      # Suppress warning messages
      write_to_bazelrc('build --copt=-w --host_copt=-w')
      # Fix winsock2.h conflicts
      write_to_bazelrc(
          'build --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN')
      # Output more verbose information when something goes wrong
      write_to_bazelrc('build --verbose_failures')
      # The host and target platforms are the same in Windows build. So we don't
      # have to distinct them. This avoids building the same targets twice.
      write_to_bazelrc('build --distinct_host_configuration=false')

      if get_var(
          environ_cp, 'TF_OVERRIDE_EIGEN_STRONG_INLINE', 'Eigen strong inline',
          True, ('Would you like to override eigen strong inline for some C++ '
                 'compilation to reduce the compilation time?'),
          'Eigen strong inline overridden.', 'Not overriding eigen strong inline, '
          'some compilations could take more than 20 mins.'):
        # Due to a known MSVC compiler issue
        # https://github.com/tensorflow/tensorflow/issues/10521
        # Overriding eigen strong inline speeds up the compiling of
        # conv_grad_ops_3d.cc and conv_ops_3d.cc by 20 minutes,
        # but this also hurts the performance. Let users decide what they want.
        write_to_bazelrc('build --define=override_eigen_strong_inline=true')


    def config_info_line(name, help_text):
      """Helper function to print formatted help text for Bazel config options."""
      print('\t--config=%-12s\t# %s' % (name, help_text))


    def configure_ios():
      """Configures TensorFlow for iOS builds.
      This function will only be executed if `is_macos()` is true.
      """
      if not is_macos():
        return
      if _TF_CURRENT_BAZEL_VERSION is None or _TF_CURRENT_BAZEL_VERSION < 23000:
        print(
            'Building Bazel rules on Apple platforms requires Bazel 0.23 or later.')
      for filepath in APPLE_BAZEL_FILES:
        existing_filepath = os.path.join(_TF_WORKSPACE_ROOT, filepath + '.apple')
        renamed_filepath = os.path.join(_TF_WORKSPACE_ROOT, filepath)
        symlink_force(existing_filepath, renamed_filepath)
      for filepath in IOS_FILES:
        filename = os.path.basename(filepath)
        new_filepath = os.path.join(_TF_WORKSPACE_ROOT, filename)
        symlink_force(filepath, new_filepath)


    def main():
      global _TF_WORKSPACE_ROOT
      global _TF_BAZELRC
      global _TF_CURRENT_BAZEL_VERSION

      parser = argparse.ArgumentParser()
      parser.add_argument(
          '--workspace',
          type=str,
          default=os.path.abspath(os.path.dirname(__file__)),
          help='The absolute path to your active Bazel workspace.')
      args = parser.parse_args()

      _TF_WORKSPACE_ROOT = args.workspace
      _TF_BAZELRC = os.path.join(_TF_WORKSPACE_ROOT, _TF_BAZELRC_FILENAME)

      # Make a copy of os.environ to be clear when functions and getting and setting
      # environment variables.
      environ_cp = dict(os.environ)

      current_bazel_version = check_bazel_version('0.22.0', '0.24.1')
      _TF_CURRENT_BAZEL_VERSION = convert_version_to_int(current_bazel_version)

      reset_tf_configure_bazelrc()

      cleanup_makefile()
      setup_python(environ_cp)

      if is_windows():
        environ_cp['TF_NEED_OPENCL_SYCL'] = '0'
        environ_cp['TF_NEED_COMPUTECPP'] = '0'
        environ_cp['TF_NEED_OPENCL'] = '0'
        environ_cp['TF_CUDA_CLANG'] = '0'
        environ_cp['TF_NEED_TENSORRT'] = '0'
        # TODO(ibiryukov): Investigate using clang as a cpu or cuda compiler on
        # Windows.
        environ_cp['TF_DOWNLOAD_CLANG'] = '0'
        environ_cp['TF_NEED_MPI'] = '0'
        environ_cp['TF_SET_ANDROID_WORKSPACE'] = '0'

      if is_macos():
        environ_cp['TF_NEED_TENSORRT'] = '0'
      else:
        environ_cp['TF_CONFIGURE_IOS'] = '0'

      # The numpy package on ppc64le uses OpenBLAS which has multi-threading
      # issues that lead to incorrect answers.  Set OMP_NUM_THREADS=1 at
      # runtime to allow the Tensorflow testcases which compare numpy
      # results to Tensorflow results to succeed.
      if is_ppc64le():
        write_action_env_to_bazelrc('OMP_NUM_THREADS', 1)

      xla_enabled_by_default = is_linux()
      set_build_var(environ_cp, 'TF_ENABLE_XLA', 'XLA JIT', 'with_xla_support',
                    xla_enabled_by_default, 'xla')

      set_action_env_var(environ_cp, 'TF_NEED_OPENCL_SYCL', 'OpenCL SYCL', False)
      if environ_cp.get('TF_NEED_OPENCL_SYCL') == '1':
        set_host_cxx_compiler(environ_cp)
        set_host_c_compiler(environ_cp)
        set_action_env_var(environ_cp, 'TF_NEED_COMPUTECPP', 'ComputeCPP', True)
        if environ_cp.get('TF_NEED_COMPUTECPP') == '1':
          set_computecpp_toolkit_path(environ_cp)
        else:
          set_trisycl_include_dir(environ_cp)

      set_action_env_var(environ_cp, 'TF_NEED_ROCM', 'ROCm', False)
      if (environ_cp.get('TF_NEED_ROCM') == '1' and
          'LD_LIBRARY_PATH' in environ_cp and
          environ_cp.get('LD_LIBRARY_PATH') != '1'):
        write_action_env_to_bazelrc('LD_LIBRARY_PATH',
                                    environ_cp.get('LD_LIBRARY_PATH'))

      set_action_env_var(environ_cp, 'TF_NEED_CUDA', 'CUDA', False)
      if (environ_cp.get('TF_NEED_CUDA') == '1' and
          'TF_CUDA_CONFIG_REPO' not in environ_cp):
        set_tf_cuda_version(environ_cp)
        set_tf_cudnn_version(environ_cp)
        if is_linux():
          set_tf_tensorrt_install_path(environ_cp)
          set_tf_nccl_install_path(environ_cp)

        set_tf_cuda_compute_capabilities(environ_cp)
        if 'LD_LIBRARY_PATH' in environ_cp and environ_cp.get(
            'LD_LIBRARY_PATH') != '1':
          write_action_env_to_bazelrc('LD_LIBRARY_PATH',
                                      environ_cp.get('LD_LIBRARY_PATH'))

        set_tf_cuda_clang(environ_cp)
        if environ_cp.get('TF_CUDA_CLANG') == '1':
          # Ask whether we should download the clang toolchain.
          set_tf_download_clang(environ_cp)
          if environ_cp.get('TF_DOWNLOAD_CLANG') != '1':
            # Set up which clang we should use as the cuda / host compiler.
            set_clang_cuda_compiler_path(environ_cp)
          else:
            # Use downloaded LLD for linking.
            write_to_bazelrc('build:cuda_clang --config=download_clang_use_lld')
            write_to_bazelrc('test:cuda_clang --config=download_clang_use_lld')
        else:
          # Set up which gcc nvcc should use as the host compiler
          # No need to set this on Windows
          if not is_windows():
            set_gcc_host_compiler_path(environ_cp)
        set_other_cuda_vars(environ_cp)
      else:
        # CUDA not required. Ask whether we should download the clang toolchain and
        # use it for the CPU build.
        set_tf_download_clang(environ_cp)
        if environ_cp.get('TF_DOWNLOAD_CLANG') == '1':
          write_to_bazelrc('build --config=download_clang')
          write_to_bazelrc('test --config=download_clang')

      # SYCL / ROCm / CUDA are mutually exclusive.
      # At most 1 GPU platform can be configured.
      gpu_platform_count = 0
      if environ_cp.get('TF_NEED_OPENCL_SYCL') == '1':
        gpu_platform_count += 1
      if environ_cp.get('TF_NEED_ROCM') == '1':
        gpu_platform_count += 1
      if environ_cp.get('TF_NEED_CUDA') == '1':
        gpu_platform_count += 1
      if gpu_platform_count >= 2:
        raise UserInputError('SYCL / CUDA / ROCm are mututally exclusive. '
                             'At most 1 GPU platform can be configured.')

      set_build_var(environ_cp, 'TF_NEED_MPI', 'MPI', 'with_mpi_support', False)
      if environ_cp.get('TF_NEED_MPI') == '1':
        set_mpi_home(environ_cp)
        set_other_mpi_vars(environ_cp)

      set_cc_opt_flags(environ_cp)
      set_system_libs_flag(environ_cp)
      if is_windows():
        set_windows_build_flags(environ_cp)

      # Add a config option to build TensorFlow 2.0 API.
      write_to_bazelrc('build:v2 --define=tf_api_version=2')

      if get_var(environ_cp, 'TF_SET_ANDROID_WORKSPACE', 'android workspace', False,
                 ('Would you like to interactively configure ./WORKSPACE for '
                  'Android builds?'), 'Searching for NDK and SDK installations.',
                 'Not configuring the WORKSPACE for Android builds.'):
        create_android_ndk_rule(environ_cp)
        create_android_sdk_rule(environ_cp)

      system_specific_test_config(os.environ)

      set_action_env_var(environ_cp, 'TF_CONFIGURE_IOS', 'iOS', False)
      if environ_cp.get('TF_CONFIGURE_IOS') == '1':
        configure_ios()
      else:
        # TODO(pcloudy): Remove BAZEL_USE_CPP_ONLY_TOOLCHAIN after Bazel is upgraded
        # to 0.24.0.
        # For working around https://github.com/bazelbuild/bazel/issues/7607
        if is_macos():
          write_to_bazelrc('build --action_env=BAZEL_USE_CPP_ONLY_TOOLCHAIN=1')

      print('Preconfigured Bazel build configs. You can use any of the below by '
            'adding "--config=<>" to your build command. See .bazelrc for more '
            'details.')
      config_info_line('mkl', 'Build with MKL support.')
      config_info_line('monolithic', 'Config for mostly static monolithic build.')
      config_info_line('gdr', 'Build with GDR support.')
      config_info_line('verbs', 'Build with libverbs support.')
      config_info_line('ngraph', 'Build with Intel nGraph support.')
      config_info_line('numa', 'Build with NUMA support.')
      config_info_line(
          'dynamic_kernels',
          '(Experimental) Build kernels into separate shared objects.')

      print('Preconfigured Bazel build configs to DISABLE default on features:')
      config_info_line('noaws', 'Disable AWS S3 filesystem support.')
      config_info_line('nogcp', 'Disable GCP support.')
      config_info_line('nohdfs', 'Disable HDFS support.')
      config_info_line('noignite', 'Disable Apache Ignite support.')
      config_info_line('nokafka', 'Disable Apache Kafka support.')
      config_info_line('nonccl', 'Disable NVIDIA NCCL support.')


    if __name__ == '__main__':
      main()

      """Built-in optimizer classes.
      """
      from __future__ import absolute_import
      from __future__ import division
      from __future__ import print_function

      import six
      import copy
      from six.moves import zip

      from . import backend as K
      from .utils.generic_utils import serialize_keras_object
      from .utils.generic_utils import deserialize_keras_object
      from .legacy import interfaces

      if K.backend() == 'tensorflow':
          import tensorflow as tf


      def clip_norm(g, c, n):
          """Clip the gradient `g` if the L2 norm `n` exceeds `c`.
          # Arguments
              g: Tensor, the gradient tensor
              c: float >= 0. Gradients will be clipped
                  when their L2 norm exceeds this value.
              n: Tensor, actual norm of `g`.
          # Returns
              Tensor, the gradient clipped if required.
          """
          if c <= 0:  # if clipnorm == 0 no need to add ops to the graph
              return g

          # tf require using a special op to multiply IndexedSliced by scalar
          if K.backend() == 'tensorflow':
              condition = n >= c
              then_expression = tf.scalar_mul(c / n, g)
              else_expression = g

              # saving the shape to avoid converting sparse tensor to dense
              if isinstance(then_expression, tf.Tensor):
                  g_shape = copy.copy(then_expression.get_shape())
              elif isinstance(then_expression, tf.IndexedSlices):
                  g_shape = copy.copy(then_expression.dense_shape)
              if condition.dtype != tf.bool:
                  condition = tf.cast(condition, 'bool')
              g = tf.cond(condition,
                          lambda: then_expression,
                          lambda: else_expression)
              if isinstance(then_expression, tf.Tensor):
                  g.set_shape(g_shape)
              elif isinstance(then_expression, tf.IndexedSlices):
                  g._dense_shape = g_shape
          else:
              g = K.switch(K.greater_equal(n, c), g * c / n, g)
          return g


      class Optimizer(object):
          """Abstract optimizer base class.
          Note: this is the parent class of all optimizers, not an actual optimizer
          that can be used for training models.
          All Keras optimizers support the following keyword arguments:
              clipnorm: float >= 0. Gradients will be clipped
                  when their L2 norm exceeds this value.
              clipvalue: float >= 0. Gradients will be clipped
                  when their absolute value exceeds this value.
          """

          def __init__(self, **kwargs):
              allowed_kwargs = {'clipnorm', 'clipvalue'}
              for k in kwargs:
                  if k not in allowed_kwargs:
                      raise TypeError('Unexpected keyword argument '
                                      'passed to optimizer: ' + str(k))
              self.__dict__.update(kwargs)
              self.updates = []
              self.weights = []

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              raise NotImplementedError

          def get_gradients(self, loss, params):
              grads = K.gradients(loss, params)
              if None in grads:
                  raise ValueError('An operation has `None` for gradient. '
                                   'Please make sure that all of your ops have a '
                                   'gradient defined (i.e. are differentiable). '
                                   'Common ops without gradient: '
                                   'K.argmax, K.round, K.eval.')
              if hasattr(self, 'clipnorm') and self.clipnorm > 0:
                  norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))
                  grads = [clip_norm(g, self.clipnorm, norm) for g in grads]
              if hasattr(self, 'clipvalue') and self.clipvalue > 0:
                  grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]
              return grads

          def set_weights(self, weights):
              """Sets the weights of the optimizer, from Numpy arrays.
              Should only be called after computing the gradients
              (otherwise the optimizer has no weights).
              # Arguments
                  weights: a list of Numpy arrays. The number
                      of arrays and their shape must match
                      number of the dimensions of the weights
                      of the optimizer (i.e. it should match the
                      output of `get_weights`).
              # Raises
                  ValueError: in case of incompatible weight shapes.
              """
              params = self.weights
              if len(params) != len(weights):
                  raise ValueError('Length of the specified weight list (' +
                                   str(len(weights)) +
                                   ') does not match the number of weights ' +
                                   'of the optimizer (' + str(len(params)) + ')')
              weight_value_tuples = []
              param_values = K.batch_get_value(params)
              for pv, p, w in zip(param_values, params, weights):
                  if pv.shape != w.shape:
                      raise ValueError('Optimizer weight shape ' +
                                       str(pv.shape) +
                                       ' not compatible with '
                                       'provided weight shape ' + str(w.shape))
                  weight_value_tuples.append((p, w))
              K.batch_set_value(weight_value_tuples)

          def get_weights(self):
              """Returns the current value of the weights of the optimizer.
              # Returns
                  A list of numpy arrays.
              """
              return K.batch_get_value(self.weights)

          def get_config(self):
              config = {}
              if hasattr(self, 'clipnorm'):
                  config['clipnorm'] = self.clipnorm
              if hasattr(self, 'clipvalue'):
                  config['clipvalue'] = self.clipvalue
              return config

          @classmethod
          def from_config(cls, config):
              return cls(**config)


      class SGD(Optimizer):
          """Stochastic gradient descent optimizer.
          Includes support for momentum,
          learning rate decay, and Nesterov momentum.
          # Arguments
              lr: float >= 0. Learning rate.
              momentum: float >= 0. Parameter that accelerates SGD
                  in the relevant direction and dampens oscillations.
              decay: float >= 0. Learning rate decay over each update.
              nesterov: boolean. Whether to apply Nesterov momentum.
          """

          def __init__(self, lr=0.01, momentum=0., decay=0.,
                       nesterov=False, **kwargs):
              super(SGD, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
                  self.lr = K.variable(lr, name='lr')
                  self.momentum = K.variable(momentum, name='momentum')
                  self.decay = K.variable(decay, name='decay')
              self.initial_decay = decay
              self.nesterov = nesterov

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))
              # momentum
              shapes = [K.int_shape(p) for p in params]
              moments = [K.zeros(shape) for shape in shapes]
              self.weights = [self.iterations] + moments
              for p, g, m in zip(params, grads, moments):
                  v = self.momentum * m - lr * g  # velocity
                  self.updates.append(K.update(m, v))

                  if self.nesterov:
                      new_p = p + self.momentum * v - lr * g
                  else:
                      new_p = p + v

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'momentum': float(K.get_value(self.momentum)),
                        'decay': float(K.get_value(self.decay)),
                        'nesterov': self.nesterov}
              base_config = super(SGD, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class RMSprop(Optimizer):
          """RMSProp optimizer.
          It is recommended to leave the parameters of this optimizer
          at their default values
          (except the learning rate, which can be freely tuned).
          # Arguments
              lr: float >= 0. Learning rate.
              rho: float >= 0.
              epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
              decay: float >= 0. Learning rate decay over each update.
          # References
              - [rmsprop: Divide the gradient by a running average of its recent magnitude
                 ](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
          """

          def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,
                       **kwargs):
              super(RMSprop, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.lr = K.variable(lr, name='lr')
                  self.rho = K.variable(rho, name='rho')
                  self.decay = K.variable(decay, name='decay')
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.epsilon = epsilon
              self.initial_decay = decay

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
              self.weights = accumulators
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))

              for p, g, a in zip(params, grads, accumulators):
                  # update accumulator
                  new_a = self.rho * a + (1. - self.rho) * K.square(g)
                  self.updates.append(K.update(a, new_a))
                  new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'rho': float(K.get_value(self.rho)),
                        'decay': float(K.get_value(self.decay)),
                        'epsilon': self.epsilon}
              base_config = super(RMSprop, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class Adagrad(Optimizer):
          """Adagrad optimizer.
          Adagrad is an optimizer with parameter-specific learning rates,
          which are adapted relative to how frequently a parameter gets
          updated during training. The more updates a parameter receives,
          the smaller the learning rate.
          It is recommended to leave the parameters of this optimizer
          at their default values.
          # Arguments
              lr: float >= 0. Initial learning rate.
              epsilon: float >= 0. If `None`, defaults to `K.epsilon()`.
              decay: float >= 0. Learning rate decay over each update.
          # References
              - [Adaptive Subgradient Methods for Online Learning and Stochastic
                 Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
          """

          def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):
              super(Adagrad, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.lr = K.variable(lr, name='lr')
                  self.decay = K.variable(decay, name='decay')
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.epsilon = epsilon
              self.initial_decay = decay

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              shapes = [K.int_shape(p) for p in params]
              accumulators = [K.zeros(shape) for shape in shapes]
              self.weights = accumulators
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))

              for p, g, a in zip(params, grads, accumulators):
                  new_a = a + K.square(g)  # update accumulator
                  self.updates.append(K.update(a, new_a))
                  new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'decay': float(K.get_value(self.decay)),
                        'epsilon': self.epsilon}
              base_config = super(Adagrad, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class Adadelta(Optimizer):
          """Adadelta optimizer.
          Adadelta is a more robust extension of Adagrad
          that adapts learning rates based on a moving window of gradient updates,
          instead of accumulating all past gradients. This way, Adadelta continues
          learning even when many updates have been done. Compared to Adagrad, in the
          original version of Adadelta you don't have to set an initial learning
          rate. In this version, initial learning rate and decay factor can
          be set, as in most other Keras optimizers.
          It is recommended to leave the parameters of this optimizer
          at their default values.
          # Arguments
              lr: float >= 0. Initial learning rate, defaults to 1.
                  It is recommended to leave it at the default value.
              rho: float >= 0. Adadelta decay factor, corresponding to fraction of
                  gradient to keep at each time step.
              epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
              decay: float >= 0. Initial learning rate decay.
          # References
              - [Adadelta - an adaptive learning rate method](
                 https://arxiv.org/abs/1212.5701)
          """

          def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0.,
                       **kwargs):
              super(Adadelta, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.lr = K.variable(lr, name='lr')
                  self.decay = K.variable(decay, name='decay')
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.rho = rho
              self.epsilon = epsilon
              self.initial_decay = decay

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              shapes = [K.int_shape(p) for p in params]
              accumulators = [K.zeros(shape) for shape in shapes]
              delta_accumulators = [K.zeros(shape) for shape in shapes]
              self.weights = accumulators + delta_accumulators
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))

              for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):
                  # update accumulator
                  new_a = self.rho * a + (1. - self.rho) * K.square(g)
                  self.updates.append(K.update(a, new_a))

                  # use the new accumulator and the *old* delta_accumulator
                  update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)
                  new_p = p - lr * update

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))

                  # update delta_accumulator
                  new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)
                  self.updates.append(K.update(d_a, new_d_a))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'rho': self.rho,
                        'decay': float(K.get_value(self.decay)),
                        'epsilon': self.epsilon}
              base_config = super(Adadelta, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class Adam(Optimizer):
          """Adam optimizer.
          Default parameters follow those provided in the original paper.
          # Arguments
              lr: float >= 0. Learning rate.
              beta_1: float, 0 < beta < 1. Generally close to 1.
              beta_2: float, 0 < beta < 1. Generally close to 1.
              epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
              decay: float >= 0. Learning rate decay over each update.
              amsgrad: boolean. Whether to apply the AMSGrad variant of this
                  algorithm from the paper "On the Convergence of Adam and
                  Beyond".
          # References
              - [Adam - A Method for Stochastic Optimization](
                 https://arxiv.org/abs/1412.6980v8)
              - [On the Convergence of Adam and Beyond](
                 https://openreview.net/forum?id=ryQu7f-RZ)
          """

          def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,
                       epsilon=None, decay=0., amsgrad=False, **kwargs):
              super(Adam, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
                  self.lr = K.variable(lr, name='lr')
                  self.beta_1 = K.variable(beta_1, name='beta_1')
                  self.beta_2 = K.variable(beta_2, name='beta_2')
                  self.decay = K.variable(decay, name='decay')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.epsilon = epsilon
              self.initial_decay = decay
              self.amsgrad = amsgrad

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))

              t = K.cast(self.iterations, K.floatx()) + 1
              lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /
                           (1. - K.pow(self.beta_1, t)))

              ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
              vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
              if self.amsgrad:
                  vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
              else:
                  vhats = [K.zeros(1) for _ in params]
              self.weights = [self.iterations] + ms + vs + vhats

              for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):
                  m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
                  v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)
                  if self.amsgrad:
                      vhat_t = K.maximum(vhat, v_t)
                      p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)
                      self.updates.append(K.update(vhat, vhat_t))
                  else:
                      p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)

                  self.updates.append(K.update(m, m_t))
                  self.updates.append(K.update(v, v_t))
                  new_p = p_t

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'beta_1': float(K.get_value(self.beta_1)),
                        'beta_2': float(K.get_value(self.beta_2)),
                        'decay': float(K.get_value(self.decay)),
                        'epsilon': self.epsilon,
                        'amsgrad': self.amsgrad}
              base_config = super(Adam, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class Adamax(Optimizer):
          """Adamax optimizer from Adam paper's Section 7.
          It is a variant of Adam based on the infinity norm.
          Default parameters follow those provided in the paper.
          # Arguments
              lr: float >= 0. Learning rate.
              beta_1: float, 0 < beta < 1. Generally close to 1.
              beta_2: float, 0 < beta < 1. Generally close to 1.
              epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
              decay: float >= 0. Learning rate decay over each update.
          # References
              - [Adam - A Method for Stochastic Optimization](
                 https://arxiv.org/abs/1412.6980v8)
          """

          def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,
                       epsilon=None, decay=0., **kwargs):
              super(Adamax, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
                  self.lr = K.variable(lr, name='lr')
                  self.beta_1 = K.variable(beta_1, name='beta_1')
                  self.beta_2 = K.variable(beta_2, name='beta_2')
                  self.decay = K.variable(decay, name='decay')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.epsilon = epsilon
              self.initial_decay = decay

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              self.updates = [K.update_add(self.iterations, 1)]

              lr = self.lr
              if self.initial_decay > 0:
                  lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                            K.dtype(self.decay))))

              t = K.cast(self.iterations, K.floatx()) + 1
              lr_t = lr / (1. - K.pow(self.beta_1, t))

              shapes = [K.int_shape(p) for p in params]
              # zero init of 1st moment
              ms = [K.zeros(shape) for shape in shapes]
              # zero init of exponentially weighted infinity norm
              us = [K.zeros(shape) for shape in shapes]
              self.weights = [self.iterations] + ms + us

              for p, g, m, u in zip(params, grads, ms, us):

                  m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
                  u_t = K.maximum(self.beta_2 * u, K.abs(g))
                  p_t = p - lr_t * m_t / (u_t + self.epsilon)

                  self.updates.append(K.update(m, m_t))
                  self.updates.append(K.update(u, u_t))
                  new_p = p_t

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'beta_1': float(K.get_value(self.beta_1)),
                        'beta_2': float(K.get_value(self.beta_2)),
                        'decay': float(K.get_value(self.decay)),
                        'epsilon': self.epsilon}
              base_config = super(Adamax, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class Nadam(Optimizer):
          """Nesterov Adam optimizer.
          Much like Adam is essentially RMSprop with momentum,
          Nadam is RMSprop with Nesterov momentum.
          Default parameters follow those provided in the paper.
          It is recommended to leave the parameters of this optimizer
          at their default values.
          # Arguments
              lr: float >= 0. Learning rate.
              beta_1: float, 0 < beta < 1. Generally close to 1.
              beta_2: float, 0 < beta < 1. Generally close to 1.
              epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.
              schedule_decay: float, 0 < schedule_decay < 1.
          # References
              - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)
              - [On the importance of initialization and momentum in deep learning](
                 http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)
          """

          def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,
                       epsilon=None, schedule_decay=0.004, **kwargs):
              super(Nadam, self).__init__(**kwargs)
              with K.name_scope(self.__class__.__name__):
                  self.iterations = K.variable(0, dtype='int64', name='iterations')
                  self.m_schedule = K.variable(1., name='m_schedule')
                  self.lr = K.variable(lr, name='lr')
                  self.beta_1 = K.variable(beta_1, name='beta_1')
                  self.beta_2 = K.variable(beta_2, name='beta_2')
              if epsilon is None:
                  epsilon = K.epsilon()
              self.epsilon = epsilon
              self.schedule_decay = schedule_decay

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.get_gradients(loss, params)
              self.updates = [K.update_add(self.iterations, 1)]

              t = K.cast(self.iterations, K.floatx()) + 1

              # Due to the recommendations in [2], i.e. warming momentum schedule
              momentum_cache_t = self.beta_1 * (1. - 0.5 * (
                  K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))
              momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (
                  K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))
              m_schedule_new = self.m_schedule * momentum_cache_t
              m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1
              self.updates.append((self.m_schedule, m_schedule_new))

              shapes = [K.int_shape(p) for p in params]
              ms = [K.zeros(shape) for shape in shapes]
              vs = [K.zeros(shape) for shape in shapes]

              self.weights = [self.iterations] + ms + vs

              for p, g, m, v in zip(params, grads, ms, vs):
                  # the following equations given in [1]
                  g_prime = g / (1. - m_schedule_new)
                  m_t = self.beta_1 * m + (1. - self.beta_1) * g
                  m_t_prime = m_t / (1. - m_schedule_next)
                  v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)
                  v_t_prime = v_t / (1. - K.pow(self.beta_2, t))
                  m_t_bar = (1. - momentum_cache_t) * g_prime + (
                      momentum_cache_t_1 * m_t_prime)

                  self.updates.append(K.update(m, m_t))
                  self.updates.append(K.update(v, v_t))

                  p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)
                  new_p = p_t

                  # Apply constraints.
                  if getattr(p, 'constraint', None) is not None:
                      new_p = p.constraint(new_p)

                  self.updates.append(K.update(p, new_p))
              return self.updates

          def get_config(self):
              config = {'lr': float(K.get_value(self.lr)),
                        'beta_1': float(K.get_value(self.beta_1)),
                        'beta_2': float(K.get_value(self.beta_2)),
                        'epsilon': self.epsilon,
                        'schedule_decay': self.schedule_decay}
              base_config = super(Nadam, self).get_config()
              return dict(list(base_config.items()) + list(config.items()))


      class TFOptimizer(Optimizer):
          """Wrapper class for native TensorFlow optimizers.
          # Arguments
              optimizer: Selected optimizer
          """

          def __init__(self, optimizer):
              self.optimizer = optimizer
              with K.name_scope(self.__class__.__name__):
                  self.iterations = K.variable(0, dtype='int64', name='iterations')

          @interfaces.legacy_get_updates_support
          def get_updates(self, loss, params):
              grads = self.optimizer.compute_gradients(loss, var_list=params)
              self.updates = [K.update_add(self.iterations, 1)]
              opt_update = self.optimizer.apply_gradients(
                  grads, global_step=self.iterations)
              self.updates.append(opt_update)
              return self.updates

          @property
          def weights(self):
              raise NotImplementedError

          def get_config(self):
              raise NotImplementedError

          def from_config(self, config):
              raise NotImplementedError


      # Aliases.

      sgd = SGD
      rmsprop = RMSprop
      adagrad = Adagrad
      adadelta = Adadelta
      adam = Adam
      adamax = Adamax
      nadam = Nadam


      def serialize(optimizer):
          return serialize_keras_object(optimizer)


      def deserialize(config, custom_objects=None):
          """Inverse of the `serialize` function.
          # Arguments
              config: Optimizer configuration dictionary.
              custom_objects: Optional dictionary mapping
                  names (strings) to custom objects
                  (classes and functions)
                  to be considered during deserialization.
          # Returns
              A Keras Optimizer instance.
          """
          all_classes = {
              'sgd': SGD,
              'rmsprop': RMSprop,
              'adagrad': Adagrad,
              'adadelta': Adadelta,
              'adam': Adam,
              'adamax': Adamax,
              'nadam': Nadam,
              'tfoptimizer': TFOptimizer,
          }
          # Make deserialization case-insensitive for built-in optimizers.
          if config['class_name'].lower() in all_classes:
              config['class_name'] = config['class_name'].lower()
          return deserialize_keras_object(config,
                                          module_objects=all_classes,
                                          custom_objects=custom_objects,
                                          printable_module_name='optimizer')


      def get(identifier):
          """Retrieves a Keras Optimizer instance.
          # Arguments
              identifier: Optimizer identifier, one of
                  - String: name of an optimizer
                  - Dictionary: configuration dictionary.
                  - Keras Optimizer instance (it will be returned unchanged).
                  - TensorFlow Optimizer instance
                      (it will be wrapped as a Keras Optimizer).
          # Returns
              A Keras Optimizer instance.
          # Raises
              ValueError: If `identifier` cannot be interpreted.
          """
          if K.backend() == 'tensorflow':
              # Wrap TF optimizer instances
              if isinstance(identifier, tf.train.Optimizer):
                  return TFOptimizer(identifier)
          if isinstance(identifier, dict):
              return deserialize(identifier)
          elif isinstance(identifier, six.string_types):
              config = {'class_name': str(identifier), 'config': {}}
              return deserialize(config)
          if isinstance(identifier, Optimizer):
              return identifier
          else:
              raise ValueError('Could not interpret optimizer identifier: ' +
                               str(identifier))

                 from __future__ import absolute_import
                 from __future__ import division
                 from __future__ import print_function

                 from collections import defaultdict
                 from contextlib import contextmanager
                 import theano
                 from theano import tensor as T
                 from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
                 from theano.tensor.signal import pool
                 from theano.printing import Print
                 from theano.ifelse import ifelse
                 try:
                     import theano.sparse as th_sparse_module
                 except ImportError:
                     th_sparse_module = None
                 try:
                     from theano.tensor.nnet.nnet import softsign as T_softsign
                 except ImportError:
                     from theano.sandbox.softsign import softsign as T_softsign

                 import numpy as np
                 from .common import floatx
                 from .common import epsilon
                 from .common import normalize_data_format
                 from ..utils.generic_utils import transpose_shape
                 from ..utils.generic_utils import has_arg
                 # Legacy functions
                 from .common import set_image_dim_ordering, image_dim_ordering

                 py_all = all
                 py_any = any
                 py_sum = sum
                 py_slice = slice


                 # INTERNAL UTILS
                 theano.config.floatX = floatx()
                 # 0 = test, 1 = train
                 _LEARNING_PHASE = T.scalar(dtype='uint8', name='keras_learning_phase')
                 _UID_PREFIXES = defaultdict(int)


                 def learning_phase():
                     # False = test, True = train
                     return _LEARNING_PHASE


                 def set_learning_phase(value):
                     global _LEARNING_PHASE
                     if value not in {0, 1}:
                         raise ValueError('Expected learning phase to be '
                                          '0 or 1.')
                     _LEARNING_PHASE = value


                 def get_uid(prefix=''):
                     """Provides a unique UID given a string prefix.
                     # Arguments
                         prefix: string.
                     # Returns
                         An integer.
                     # Example
                     ```python
                         >>> keras.backend.get_uid('dense')
                         1
                         >>> keras.backend.get_uid('dense')
                         2
                     ```
                     """
                     _UID_PREFIXES[prefix] += 1
                     return _UID_PREFIXES[prefix]


                 def reset_uids():
                     global _UID_PREFIXES
                     _UID_PREFIXES = defaultdict(int)


                 # VARIABLE MANIPULATION


                 def _assert_sparse_module():
                     if not th_sparse_module:
                         raise ImportError("Failed to import theano.sparse\n"
                                           "You probably need to pip install nose-parameterized")


                 def is_sparse(tensor):
                     return th_sparse_module and isinstance(tensor.type, th_sparse_module.SparseType)


                 def to_dense(tensor):
                     if is_sparse(tensor):
                         return th_sparse_module.dense_from_sparse(tensor)
                     else:
                         return tensor


                 NAME_SCOPE_STACK = []


                 @contextmanager
                 def name_scope(name):
                     global NAME_SCOPE_STACK
                     NAME_SCOPE_STACK.append(name)
                     yield
                     NAME_SCOPE_STACK.pop()


                 def _prepare_name(name, default):
                     prefix = '/'.join(NAME_SCOPE_STACK)
                     if name is None:
                         return prefix + '/' + default
                     return prefix + '/' + name


                 def variable(value, dtype=None, name=None, constraint=None):
                     """Instantiates a variable and returns it.
                     # Arguments
                         value: Numpy array, initial value of the tensor.
                         dtype: Tensor type.
                         name: Optional name string for the tensor.
                         constraint: Optional projection function to be
                             applied to the variable after an optimizer update.
                     # Returns
                         A variable instance (with Keras metadata included).
                     """
                     if dtype is None:
                         dtype = floatx()
                     if hasattr(value, 'tocoo'):
                         _assert_sparse_module()
                         variable = th_sparse_module.as_sparse_variable(
                             value, name=_prepare_name(name, 'variable'))
                     else:
                         if isinstance(value, (theano.tensor.TensorVariable,
                                               theano.tensor.sharedvar.TensorSharedVariable,
                                               theano.tensor.TensorConstant)):
                             # Support for RandomStreams().normal(), .uniform().
                             value = value.eval()
                         value = np.asarray(value, dtype=dtype)
                         variable = theano.shared(value=value,
                                                  name=_prepare_name(name, 'variable'),
                                                  strict=False)
                     variable._keras_shape = value.shape
                     variable._uses_learning_phase = False
                     variable.constraint = constraint
                     return variable


                 def constant(value, dtype=None, shape=None, name=None):
                     if dtype is None:
                         dtype = floatx()
                     if shape is None:
                         shape = ()
                     np_value = value * np.ones(shape)
                     const = T.constant(np_value,
                                        dtype=dtype,
                                        name=_prepare_name(name, 'constant'))
                     const._keras_shape = shape
                     const._uses_learning_phase = False
                     return const


                 def is_keras_tensor(x):
                     """Returns whether `x` is a Keras tensor.
                     A "Keras tensor" is a tensor that was returned by a Keras layer,
                     (`Layer` class) or by `Input`.
                     # Arguments
                         x: A candidate tensor.
                     # Returns
                         A boolean: Whether the argument is a Keras tensor.
                     # Raises
                         ValueError: In case `x` is not a symbolic tensor.
                     # Examples
                     ```python
                         >>> from keras import backend as K
                         >>> from keras.layers import Input, Dense
                         >>> np_var = numpy.array([1, 2])
                         >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
                         ValueError
                         >>> k_var = tf.placeholder('float32', shape=(1,1))
                         >>> # A variable indirectly created outside of keras is not a Keras tensor.
                         >>> K.is_keras_tensor(k_var)
                         False
                         >>> keras_var = K.variable(np_var)
                         >>> # A variable created with the keras backend is not a Keras tensor.
                         >>> K.is_keras_tensor(keras_var)
                         False
                         >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))
                         >>> # A placeholder is not a Keras tensor.
                         >>> K.is_keras_tensor(keras_placeholder)
                         False
                         >>> keras_input = Input([10])
                         >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
                         True
                         >>> keras_layer_output = Dense(10)(keras_input)
                         >>> # Any Keras layer output is a Keras tensor.
                         >>> K.is_keras_tensor(keras_layer_output)
                         True
                     ```
                     """
                     if not is_tensor(x):
                         raise ValueError('Unexpectedly found an instance of type `' +
                                          str(type(x)) + '`. '
                                          'Expected a symbolic tensor instance.')
                     return hasattr(x, '_keras_history')


                 def is_tensor(x):
                     return isinstance(x, (T.TensorVariable,
                                           T.sharedvar.TensorSharedVariable))


                 def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):
                     """Instantiate an input data placeholder variable.
                     """
                     if dtype is None:
                         dtype = floatx()
                     if shape is None and ndim is None:
                         raise ValueError('Specify either a shape or ndim value.')
                     if shape is not None:
                         ndim = len(shape)
                     else:
                         shape = tuple([None for _ in range(ndim)])

                     name = _prepare_name(name, 'placeholder')
                     broadcast = (False,) * ndim
                     if sparse:
                         _assert_sparse_module()
                         x = th_sparse_module.csr_matrix(name=name, dtype=dtype)
                     else:
                         x = T.TensorType(dtype, broadcast)(name)
                     x._keras_shape = shape
                     x._uses_learning_phase = False
                     x._theano_placeholder = True
                     return x


                 def is_placeholder(x):
                     """Returns whether `x` is a placeholder.
                     # Arguments
                         x: A candidate placeholder.
                     # Returns
                         Boolean.
                     """
                     return hasattr(x, '_theano_placeholder') and x._theano_placeholder


                 def shape(x):
                     """Returns the shape of a tensor.
                     Warning: type returned will be different for
                     Theano backend (Theano tensor type) and TF backend (TF TensorShape).
                     """
                     return x.shape


                 def int_shape(x):
                     """Returns the shape of a Keras tensor or a Keras variable as a tuple of
                     integers or None entries.
                     # Arguments
                         x: Tensor or variable.
                     # Returns
                         A tuple of integers (or None entries).
                     """
                     if hasattr(x, '_keras_shape'):
                         return x._keras_shape
                     else:
                         return None


                 def ndim(x):
                     return x.ndim


                 def dtype(x):
                     return x.dtype


                 def eval(x):
                     """Returns the value of a tensor.
                     """
                     return to_dense(x).eval()


                 def zeros(shape, dtype=None, name=None):
                     """Instantiates an all-zeros variable.
                     """
                     if dtype is None:
                         dtype = floatx()
                     return variable(np.zeros(shape), dtype, name)


                 def ones(shape, dtype=None, name=None):
                     """Instantiates an all-ones variable.
                     """
                     if dtype is None:
                         dtype = floatx()
                     return variable(np.ones(shape), dtype, name)


                 def eye(size, dtype=None, name=None):
                     """Instantiates an identity matrix.
                     """
                     if dtype is None:
                         dtype = floatx()
                     if isinstance(size, (list, tuple)):
                         n, m = size
                     else:
                         n, m = size, size
                     return variable(np.eye(n, m), dtype, name)


                 def ones_like(x, dtype=None, name=None):
                     if dtype is None:
                         dtype = floatx()
                     return T.ones_like(x, dtype=dtype)


                 def zeros_like(x, dtype=None, name=None):
                     if dtype is None:
                         dtype = floatx()
                     return T.zeros_like(x, dtype=dtype)


                 def identity(x, name=None):
                     """Returns a tensor with the same content as the input tensor.
                     # Arguments
                         x: The input tensor.
                         name: String, name for the variable to create.
                     # Returns
                         A tensor of the same shape, type and content.
                     """
                     return x.copy(name=name)


                 def random_uniform_variable(shape, low, high, dtype=None, name=None):
                     return variable(np.random.uniform(low=low, high=high, size=shape),
                                     dtype=dtype, name=name)


                 def random_normal_variable(shape, mean, scale, dtype=None, name=None):
                     return variable(np.random.normal(loc=0.0, scale=scale, size=shape),
                                     dtype=dtype, name=name)


                 def count_params(x):
                     """Returns the number of scalars in a tensor.
                     Return: numpy integer.
                     """
                     # We don't want those compilation to show up in Theano profiler.
                     f = theano.function([], x.shape, profile=False)
                     return np.prod(f())


                 def cast(x, dtype):
                     return T.cast(x, dtype)


                 # UPDATES OPS


                 def update(x, new_x):
                     return (x, new_x)


                 def update_add(x, increment):
                     return (x, x + increment)


                 def update_sub(x, decrement):
                     return (x, x - decrement)


                 def moving_average_update(variable, value, momentum):
                     return (variable, variable * momentum + value * (1. - momentum))


                 # LINEAR ALGEBRA

                 """
                 Assumed overridden:
                 +, -, /, *, +=, -=, *=, /=
                 """


                 def dot(x, y):
                     if is_sparse(x):
                         out = th_sparse_module.basic.structured_dot(x, y)
                     else:
                         out = T.dot(x, y)
                     if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):
                         x_shape = list(x._keras_shape)
                         y_shape = list(y._keras_shape)
                         if len(x_shape) > 0:
                             x_shape.pop()
                         if len(y_shape) == 1:
                             y_shape.pop()
                         elif len(y_shape) > 1:
                             y_shape.pop(-2)
                         out._keras_shape = tuple(x_shape + y_shape)
                     return out


                 def batch_dot(x, y, axes=None):
                     """Batchwise dot product.
                     batch_dot results in a tensor with less dimensions than the input.
                     If the number of dimensions is reduced to 1, we use `expand_dims` to
                     make sure that ndim is at least 2.
                     # Arguments
                         x, y: tensors with ndim >= 2
                         axes: list (or single) int with target dimensions
                     # Returns
                         A tensor with shape equal to the concatenation of x's shape
                         (less the dimension that was summed over) and y's shape
                         (less the batch dimension and the dimension that was summed over).
                         If the final rank is 1, we reshape it to (batch_size, 1).
                     # Examples
                         Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]
                         batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal
                         of x.dot(y.T), although we never have to calculate the off-diagonal
                         elements.
                         Shape inference:
                         Let x's shape be (100, 20) and y's shape be (100, 30, 20).
                         If dot_axes is (1, 2), to find the output shape of resultant tensor,
                             loop through each dimension in x's shape and y's shape:
                         x.shape[0] : 100 : append to output shape
                         x.shape[1] : 20 : do not append to output shape,
                             dimension 1 of x has been summed over. (dot_axes[0] = 1)
                         y.shape[0] : 100 : do not append to output shape,
                             always ignore first dimension of y
                         y.shape[1] : 30 : append to output shape
                         y.shape[2] : 20 : do not append to output shape,
                             dimension 2 of y has been summed over. (dot_axes[1] = 2)
                         output_shape = (100, 30)
                     """
                     if isinstance(axes, int):
                         axes = (axes, axes)
                     if axes is None:
                         # behaves like tf.batch_matmul as default
                         if y.ndim == 2:
                             axes = [x.ndim - 1, y.ndim - 1]
                         else:
                             axes = [x.ndim - 1, y.ndim - 2]
                     if py_any([isinstance(a, (list, tuple)) for a in axes]):
                         raise ValueError('Multiple target dimensions are not supported. ' +
                                          'Expected: None, int, (int, int), ' +
                                          'Provided: ' + str(axes))
                     if isinstance(axes, tuple):
                         axes = list(axes)

                     if 0 in axes:
                         raise ValueError('Can not perform batch_dot over axis 0.'
                                          'If your inputs are not batched,'
                                          ' add a dummy batch dimension to your '
                                          'inputs using K.expand_dims(x, 0)')

                     out = T.batched_tensordot(x, y, axes=axes)
                     if ndim(out) == 1:
                         out = expand_dims(out, 1)

                     if hasattr(x, '_keras_shape') and hasattr(y, '_keras_shape'):
                         shape = []
                         for axis in range(len(x._keras_shape)):
                             if axis != axes[0]:
                                 shape.append(x._keras_shape[axis])
                         for axis in range(1, len(y._keras_shape)):
                             if axis != axes[1]:
                                 shape.append(y._keras_shape[axis])
                         if len(shape) == 1:
                             shape.append(1)     # Expand dims if ndim == 1
                         out._keras_shape = tuple(shape)
                     return out


                 def transpose(x):
                     y = T.transpose(x)
                     if hasattr(x, '_keras_shape'):
                         y._keras_shape = tuple(reversed(x._keras_shape))
                     return y


                 def gather(reference, indices):
                     """Retrieves the elements of indices `indices` in the tensor `reference`.
                     # Arguments
                         reference: A tensor.
                         indices: An integer tensor of indices.
                     # Returns
                         A tensor of same type as `reference`.
                     """
                     y = reference[indices]
                     if hasattr(reference, '_keras_shape') and hasattr(indices, '_keras_shape'):
                         y._keras_shape = indices._keras_shape + reference._keras_shape[1:]
                     return y


                 # ELEMENT-WISE OPERATIONS


                 def max(x, axis=None, keepdims=False):
                     return T.max(x, axis=axis, keepdims=keepdims)


                 def min(x, axis=None, keepdims=False):
                     return T.min(x, axis=axis, keepdims=keepdims)


                 def sum(x, axis=None, keepdims=False):
                     """Sum of the values in a tensor, alongside the specified axis.
                     """
                     return T.sum(x, axis=axis, keepdims=keepdims)


                 def prod(x, axis=None, keepdims=False):
                     """Multiply the values in a tensor, alongside the specified axis.
                     """
                     return T.prod(x, axis=axis, keepdims=keepdims)


                 def cumsum(x, axis=0):
                     """Cumulative sum of the values in a tensor, alongside the specified axis.
                     # Arguments
                         x: A tensor or variable.
                         axis: An integer, the axis to compute the sum.
                     # Returns
                         A tensor of the cumulative sum of values of `x` along `axis`.
                     """
                     return T.extra_ops.cumsum(x, axis=axis)


                 def cumprod(x, axis=0):
                     """Cumulative product of the values in a tensor, alongside the specified axis.
                     # Arguments
                         x: A tensor or variable.
                         axis: An integer, the axis to compute the product.
                     # Returns
                         A tensor of the cumulative product of values of `x` along `axis`.
                     """
                     return T.extra_ops.cumprod(x, axis=axis)


                 def mean(x, axis=None, keepdims=False):
                     """Mean of a tensor, alongside the specified axis.
                     """
                     dtype = None
                     # bool is available since theano v0.9dev
                     if 'int' in x.dtype or x.dtype == 'bool':
                         dtype = floatx()
                     return T.mean(x, axis=axis, keepdims=keepdims, dtype=dtype)


                 def std(x, axis=None, keepdims=False):
                     return T.std(x, axis=axis, keepdims=keepdims)


                 def var(x, axis=None, keepdims=False):
                     return T.var(x, axis=axis, keepdims=keepdims)


                 def any(x, axis=None, keepdims=False):
                     """Bitwise reduction (logical OR).
                     """
                     y = T.any(x, axis=axis, keepdims=keepdims)
                     y = _set_keras_shape_for_reduction(x, y, axis, keepdims)
                     return y


                 def all(x, axis=None, keepdims=False):
                     """Bitwise reduction (logical AND).
                     """
                     y = T.all(x, axis=axis, keepdims=keepdims)
                     y = _set_keras_shape_for_reduction(x, y, axis, keepdims)
                     return y


                 def _set_keras_shape_for_reduction(x, y, axis, keepdims):
                     if hasattr(x, '_keras_shape'):
                         if axis is None:
                             y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)
                         else:
                             if isinstance(axis, int):
                                 axis_list = [axis]
                             else:
                                 axis_list = list(set(int(a) for a in axis))
                             keras_shape_list = list(x._keras_shape)
                             if keepdims:
                                 for a in axis_list:
                                     keras_shape_list[a] = 1
                             else:
                                 for a in axis_list[::-1]:
                                     keras_shape_list.pop(a)
                                 if not keras_shape_list:
                                     keras_shape_list = (1,)
                             y._keras_shape = tuple(keras_shape_list)
                     return y


                 def argmax(x, axis=-1):
                     return T.argmax(x, axis=axis, keepdims=False)


                 def argmin(x, axis=-1):
                     return T.argmin(x, axis=axis, keepdims=False)


                 def square(x):
                     return T.sqr(x)


                 def abs(x):
                     return T.abs_(x)


                 def sqrt(x):
                     x = T.clip(x, 0., np.inf)
                     return T.sqrt(x)


                 def exp(x):
                     return T.exp(x)


                 def log(x):
                     return T.log(x)


                 def logsumexp(x, axis=None, keepdims=False):
                     """Computes log(sum(exp(elements across dimensions of a tensor))).
                     This function is more numerically stable than log(sum(exp(x))).
                     It avoids overflows caused by taking the exp of large inputs and
                     underflows caused by taking the log of small inputs.
                     # Arguments
                         x: A tensor or variable.
                         axis: An integer, the axis to reduce over.
                         keepdims: A boolean, whether to keep the dimensions or not.
                             If `keepdims` is `False`, the rank of the tensor is reduced
                             by 1. If `keepdims` is `True`, the reduced dimension is
                             retained with length 1.
                     # Returns
                         The reduced tensor.
                     """
                     # Theano has a built-in optimization for logsumexp
                     # (see https://github.com/Theano/Theano/pull/4736)
                     # so we can just write the expression directly:
                     return T.log(T.sum(T.exp(x), axis=axis, keepdims=keepdims))


                 def round(x):
                     return T.round(x, mode='half_to_even')


                 def sign(x):
                     return T.sgn(x)


                 def pow(x, a):
                     return T.pow(x, a)


                 def clip(x, min_value, max_value):
                     if (isinstance(min_value, (int, float)) and
                             isinstance(max_value, (int, float))):
                         if max_value < min_value:
                             max_value = min_value
                     if min_value is None:
                         min_value = -np.inf
                     if max_value is None:
                         max_value = np.inf
                     return T.clip(x, min_value, max_value)


                 def equal(x, y):
                     return T.eq(x, y)


                 def not_equal(x, y):
                     z = T.neq(x, y)
                     if hasattr(x, '_keras_shape'):
                         z._keras_shape = x._keras_shape
                     elif hasattr(y, '_keras_shape'):
                         z._keras_shape = y._keras_shape
                     return z


                 def greater(x, y):
                     return T.gt(x, y)


                 def greater_equal(x, y):
                     return T.ge(x, y)


                 def less(x, y):
                     return T.lt(x, y)


                 def less_equal(x, y):
                     return T.le(x, y)


                 def maximum(x, y):
                     return T.maximum(x, y)


                 def minimum(x, y):
                     return T.minimum(x, y)


                 def sin(x):
                     return T.sin(x)


                 def cos(x):
                     return T.cos(x)


                 def normalize_batch_in_training(x, gamma, beta,
                                                 reduction_axes, epsilon=1e-3):
                     """Computes mean and std for batch then apply batch_normalization on batch.
                     """
                     # TODO remove this if statement when Theano without
                     # T.nnet.bn.batch_normalization_train is deprecated
                     if not hasattr(T.nnet.bn, 'batch_normalization_train'):
                         return _old_normalize_batch_in_training(
                             x, gamma, beta, reduction_axes, epsilon)

                     if gamma is None:
                         if beta is None:
                             gamma = ones_like(x)
                         else:
                             gamma = ones_like(beta)
                     if beta is None:
                         if gamma is None:
                             beta = zeros_like(x)
                         beta = zeros_like(gamma)

                     normed, mean, stdinv = T.nnet.bn.batch_normalization_train(
                         x, gamma, beta, reduction_axes, epsilon)

                     return normed, mean, T.inv(stdinv ** 2)


                 def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):
                     """Apply batch normalization on x given mean, var, beta and gamma.
                     """
                     # TODO remove this if statement when Theano without
                     # T.nnet.bn.batch_normalization_test is deprecated
                     if not hasattr(T.nnet.bn, 'batch_normalization_test'):
                         return _old_batch_normalization(x, mean, var, beta, gamma, epsilon)

                     if gamma is None:
                         gamma = ones_like(var)
                     if beta is None:
                         beta = zeros_like(mean)

                     if mean.ndim == 1:
                         # based on TensorFlow's default: normalize along rightmost dimension
                         reduction_axes = list(range(x.ndim - 1))
                     else:
                         reduction_axes = [i for i in range(x.ndim) if mean.broadcastable[i]]

                     return T.nnet.bn.batch_normalization_test(
                         x, gamma, beta, mean, var, reduction_axes, epsilon)


                 # TODO remove this function when Theano without
                 # T.nnet.bn.batch_normalization_train is deprecated
                 def _old_normalize_batch_in_training(x, gamma, beta, reduction_axes,
                                                      epsilon=1e-3):  # pragma: no cover
                     """Computes mean and std for batch then apply batch_normalization on batch.
                     """
                     if gamma is None:
                         gamma = ones_like(x)
                     if beta is None:
                         beta = zeros_like(x)

                     dev = theano.config.device
                     use_cudnn = (ndim(x) < 5 and
                                  reduction_axes == [0, 2, 3] and
                                  (dev.startswith('cuda') or dev.startswith('gpu')))
                     if use_cudnn:
                         broadcast_beta = beta.dimshuffle('x', 0, 'x', 'x')
                         broadcast_gamma = gamma.dimshuffle('x', 0, 'x', 'x')
                         try:
                             trained = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(
                                 x, broadcast_gamma, broadcast_beta, 'spatial', epsilon)
                             normed, mean, stdinv = trained
                             normed = theano.tensor.as_tensor_variable(normed)
                             mean = theano.tensor.as_tensor_variable(mean)
                             stdinv = theano.tensor.as_tensor_variable(stdinv)
                             var = T.inv(stdinv ** 2)
                             return normed, T.flatten(mean), T.flatten(var)
                         except AttributeError:
                             pass

                     var = x.var(reduction_axes)
                     mean = x.mean(reduction_axes)

                     target_shape = []
                     for axis in range(ndim(x)):
                         if axis in reduction_axes:
                             target_shape.append(1)
                         else:
                             target_shape.append(x.shape[axis])
                     target_shape = T.stack(*target_shape)

                     broadcast_mean = T.reshape(mean, target_shape)
                     broadcast_var = T.reshape(var, target_shape)
                     broadcast_beta = T.reshape(beta, target_shape)
                     broadcast_gamma = T.reshape(gamma, target_shape)
                     normed = batch_normalization(x, broadcast_mean, broadcast_var,
                                                  broadcast_beta, broadcast_gamma,
                                                  epsilon)
                     return normed, mean, var


                 # TODO remove this if statement when Theano without
                 # T.nnet.bn.batch_normalization_test is deprecated
                 def _old_batch_normalization(x, mean, var, beta, gamma,
                                              epsilon=1e-3):  # pragma: no cover
                     """Apply batch normalization on x given mean, var, beta and gamma.
                     """
                     if gamma is None:
                         gamma = ones_like(var)
                     if beta is None:
                         beta = zeros_like(mean)

                     if mean.ndim == 1 and x.ndim > 1:
                         # in TensorFlow's batch_normalization, if the parameters are vectors
                         # the batch normalization should be applied along the rightmost axis.
                         # Theano expects the parameters to always have x.ndim dimensions.
                         shuffle_pattern = ['x'] * (x.ndim - 1) + [0]
                         mean = mean.dimshuffle(shuffle_pattern)
                         var = var.dimshuffle(shuffle_pattern)
                         beta = beta.dimshuffle(shuffle_pattern)
                         gamma = gamma.dimshuffle(shuffle_pattern)

                     ndim = x.ndim
                     dev = theano.config.device
                     use_cudnn = ndim < 5 and (dev.startswith('cuda') or dev.startswith('gpu'))
                     if use_cudnn:
                         try:
                             axis = mean.broadcastable.index(False)
                             if axis != 1:
                                 shuffle_pattern = list(range(ndim))
                                 shuffle_pattern[1] = shuffle_pattern[axis]
                                 shuffle_pattern[axis] = 1
                                 result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(
                                     x.dimshuffle(shuffle_pattern),
                                     gamma.dimshuffle(shuffle_pattern),
                                     beta.dimshuffle(shuffle_pattern),
                                     mean.dimshuffle(shuffle_pattern),
                                     var.dimshuffle(shuffle_pattern),
                                     'spatial', epsilon).dimshuffle(shuffle_pattern)
                             else:
                                 result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(
                                     x, gamma, beta, mean, var, 'spatial', epsilon)
                             return theano.tensor.as_tensor_variable(result)
                         except AttributeError:
                             pass
                         except ValueError:
                             pass
                     return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var + epsilon),
                                                          mode='high_mem')


                 # SHAPE OPERATIONS

                 def concatenate(tensors, axis=-1):
                     if py_all([is_sparse(x) for x in tensors]):
                         axis = axis % ndim(tensors[0])
                         if axis == 0:
                             output = th_sparse_module.basic.vstack(tensors, format='csr')
                         elif axis == 1:
                             output = th_sparse_module.basic.hstack(tensors, format='csr')
                         else:
                             raise ValueError('Invalid concat axis for sparse matrix:', axis)
                     else:
                         output = T.concatenate([to_dense(x) for x in tensors], axis=axis)

                     if py_all([hasattr(tensor, '_keras_shape') for tensor in tensors]):
                         input_shapes = [tensor._keras_shape for tensor in tensors]
                         output_shape = list(input_shapes[0])
                         for shape in input_shapes[1:]:
                             if output_shape[axis] is None or shape[axis] is None:
                                 output_shape[axis] = None
                                 break
                             output_shape[axis] += shape[axis]
                         output._keras_shape = tuple(output_shape)

                     return output


                 def reshape(x, shape):
                     y = T.reshape(x, shape)
                     shape = tuple(x if isinstance(x, int) and x > 0 else None for x in shape)
                     y._keras_shape = shape
                     if hasattr(x, '_uses_learning_phase'):
                         y._uses_learning_phase = x._uses_learning_phase
                     else:
                         y._uses_learning_phase = False
                     return y


                 def permute_dimensions(x, pattern):
                     """Transpose dimensions.
                     pattern should be a tuple or list of
                     dimension indices, e.g. [0, 2, 1].
                     """
                     pattern = tuple(pattern)
                     y = x.dimshuffle(pattern)
                     if hasattr(x, '_keras_shape'):
                         y._keras_shape = tuple(np.asarray(x._keras_shape)[list(pattern)])
                     return y


                 def repeat_elements(x, rep, axis):
                     """Repeat the elements of a tensor along an axis, like np.repeat.
                     If x has shape (s1, s2, s3) and axis=1, the output
                     will have shape (s1, s2 * rep, s3).
                     """
                     y = T.repeat(x, rep, axis=axis)
                     if hasattr(x, '_keras_shape'):
                         y._keras_shape = list(x._keras_shape)
                         repeat_dim = x._keras_shape[axis]
                         if repeat_dim is not None:
                                 y._keras_shape[axis] = repeat_dim * rep
                         y._keras_shape = tuple(y._keras_shape)
                     return y


                 def resize_images(x,
                                   height_factor,
                                   width_factor,
                                   data_format,
                                   interpolation='nearest'):
                     """Resize the images contained in a 4D tensor of shape
                     - [batch, channels, height, width] (for 'channels_first' data_format)
                     - [batch, height, width, channels] (for 'channels_last' data_format)
                     by a factor of (height_factor, width_factor). Both factors should be
                     positive integers.
                     """
                     if data_format == 'channels_first':
                         axis_1 = 2
                         axis_2 = 3
                     elif data_format == 'channels_last':
                         axis_1 = 1
                         axis_2 = 2
                     else:
                         raise ValueError('Invalid data_format:', data_format)

                     if interpolation == 'nearest':
                         output = repeat_elements(x, height_factor, axis=axis_1)
                         output = repeat_elements(output, width_factor, axis=axis_2)
                     elif interpolation == 'bilinear':
                         if not (height_factor == width_factor == 2):
                             raise NotImplementedError(
                                 'Bilinear upscaling with factors other than (2, 2)'
                                 'is not available when using the Theano backend.')
                         if data_format == 'channels_last':
                             output = permute_dimensions(x, [0, 3, 1, 2])
                         else:
                             output = x
                         output = T.nnet.abstract_conv.bilinear_upsampling(output,
                                                                           ratio=height_factor)
                         if data_format == 'channels_last':
                             output = permute_dimensions(output, [0, 2, 3, 1])
                         if hasattr(x, '_keras_shape'):
                             output._keras_shape = list(x._keras_shape)
                             output._keras_shape[axis_1] *= height_factor
                             output._keras_shape[axis_2] *= width_factor
                             output._keras_shape = tuple(output._keras_shape)
                     else:
                         raise ValueError('interpolation should be one of "nearest" or "bilinear".')

                     return output


                 def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):
                     """Resize the volume contained in a 5D tensor of shape
                     - [batch, channels, depth, height, width] (for 'channels_first' data_format)
                     - [batch, depth, height, width, channels] (for 'channels_last' data_format)
                     by a factor of (depth_factor, height_factor, width_factor).
                     Both factors should be positive integers.
                     """
                     if data_format == 'channels_first':
                         output = repeat_elements(x, depth_factor, axis=2)
                         output = repeat_elements(output, height_factor, axis=3)
                         output = repeat_elements(output, width_factor, axis=4)
                         return output
                     elif data_format == 'channels_last':
                         output = repeat_elements(x, depth_factor, axis=1)
                         output = repeat_elements(output, height_factor, axis=2)
                         output = repeat_elements(output, width_factor, axis=3)
                         return output
                     else:
                         raise ValueError('Invalid data_format:', data_format)


                 def repeat(x, n):
                     """Repeat a 2D tensor.
                     If x has shape (samples, dim) and n=2,
                     the output will have shape (samples, 2, dim).
                     """
                     assert x.ndim == 2
                     y = x.dimshuffle((0, 'x', 1))
                     y = T.extra_ops.repeat(y, n, axis=1)
                     if hasattr(x, '_keras_shape'):
                         shape = list(x._keras_shape)
                         shape.insert(1, n)
                         y._keras_shape = tuple(shape)

                     return y


                 def arange(start, stop=None, step=1, dtype='int32'):
                     """Creates a 1-D tensor containing a sequence of integers.
                     The function arguments use the same convention as
                     Theano's arange: if only one argument is provided,
                     it is in fact the "stop" argument.
                     The default type of the returned tensor is 'int32' to
                     match TensorFlow's default.
                     """
                     return T.arange(start, stop=stop, step=step, dtype=dtype)


                 def tile(x, n):
                     if isinstance(n, int):
                         n = (n,)
                     elif isinstance(n, list):
                         n = tuple(n)

                     y = T.tile(x, n)
                     shape = int_shape(x)
                     if shape is None:
                         return y
                     elif len(n) < len(shape):  # Padding the axis
                         n = tuple([1 for _ in range(len(shape) - len(n))]) + n
                     elif len(n) != len(shape):
                         raise NotImplementedError

                     y._keras_shape = tuple([None if a is None else a * b
                                             for (a, b) in zip(shape, n)])
                     return y


                 def flatten(x):
                     y = T.flatten(x)
                     if hasattr(x, '_keras_shape'):
                         if None in x._keras_shape:
                             y._keras_shape = (None,)
                         else:
                             y._keras_shape = (np.prod(x._keras_shape), )
                     return y


                 def batch_flatten(x):
                     """Turn a n-D tensor into a 2D tensor where
                     the first dimension is conserved.
                     """
                     y = T.reshape(x, (x.shape[0], T.prod(x.shape[1:])))
                     if hasattr(x, '_keras_shape'):
                         if None in x._keras_shape[1:]:
                             y._keras_shape = (x._keras_shape[0], None)
                         else:
                             y._keras_shape = (x._keras_shape[0], np.prod(x._keras_shape[1:]))
                     return y


                 def expand_dims(x, axis=-1):
                     """Add a 1-sized dimension at index "dim".
                     """
                     pattern = [i for i in range(x.type.ndim)]
                     if axis < 0:
                         if x.type.ndim == 0:
                             axis = 0
                         else:
                             axis = axis % x.type.ndim + 1
                     pattern.insert(axis, 'x')
                     y = x.dimshuffle(pattern)
                     if hasattr(x, '_keras_shape'):
                         shape = list(x._keras_shape)
                         shape.insert(axis, 1)
                         y._keras_shape = tuple(shape)
                     return y


                 def squeeze(x, axis):
                     """Remove a 1-dimension from the tensor at index "axis".
                     """
                     shape = list(x.shape)
                     shape.pop(axis)
                     y = T.reshape(x, tuple(shape))
                     if hasattr(x, '_keras_shape'):
                         kshape = list(x._keras_shape)
                         kshape.pop(axis)
                         y._keras_shape = tuple(kshape)
                     return y


                 def temporal_padding(x, padding=(1, 1)):
                     """Pad the middle dimension of a 3D tensor
                     with "padding" zeros left and right.
                     Apologies for the inane API, but Theano makes this
                     really hard.
                     """
                     assert len(padding) == 2
                     input_shape = x.shape
                     output_shape = (input_shape[0],
                                     input_shape[1] + padding[0] + padding[1],
                                     input_shape[2])
                     output = T.zeros(output_shape)
                     result = T.set_subtensor(output[:, padding[0]:x.shape[1] + padding[0], :], x)
                     if hasattr(x, '_keras_shape'):
                         result._keras_shape = (x._keras_shape[0],
                                                x._keras_shape[1] + py_sum(padding),
                                                x._keras_shape[2])
                     return result


                 def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):
                     """Pad the 2nd and 3rd dimensions of a 4D tensor
                     with "padding[0]" and "padding[1]" (resp.) zeros left and right.
                     """
                     assert len(padding) == 2
                     assert len(padding[0]) == 2
                     assert len(padding[1]) == 2
                     top_pad, bottom_pad = padding[0]
                     left_pad, right_pad = padding[1]
                     data_format = normalize_data_format(data_format)

                     input_shape = x.shape
                     if data_format == 'channels_first':
                         output_shape = (input_shape[0],
                                         input_shape[1],
                                         input_shape[2] + top_pad + bottom_pad,
                                         input_shape[3] + left_pad + right_pad)
                         output = T.zeros(output_shape)
                         indices = (py_slice(None),
                                    py_slice(None),
                                    py_slice(top_pad, input_shape[2] + top_pad),
                                    py_slice(left_pad, input_shape[3] + left_pad))

                     else:
                         output_shape = (input_shape[0],
                                         input_shape[1] + top_pad + bottom_pad,
                                         input_shape[2] + left_pad + right_pad,
                                         input_shape[3])
                         output = T.zeros(output_shape)
                         indices = (py_slice(None),
                                    py_slice(top_pad, input_shape[1] + top_pad),
                                    py_slice(left_pad, input_shape[2] + left_pad),
                                    py_slice(None))
                     y = T.set_subtensor(output[indices], x)
                     if hasattr(x, '_keras_shape'):
                         if data_format == 'channels_first':
                             if x._keras_shape[2] is not None:
                                 h = x._keras_shape[2] + top_pad + bottom_pad
                             else:
                                 h = None
                             if x._keras_shape[3] is not None:
                                 w = x._keras_shape[3] + left_pad + right_pad
                             else:
                                 w = None
                             output_keras_shape = (x._keras_shape[0],
                                                   x._keras_shape[1],
                                                   h,
                                                   w)
                         else:
                             if x._keras_shape[1] is not None:
                                 h = x._keras_shape[1] + top_pad + bottom_pad
                             else:
                                 h = None
                             if x._keras_shape[2] is not None:
                                 w = x._keras_shape[2] + left_pad + right_pad
                             else:
                                 w = None
                             output_keras_shape = (x._keras_shape[0],
                                                   h,
                                                   w,
                                                   x._keras_shape[3])
                         y._keras_shape = output_keras_shape
                     return y


                 def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):
                     """Pad the 2nd, 3rd and 4th dimensions of a 5D tensor
                     with "padding[0]", "padding[1]" and "padding[2]" (resp.) zeros left and right.
                     """
                     data_format = normalize_data_format(data_format)

                     input_shape = x.shape
                     if data_format == 'channels_first':
                         output_shape = (input_shape[0],
                                         input_shape[1],
                                         input_shape[2] + padding[0][0] + padding[0][1],
                                         input_shape[3] + padding[1][0] + padding[1][1],
                                         input_shape[4] + padding[2][0] + padding[2][1])
                         output = T.zeros(output_shape)
                         indices = (py_slice(None),
                                    py_slice(None),
                                    py_slice(padding[0][0], input_shape[2] + padding[0][0]),
                                    py_slice(padding[1][0], input_shape[3] + padding[1][0]),
                                    py_slice(padding[2][0], input_shape[4] + padding[2][0]))

                     else:
                         output_shape = (input_shape[0],
                                         input_shape[1] + padding[0][0] + padding[0][1],
                                         input_shape[2] + padding[1][0] + padding[1][1],
                                         input_shape[3] + padding[2][0] + padding[2][1],
                                         input_shape[4])
                         output = T.zeros(output_shape)
                         indices = (py_slice(None),
                                    py_slice(padding[0][0], input_shape[1] + padding[0][0]),
                                    py_slice(padding[1][0], input_shape[2] + padding[1][0]),
                                    py_slice(padding[2][0], input_shape[3] + padding[2][0]),
                                    py_slice(None))
                     y = T.set_subtensor(output[indices], x)
                     if hasattr(x, '_keras_shape'):
                         if data_format == 'channels_first':
                             if x._keras_shape[2] is not None:
                                 h = x._keras_shape[2] + padding[0][0] + padding[0][1]
                             else:
                                 h = None
                             if x._keras_shape[3] is not None:
                                 w = x._keras_shape[3] + padding[1][0] + padding[1][1]
                             else:
                                 w = None
                             if x._keras_shape[4] is not None:
                                 d = x._keras_shape[4] + padding[2][0] + padding[2][1]
                             else:
                                 d = None
                             output_keras_shape = (x._keras_shape[0],
                                                   x._keras_shape[1],
                                                   h,
                                                   w,
                                                   d)
                         else:
                             if x._keras_shape[1] is not None:
                                 h = x._keras_shape[1] + padding[0][0] + padding[0][1]
                             else:
                                 h = None
                             if x._keras_shape[2] is not None:
                                 w = x._keras_shape[2] + padding[1][0] + padding[1][1]
                             else:
                                 w = None
                             if x._keras_shape[3] is not None:
                                 d = x._keras_shape[3] + padding[2][0] + padding[2][1]
                             else:
                                 d = None
                             output_keras_shape = (x._keras_shape[0],
                                                   h,
                                                   w,
                                                   d,
                                                   x._keras_shape[4])
                         y._keras_shape = output_keras_shape
                     return y


                 def stack(x, axis=0):
                     return T.stack(x, axis=axis)


                 def one_hot(indices, num_classes):
                     """Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))
                     Output: (n + 1)D one hot representation of the input
                     with shape (batch_size, dim1, dim2, ... dim(n-1), num_classes)
                     """
                     input_shape = tuple((indices.shape[i] for i in range(indices.ndim)))
                     indices = T.flatten(indices)
                     oh = T.extra_ops.to_one_hot(indices, num_classes)
                     oh = T.reshape(oh, input_shape + (num_classes,))
                     return oh


                 def reverse(x, axes):
                     """Reverse a tensor along the specified axes
                     """
                     if isinstance(axes, int):
                         axes = [axes]
                     elif isinstance(axes, tuple):
                         axes = list(axes)
                     for i in range(len(axes)):
                         if axes[i] == -1:
                             axes[i] = x.ndim - 1
                     slices = []
                     for i in range(x.ndim):
                         if i in axes:
                             slices.append(py_slice(None, None, -1))
                         else:
                             slices.append(py_slice(None, None, None))
                     return x[slices]


                 def slice(x, start, size):
                     if not (len(int_shape(x)) == len(start) == len(size)):
                         raise ValueError('The dimension and the size of indices should match.')
                     out = x[tuple([py_slice(i, i + j) for (i, j) in zip(start, size)])]
                     out._keras_shape = tuple(size)
                     return out


                 def pattern_broadcast(x, broadcastable):
                     return T.patternbroadcast(x, broadcastable)

                 # VALUE MANIPULATION


                 def get_value(x):
                     if not hasattr(x, 'get_value'):
                         raise TypeError('`get_value` can only be called on a variable. '
                                         'If you have an expression instead, use `eval()`.')
                     return x.get_value()


                 def batch_get_value(xs):
                     """Returns the value of more than one tensor variable,
                     as a list of Numpy arrays.
                     """
                     return [get_value(x) for x in xs]


                 def set_value(x, value):
                     x.set_value(np.asarray(value, dtype=x.dtype))


                 def batch_set_value(tuples):
                     for x, value in tuples:
                         x.set_value(np.asarray(value, dtype=x.dtype))


                 def get_variable_shape(x):
                     return x.get_value(borrow=True, return_internal_type=True).shape


                 def print_tensor(x, message=''):
                     """Print the message and the tensor when evaluated and return the same
                     tensor.
                     """
                     p_op = Print(message)
                     return p_op(x)


                 # GRAPH MANIPULATION

                 class Function(object):

                     def __init__(self, inputs, outputs, updates=[], name=None, **kwargs):
                         unique_variables_to_update = {}
                         for v, nv in updates:
                             if v not in unique_variables_to_update:
                                 unique_variables_to_update[v] = nv
                         updates = unique_variables_to_update.items()
                         self.function = theano.function(inputs, outputs, updates=updates,
                                                         allow_input_downcast=True,
                                                         on_unused_input='ignore',
                                                         name=name,
                                                         **kwargs)
                         self.name = name

                     def __call__(self, inputs):
                         assert isinstance(inputs, (list, tuple))
                         return self.function(*inputs)


                 def _raise_invalid_arg(key):
                     msg = 'Invalid argument "%s" passed to K.function with Theano backend' % key
                     raise ValueError(msg)


                 def function(inputs, outputs, updates=[], **kwargs):
                     if len(kwargs) > 0:
                         for key in kwargs.keys():
                             if not has_arg(theano.function, key, True):
                                 _raise_invalid_arg(key)
                     return Function(inputs, outputs, updates=updates, **kwargs)


                 def gradients(loss, variables):
                     return T.grad(loss, variables)


                 def stop_gradient(variables):
                     """Returns `variables` but with zero gradient w.r.t. every other variable.
                     # Arguments
                         variables: tensor or list of tensors to consider constant with respect
                             to any other variable.
                     # Returns
                         A single tensor or a list of tensors (depending on the passed argument)
                             that has constant gradient with respect to any other variable.
                     """
                     if isinstance(variables, (list, tuple)):
                         return map(theano.gradient.disconnected_grad, variables)
                     else:
                         return theano.gradient.disconnected_grad(variables)


                 # CONTROL FLOW

                 def rnn(step_function, inputs, initial_states,
                         go_backwards=False, mask=None, constants=None,
                         unroll=False, input_length=None):
                     """Iterates over the time dimension of a tensor.
                     # Arguments
                         step_function:
                             Parameters:
                                 inputs: Tensor with shape (samples, ...) (no time dimension),
                                     representing input for the batch of samples at a certain
                                     time step.
                                 states: List of tensors.
                             Returns:
                                 outputs: Tensor with shape (samples, ...) (no time dimension),
                                 new_states: List of tensors, same length and shapes
                                     as 'states'.
                         inputs: Tensor of temporal data of shape (samples, time, ...)
                             (at least 3D).
                         initial_states: Tensor with shape (samples, ...) (no time dimension),
                             containing the initial values for the states used in
                             the step function.
                         go_backwards: Boolean. If True, do the iteration over the time
                             dimension in reverse order and return the reversed sequence.
                         mask: Binary tensor with shape (samples, time),
                             with a zero for every element that is masked.
                         constants: A list of constant values passed at each step.
                         unroll: Whether to unroll the RNN or to use a symbolic loop
                             (`while_loop` or `scan` depending on backend).
                         input_length: Static number of timesteps in the input.
                             Must be specified if using `unroll`.
                     # Returns
                         A tuple (last_output, outputs, new_states).
                         last_output: The latest output of the rnn, of shape `(samples, ...)`
                         outputs: Tensor with shape `(samples, time, ...)` where each
                             entry `outputs[s, t]` is the output of the step function
                             at time `t` for sample `s`.
                         new_states: List of tensors, latest states returned by
                             the step function, of shape `(samples, ...)`.
                     """
                     ndim = inputs.ndim
                     assert ndim >= 3, 'Input should be at least 3D.'

                     if unroll:
                         if input_length is None:
                             raise ValueError('When specifying `unroll=True`, '
                                              'an `input_length` '
                                              'must be provided to `rnn`.')

                     axes = [1, 0] + list(range(2, ndim))
                     inputs = inputs.dimshuffle(axes)

                     if constants is None:
                         constants = []

                     global uses_learning_phase
                     uses_learning_phase = False

                     if mask is not None:
                         if mask.ndim != 2:
                             raise ValueError(
                                 'mask should have `shape=(samples, time)`, '
                                 'got {}'.format(mask.shape))
                         mask = mask.dimshuffle([1, 0])

                         def get_matching_mask(mask_t, ref_tensor_t):
                             # tf.where needs its condition tensor
                             # to be the same shape as its two
                             # result tensors
                             ndim = ref_tensor_t.ndim
                             for _ in range(ndim - 1):
                                 mask_t = expand_dims(mask_t)
                             add_shape = ref_tensor_t.shape[1:]
                             reps = T.concatenate([[1], add_shape], 0)
                             return T.tile(mask_t, reps, ndim=ndim)

                         if unroll:
                             indices = list(range(input_length))
                             if go_backwards:
                                 indices = indices[::-1]

                             successive_outputs = []
                             successive_states = []
                             states = initial_states
                             for i in indices:
                                 output, new_states = step_function(inputs[i], states + constants)
                                 if getattr(output, '_uses_learning_phase', False):
                                     uses_learning_phase = True

                                 if len(successive_outputs) == 0:
                                     prev_output = zeros_like(output)
                                 else:
                                     prev_output = successive_outputs[-1]

                                 output_mask = get_matching_mask(mask[i], output)
                                 output = T.switch(output_mask, output, prev_output)
                                 kept_states = []
                                 for state, new_state in zip(states, new_states):
                                     state_mask = get_matching_mask(mask[i], state)
                                     kept_states.append(T.switch(state_mask, new_state, state))
                                 states = kept_states

                                 successive_outputs.append(output)
                                 successive_states.append(states)

                             outputs = T.stack(*successive_outputs)
                             states = []
                             for i in range(len(successive_states[-1])):
                                 new_states = []
                                 for states_at_step in successive_states:
                                     new_states.append(states_at_step[i])
                                 states.append(T.stack(*new_states))
                         else:
                             # build an all-zero tensor of shape (samples, output_dim)
                             initial_output = step_function(inputs[0], initial_states + constants)
                             initial_output = initial_output[0] * 0
                             # Theano gets confused by broadcasting patterns in the scan op
                             initial_output = T.unbroadcast(initial_output, 0, 1)
                             if len(initial_states) > 0:
                                 initial_states[0] = T.unbroadcast(initial_states[0], 0, 1)

                             def _step(inputs, mask, output_tm1, *states):
                                 outputs, new_states = step_function(inputs, states)
                                 if getattr(outputs, '_uses_learning_phase', False):
                                     global uses_learning_phase
                                     uses_learning_phase = True
                                 # output previous output if masked.
                                 output_mask = get_matching_mask(mask, outputs)
                                 outputs = T.switch(output_mask, outputs, output_tm1)
                                 return_states = []
                                 for state, new_state in zip(states, new_states):
                                     state_mask = get_matching_mask(mask, state)
                                     return_states.append(T.switch(state_mask, new_state, state))
                                 return [outputs] + return_states

                             results, _ = theano.scan(
                                 _step,
                                 sequences=[inputs, mask],
                                 outputs_info=[initial_output] + initial_states,
                                 non_sequences=constants,
                                 go_backwards=go_backwards)

                             # deal with Theano API inconsistency
                             if isinstance(results, list):
                                 outputs = results[0]
                                 states = results[1:]
                             else:
                                 outputs = results
                                 states = []
                     else:
                         if unroll:
                             indices = list(range(input_length))
                             if go_backwards:
                                 indices = indices[::-1]

                             successive_outputs = []
                             successive_states = []
                             states = initial_states
                             for i in indices:
                                 outputs, states = step_function(inputs[i], states + constants)
                                 if getattr(outputs, '_uses_learning_phase', False):
                                     uses_learning_phase = True
                                 successive_outputs.append(outputs)
                                 successive_states.append(states)
                             outputs = T.stack(*successive_outputs)
                             states = []
                             for i in range(len(successive_states[-1])):
                                 states.append(T.stack(
                                     *[states_at_step[i] for states_at_step in successive_states]))

                         else:
                             def _step(inputs, *states):
                                 outputs, new_states = step_function(inputs, states)
                                 if getattr(outputs, '_uses_learning_phase', False):
                                     global uses_learning_phase
                                     uses_learning_phase = True
                                 return [outputs] + new_states

                             # Theano likes to make shape==1 dimensions
                             # in the initial states (outputs_info) broadcastable
                             if len(initial_states) > 0:
                                 initial_states[0] = T.unbroadcast(initial_states[0], 0, 1)

                             results, _ = theano.scan(
                                 _step,
                                 sequences=inputs,
                                 outputs_info=[None] + initial_states,
                                 non_sequences=constants,
                                 go_backwards=go_backwards)

                             # deal with Theano API inconsistency
                             if isinstance(results, list):
                                 outputs = results[0]
                                 states = results[1:]
                             else:
                                 outputs = results
                                 states = []

                     outputs = T.squeeze(outputs)
                     last_output = outputs[-1]

                     axes = [1, 0] + list(range(2, outputs.ndim))
                     outputs = outputs.dimshuffle(axes)
                     states = [T.squeeze(state[-1]) for state in states]
                     last_output._uses_learning_phase = uses_learning_phase
                     return last_output, outputs, states


                 def switch(condition, then_expression, else_expression):
                     """Switches between two operations depending on a scalar value.
                     Note that both `then_expression` and `else_expression`
                     should be symbolic tensors of the *same shape*.
                     # Arguments
                         condition: scalar tensor (`int` or `bool`).
                         then_expression: either a tensor, or a callable that returns a tensor.
                         else_expression: either a tensor, or a callable that returns a tensor.
                     # Returns
                         The selected tensor.
                     """
                     if callable(then_expression):
                         then_expression = then_expression()
                     if callable(else_expression):
                         else_expression = else_expression()
                     cond_ndim = ndim(condition)
                     expr_ndim = ndim(then_expression)
                     if cond_ndim < expr_ndim:
                         ndim_diff = expr_ndim - cond_ndim
                         for _ in range(ndim_diff):
                             condition = expand_dims(condition)
                     return T.switch(condition, then_expression, else_expression)


                 def in_train_phase(x, alt, training=None):
                     """Selects `x` in train phase, and `alt` otherwise.
                     Note that `alt` should have the *same shape* as `x`.
                     # Returns
                         Either `x` or `alt` based on the `training` flag.
                         the `training` flag defaults to `K.learning_phase()`.
                     """
                     if training is None:
                         training = learning_phase()
                         uses_learning_phase = True
                     else:
                         uses_learning_phase = False

                     if training is 1 or training is True:
                         if callable(x):
                             return x()
                         else:
                             return x

                     elif training is 0 or training is False:
                         if callable(alt):
                             return alt()
                         else:
                             return alt

                     if callable(x):
                         x = x()
                     if callable(alt):
                         alt = alt()

                     # else: assume learning phase is a placeholder tensor.
                     x = ifelse(training, x, alt)
                     if uses_learning_phase:
                         x._uses_learning_phase = True
                     return x


                 def in_test_phase(x, alt, training=None):
                     """Selects `x` in test phase, and `alt` otherwise.
                     Note that `alt` should have the *same shape* as `x`.
                     # Returns
                         Either `x` or `alt` based on `K.learning_phase`.
                     """
                     return in_train_phase(alt, x, training=training)


                 # NN OPERATIONS

                 def _assert_has_capability(module, func):
                     if not hasattr(module, func):
                         raise EnvironmentError(
                             'It looks like like your version of '
                             'Theano is out of date. '
                             'Install the latest version with:\n'
                             'pip install git+git://github.com/Theano/Theano.git '
                             '--upgrade --no-deps')


                 def elu(x, alpha=1.0):
                     """ Exponential linear unit
                     # Arguments
                         x: Tensor to compute the activation function for.
                         alpha: scalar
                     """
                     _assert_has_capability(T.nnet, 'elu')
                     return T.nnet.elu(x, alpha)


                 def relu(x, alpha=0., max_value=None, threshold=0.):
                     _assert_has_capability(T.nnet, 'relu')

                     if alpha != 0.:
                         if threshold != 0.:
                             negative_part = T.nnet.relu(-x + threshold)
                         else:
                             negative_part = T.nnet.relu(-x)

                     if threshold != 0.:
                         x = x * T.cast(T.gt(x, threshold), floatx())
                     else:
                         x = T.nnet.relu(x)

                     if max_value is not None:
                         x = T.clip(x, 0.0, max_value)

                     if alpha != 0.:
                         x -= alpha * negative_part

                     return x


                 def softmax(x, axis=-1):
                     if (axis == -1 or axis == x.ndim - 1) and x.ndim == 2:
                         return T.nnet.softmax(x)
                     xm = x.max(axis=axis, keepdims=True)
                     return T.exp(x - xm) / T.exp(
                         x - xm).sum(axis=axis, keepdims=True)


                 def softplus(x):
                     return T.nnet.softplus(x)


                 def softsign(x):
                     return T_softsign(x)


                 def categorical_crossentropy(target, output, from_logits=False, axis=-1):
                     output_dimensions = list(range(len(int_shape(output))))
                     if axis != -1 and axis not in output_dimensions:
                         raise ValueError(
                             '{}{}{}'.format(
                                 'Unexpected channels axis {}. '.format(axis),
                                 'Expected to be -1 or one of the axes of `output`, ',
                                 'which has {} dimensions.'.format(len(int_shape(output)))))
                     # If the channels are not in the last axis, move them to be there:
                     if axis != -1 and axis != output_dimensions[-1]:
                         permutation = output_dimensions[:axis]
                         permutation += output_dimensions[axis + 1:] + [axis]
                         output = permute_dimensions(output, permutation)
                         target = permute_dimensions(target, permutation)
                     if from_logits:
                         output = T.nnet.softmax(output)
                     else:
                         # scale preds so that the class probas of each sample sum to 1
                         output /= output.sum(axis=-1, keepdims=True)
                     # avoid numerical instability with _EPSILON clipping
                     output = T.clip(output, epsilon(), 1.0 - epsilon())
                     return T.nnet.categorical_crossentropy(output, target)


                 def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):
                     output_dimensions = list(range(len(int_shape(output))))
                     if axis != -1 and axis not in output_dimensions:
                         raise ValueError(
                             '{}{}{}'.format(
                                 'Unexpected channels axis {}. '.format(axis),
                                 'Expected to be -1 or one of the axes of `output`, ',
                                 'which has {} dimensions.'.format(len(int_shape(output)))))
                     # If the channels are not in the last axis, move them to be there:
                     if axis != -1 and axis != output_dimensions[-1]:
                         permutation = output_dimensions[:axis]
                         permutation += output_dimensions[axis + 1:] + [axis]
                         output = permute_dimensions(output, permutation)
                         target = permute_dimensions(target, permutation)
                     target = T.cast(T.flatten(target), 'int32')
                     target = T.extra_ops.to_one_hot(target, nb_class=output.shape[-1])
                     target = reshape(target, shape(output))
                     return categorical_crossentropy(target, output, from_logits, axis=-1)


                 def binary_crossentropy(target, output, from_logits=False):
                     if from_logits:
                         output = T.nnet.sigmoid(output)
                     # avoid numerical instability with _EPSILON clipping
                     output = T.clip(output, epsilon(), 1.0 - epsilon())
                     return T.nnet.binary_crossentropy(output, target)


                 def sigmoid(x):
                     return T.nnet.sigmoid(x)


                 def hard_sigmoid(x):
                     return T.nnet.hard_sigmoid(x)


                 def tanh(x):
                     return T.tanh(x)


                 def dropout(x, level, noise_shape=None, seed=None):
                     """Sets entries in `x` to zero at random,
                     while scaling the entire tensor.
                     # Arguments
                         x: tensor
                         level: fraction of the entries in the tensor
                             that will be set to 0.
                         noise_shape: shape for randomly generated keep/drop flags,
                             must be broadcastable to the shape of `x`
                         seed: random seed to ensure determinism.
                     """
                     if level < 0. or level >= 1:
                         raise ValueError('Dropout level must be in interval [0, 1[.')
                     if seed is None:
                         seed = np.random.randint(1, 10e6)
                     if isinstance(noise_shape, list):
                         noise_shape = tuple(noise_shape)

                     rng = RandomStreams(seed=seed)
                     retain_prob = 1. - level

                     if noise_shape is None:
                         random_tensor = rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)
                     else:
                         random_tensor = rng.binomial(noise_shape, p=retain_prob, dtype=x.dtype)
                         random_tensor = T.patternbroadcast(random_tensor,
                                                            [dim == 1 for dim in noise_shape])
                     x *= random_tensor
                     x /= retain_prob
                     return x


                 def l2_normalize(x, axis=None):
                     square_sum = T.sum(T.square(x), axis=axis, keepdims=True)
                     norm = T.sqrt(T.maximum(square_sum, epsilon()))
                     return x / norm


                 def in_top_k(predictions, targets, k):
                     """Returns whether the `targets` are in the top `k` `predictions`.
                     # Arguments
                         predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
                         targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
                         k: An `int`, number of top elements to consider.
                     # Returns
                         A 1D tensor of length `batch_size` and type `bool`.
                         `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
                         values of `predictions[i]`.
                     """
                     # handle k < 1 and k >= predictions.shape[1] cases to match TF behavior
                     if k < 1:
                         # dtype='bool' is only available since Theano 0.9.0
                         try:
                             return T.zeros_like(targets, dtype='bool')
                         except TypeError:
                             return T.zeros_like(targets, dtype='int8')

                     if k >= int_shape(predictions)[1]:
                         try:
                             return T.ones_like(targets, dtype='bool')
                         except TypeError:
                             return T.ones_like(targets, dtype='int8')

                     predictions_k = T.sort(predictions)[:, -k]
                     targets_values = predictions[T.arange(targets.shape[0]), targets]
                     return T.ge(targets_values, predictions_k)


                 # CONVOLUTIONS

                 def _preprocess_conv2d_input(x, data_format):
                     if data_format == 'channels_last':
                         # TF uses the last dimension as channel dimension,
                         # instead of the 2nd one.
                         # TH input shape: (samples, input_depth, rows, cols)
                         # TF input shape: (samples, rows, cols, input_depth)
                         x = x.dimshuffle((0, 3, 1, 2))
                     return x


                 def _preprocess_conv3d_input(x, data_format):
                     if data_format == 'channels_last':
                         # TF uses the last dimension as channel dimension,
                         # instead of the 2nd one.
                         # TH input shape: (samples, input_depth, rows, cols, slices)
                         # TF input shape: (samples, rows, cols, slices, input_depth)
                         x = x.dimshuffle((0, 4, 1, 2, 3))
                     return x


                 def _preprocess_conv2d_kernel(kernel, data_format):
                     # As of Keras 2.0.0, all kernels are normalized
                     # on the format `(rows, cols, input_depth, depth)`,
                     # independently of `data_format`.
                     # Theano expects `(depth, input_depth, rows, cols)`.
                     kernel = kernel.dimshuffle((3, 2, 0, 1))
                     return kernel


                 def _preprocess_conv2d_depthwise_kernel(kernel, kernel_shape, data_format):
                     # As of Keras 2.0.0, all kernels are normalized
                     # on the format `(rows, cols, input_depth, depth)`,
                     # independently of `data_format`.
                     # Theano expects `(input_depth * depth, 1, rows, cols)`
                     # for depthwise convolution.
                     kernel = kernel[::-1, ::-1, :, :]
                     kernel = kernel.dimshuffle((2, 3, 0, 1))
                     kernel = reshape(kernel, kernel_shape)
                     return kernel


                 def _preprocess_conv3d_kernel(kernel, data_format):
                     # As of Keras 2.0.0, all kernels are normalized
                     # on the format `(space, input_depth, depth)`,
                     # independently of `data_format`.
                     # Theano expects `(depth, input_depth, space)`.
                     kernel = kernel.dimshuffle((4, 3, 0, 1, 2))
                     return kernel


                 def _preprocess_padding(padding):
                     if padding == 'same':
                         th_padding = 'half'
                     elif padding == 'valid':
                         th_padding = 'valid'
                     elif padding == 'full':
                         th_padding = 'full'
                     else:
                         raise ValueError('Border mode not supported:', str(padding))
                     return th_padding


                 def _preprocess_conv2d_image_shape(image_shape, data_format):
                     # Theano might not accept long type
                     def int_or_none(value):
                         try:
                             return int(value)
                         except TypeError:
                             return None
                     if data_format == 'channels_last':
                         if image_shape:
                             image_shape = transpose_shape(image_shape, 'channels_first',
                                                           spatial_axes=(1, 2))
                     if image_shape is not None:
                         image_shape = tuple(int_or_none(v) for v in image_shape)
                     return image_shape


                 def _preprocess_conv3d_volume_shape(volume_shape, data_format):
                     # Theano might not accept long type
                     def int_or_none(value):
                         try:
                             return int(value)
                         except TypeError:
                             return None
                     if data_format == 'channels_last':
                         if volume_shape:
                             volume_shape = (volume_shape[0], volume_shape[4],
                                             volume_shape[1], volume_shape[2], volume_shape[3])
                     if volume_shape is not None:
                         volume_shape = tuple(int_or_none(v) for v in volume_shape)
                     return volume_shape


                 def _preprocess_conv2d_filter_shape(filter_shape, data_format):
                     # Theano might not accept long type
                     def int_or_none(value):
                         try:
                             return int(value)
                         except TypeError:
                             return None
                     if filter_shape:
                         filter_shape = (filter_shape[3], filter_shape[2],
                                         filter_shape[0], filter_shape[1])
                     if filter_shape is not None:
                         filter_shape = tuple(int_or_none(v) for v in filter_shape)
                     return filter_shape


                 def _preprocess_conv2d_depthwise_filter_shape(filter_shape, data_format):
                     # Theano might not accept long type
                     def int_or_none(value):
                         try:
                             return int(value)
                         except TypeError:
                             return None
                     if filter_shape:
                         filter_shape = (filter_shape[3] * filter_shape[2], 1,
                                         filter_shape[0], filter_shape[1])
                     if filter_shape is not None:
                         filter_shape = tuple(int_or_none(v) for v in filter_shape)
                     return filter_shape


                 def _preprocess_conv3d_filter_shape(filter_shape, data_format):
                     # Theano might not accept long type
                     def int_or_none(value):
                         try:
                             return int(value)
                         except TypeError:
                             return None
                     if filter_shape:
                         filter_shape = (filter_shape[4], filter_shape[3],
                                         filter_shape[0], filter_shape[1], filter_shape[2])
                     if filter_shape is not None:
                         filter_shape = tuple(int_or_none(v) for v in filter_shape)
                     return filter_shape


                 def _postprocess_conv2d_output(conv_out, x,
                                                padding, kernel_shape,
                                                strides, data_format):
                     if padding == 'same':
                         if kernel_shape[2] % 2 == 0:
                             i = (x.shape[2] + strides[0] - 1) // strides[0]
                             conv_out = conv_out[:, :, :i, :]
                         if kernel_shape[3] % 2 == 0:
                             i = (x.shape[3] + strides[1] - 1) // strides[1]
                             conv_out = conv_out[:, :, :, :i]
                     if data_format == 'channels_last':
                         conv_out = conv_out.dimshuffle((0, 2, 3, 1))
                     return conv_out


                 def _postprocess_conv3d_output(conv_out, x,
                                                padding, kernel_shape,
                                                strides, data_format):
                     if padding == 'same':
                         if kernel_shape[2] % 2 == 0:
                             i = (x.shape[2] + strides[0] - 1) // strides[0]
                             conv_out = conv_out[:, :, :i, :, :]
                         if kernel_shape[3] % 2 == 0:
                             i = (x.shape[3] + strides[1] - 1) // strides[1]
                             conv_out = conv_out[:, :, :, :i, :]
                         if kernel_shape[4] % 2 == 0:
                             i = (x.shape[4] + strides[2] - 1) // strides[2]
                             conv_out = conv_out[:, :, :, :, :i]
                     if data_format == 'channels_last':
                         conv_out = conv_out.dimshuffle((0, 2, 3, 4, 1))
                     return conv_out


                 def conv1d(x, kernel, strides=1, padding='valid',
                            data_format=None, dilation_rate=1):
                     """1D convolution.
                     # Arguments
                         kernel: kernel tensor.
                         strides: stride integer.
                         padding: string, `"same"`, `"causal"` or `"valid"`.
                         data_format: string, one of "channels_last", "channels_first"
                         dilation_rate: integer.
                     """
                     data_format = normalize_data_format(data_format)

                     kernel_shape = int_shape(kernel)
                     if padding == 'causal':
                         # causal (dilated) convolution:
                         if not kernel_shape:
                             raise AttributeError('Causal padding requires kernel._keras_shape set.')
                         left_pad = dilation_rate * (kernel_shape[0] - 1)
                         x = temporal_padding(x, (left_pad, 0))
                         padding = 'valid'
                     shape = int_shape(x)
                     if data_format == 'channels_last':
                         # original shape: (batch, length, input_dim)
                         # add dim to x to have (batch, length, 1, input_dim)
                         x = expand_dims(x, 2)
                         # update x._keras_shape
                         if shape is not None:
                             x._keras_shape = (shape[0], shape[1], 1, shape[2])
                     else:
                         # original shape: (batch, input_dim, length)
                         # add dim to x to have (batch, input_dim, length, 1)
                         x = expand_dims(x, 3)
                         # update x._keras_shape
                         if shape is not None:
                             x._keras_shape = (shape[0], shape[1], shape[2], 1)
                     # update dilation rate, strides
                     dilation_rate = (dilation_rate, 1)
                     strides = (strides, 1)
                     # add dim to kernel (always same format independently of data_format)
                     # i.e. (rows, 1, input_depth, depth)
                     kernel = expand_dims(kernel, 1)
                     output = conv2d(x, kernel,
                                     strides=strides, padding=padding,
                                     data_format=data_format, dilation_rate=dilation_rate)
                     # remove added dim
                     if data_format == 'channels_last':
                         output = squeeze(output, 2)
                     else:
                         output = squeeze(output, 3)
                     return output


                 def conv2d(x, kernel, strides=(1, 1), padding='valid',
                            data_format=None, dilation_rate=(1, 1)):
                     """2D convolution.
                     # Arguments
                         kernel: kernel tensor.
                         strides: strides tuple.
                         padding: string, "same" or "valid".
                         data_format: "channels_last" or "channels_first".
                             Whether to use Theano or TensorFlow data format
                         in inputs/kernels/outputs.
                     """
                     data_format = normalize_data_format(data_format)

                     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)
                     kernel_shape = int_shape(kernel)
                     if kernel_shape is None:
                         kernel_shape = kernel.eval().shape  # in case of a shared variable
                     kernel_shape = _preprocess_conv2d_filter_shape(kernel_shape, data_format)

                     x = _preprocess_conv2d_input(x, data_format)
                     kernel = _preprocess_conv2d_kernel(kernel, data_format)
                     th_padding = _preprocess_padding(padding)

                     conv_out = T.nnet.conv2d(x, kernel,
                                              border_mode=th_padding,
                                              subsample=strides,
                                              input_shape=image_shape,
                                              filter_shape=kernel_shape,
                                              filter_dilation=dilation_rate)
                     conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                                           kernel_shape, strides, data_format)
                     return conv_out


                 def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                                      padding='valid', data_format=None, dilation_rate=(1, 1)):
                     """2D deconvolution (transposed convolution).
                     # Arguments
                         kernel: kernel tensor.
                         output_shape: desired dimensions of output.
                         strides: strides tuple.
                         padding: string, "same" or "valid".
                         data_format: "channels_last" or "channels_first".
                             Whether to use Theano or TensorFlow data format
                             in inputs/kernels/outputs.
                         dilation_rate: tuple of 2 integers.
                     # Raises
                         ValueError: if using an even kernel size with padding 'same'.
                     """
                     flip_filters = False
                     data_format = normalize_data_format(data_format)

                     if data_format == 'channels_last':
                         output_shape = (output_shape[0],
                                         output_shape[3],
                                         output_shape[1],
                                         output_shape[2])

                     kernel_shape = int_shape(kernel)
                     if kernel_shape is None:
                         kernel_shape = kernel.eval().shape  # in case of a shared variable

                     if padding == 'same' and kernel_shape[0] % 2 == 0:
                         raise ValueError('In `Conv2DTranspose`, with padding mode `same`, '
                                          'even kernel sizes are not supported with Theano. '
                                          'You can set `kernel_size` to an odd number.')

                     kernel_shape = _preprocess_conv2d_filter_shape(kernel_shape, data_format)

                     x = _preprocess_conv2d_input(x, data_format)
                     kernel = _preprocess_conv2d_kernel(kernel, data_format)

                     th_padding = _preprocess_padding(padding)
                     op = T.nnet.abstract_conv.AbstractConv2d_gradInputs(
                         imshp=None,
                         kshp=kernel_shape,
                         subsample=strides,
                         border_mode=th_padding,
                         filter_flip=not flip_filters,
                         filter_dilation=dilation_rate)
                     conv_out = op(kernel, x, output_shape[2:])
                     conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                                           kernel_shape, strides, data_format)
                     return conv_out


                 def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                                      padding='valid', data_format=None, dilation_rate=1):
                     """1D convolution with separable filters.
                     # Arguments
                         x: input tensor
                         depthwise_kernel: convolution kernel for the depthwise convolution.
                         pointwise_kernel: kernel for the 1x1 convolution.
                         strides: strides integer.
                         padding: string, `"same"` or `"valid"`.
                         data_format: string, `"channels_last"` or `"channels_first"`.
                         dilation_rate: integer dilation rate.
                     # Returns
                         Output tensor.
                     # Raises
                         ValueError: if `data_format` is neither `"channels_last"` or
                         `"channels_first"`.
                     """
                     data_format = normalize_data_format(data_format)
                     if isinstance(strides, int):
                         strides = (strides,)
                     if isinstance(dilation_rate, int):
                         dilation_rate = (dilation_rate,)

                     if data_format == 'channels_last':
                         spatial_start_dim = 2
                     else:
                         spatial_start_dim = 3
                     x = expand_dims(x, spatial_start_dim)
                     depthwise_kernel = expand_dims(depthwise_kernel, 1)
                     pointwise_kernel = expand_dims(pointwise_kernel, 1)
                     strides = strides + (1,)
                     dilation_rate = dilation_rate + (1,)

                     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)
                     depthwise_kernel_shape = int_shape(depthwise_kernel)
                     if depthwise_kernel_shape is None:
                         # in case of a shared variable
                         depthwise_kernel_shape = depthwise_kernel.eval().shape
                     depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(
                         depthwise_kernel_shape, data_format)
                     pointwise_kernel_shape = int_shape(pointwise_kernel)
                     if pointwise_kernel_shape is None:
                         # in case of a shared variable
                         pointwise_kernel_shape = pointwise_kernel.eval().shape
                     pointwise_kernel_shape = _preprocess_conv2d_filter_shape(
                         pointwise_kernel_shape, data_format)

                     x = _preprocess_conv2d_input(x, data_format)
                     depthwise_kernel = _preprocess_conv2d_depthwise_kernel(
                         depthwise_kernel, depthwise_kernel_shape, data_format)
                     pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)
                     th_padding = _preprocess_padding(padding)

                     conv_out = T.nnet.conv2d(x, depthwise_kernel,
                                              border_mode=th_padding,
                                              subsample=strides,
                                              input_shape=image_shape,
                                              filter_shape=depthwise_kernel_shape,
                                              filter_dilation=dilation_rate,
                                              num_groups=image_shape[1])
                     conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,
                                              border_mode=th_padding,
                                              subsample=(1, 1),
                                              input_shape=None,
                                              filter_shape=pointwise_kernel_shape,
                                              filter_dilation=dilation_rate)
                     conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                                           pointwise_kernel_shape,
                                                           strides, data_format)
                     conv_out = squeeze(conv_out, spatial_start_dim)
                     return conv_out


                 def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),
                                      padding='valid', data_format=None, dilation_rate=(1, 1)):
                     """2D convolution with separable filters.
                     # Arguments
                         x: input tensor
                         depthwise_kernel: convolution kernel for the depthwise convolution.
                         pointwise_kernel: kernel for the 1x1 convolution.
                         strides: strides tuple (length 2).
                         padding: string, `"same"` or `"valid"`.
                         data_format: string, `"channels_last"` or `"channels_first"`.
                         dilation_rate: tuple of integers,
                             dilation rates for the separable convolution.
                     # Returns
                         Output tensor.
                     # Raises
                         ValueError: if `data_format` is neither `"channels_last"` or
                         `"channels_first"`.
                     """
                     data_format = normalize_data_format(data_format)

                     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)
                     depthwise_kernel_shape = int_shape(depthwise_kernel)
                     if depthwise_kernel_shape is None:
                         # in case of a shared variable
                         depthwise_kernel_shape = depthwise_kernel.eval().shape
                     depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(
                         depthwise_kernel_shape, data_format)
                     pointwise_kernel_shape = int_shape(pointwise_kernel)
                     if pointwise_kernel_shape is None:
                         # in case of a shared variable
                         pointwise_kernel_shape = pointwise_kernel.eval().shape
                     pointwise_kernel_shape = _preprocess_conv2d_filter_shape(
                         pointwise_kernel_shape, data_format)

                     x = _preprocess_conv2d_input(x, data_format)
                     depthwise_kernel = _preprocess_conv2d_depthwise_kernel(
                         depthwise_kernel, depthwise_kernel_shape, data_format)
                     pointwise_kernel = _preprocess_conv2d_kernel(pointwise_kernel, data_format)
                     th_padding = _preprocess_padding(padding)

                     conv_out = T.nnet.conv2d(x, depthwise_kernel,
                                              border_mode=th_padding,
                                              subsample=strides,
                                              input_shape=image_shape,
                                              filter_shape=depthwise_kernel_shape,
                                              filter_dilation=dilation_rate,
                                              num_groups=image_shape[1])
                     conv_out = T.nnet.conv2d(conv_out, pointwise_kernel,
                                              border_mode=th_padding,
                                              subsample=(1, 1),
                                              input_shape=None,
                                              filter_shape=pointwise_kernel_shape,
                                              filter_dilation=dilation_rate)
                     conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                                           pointwise_kernel_shape,
                                                           strides, data_format)
                     return conv_out


                 def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',
                                      data_format=None, dilation_rate=(1, 1)):
                     """2D convolution with separable filters.
                     # Arguments
                         x: input tensor
                         depthwise_kernel: convolution kernel for the depthwise convolution.
                         strides: strides tuple (length 2).
                         padding: string, `"same"` or `"valid"`.
                         data_format: string, `"channels_last"` or `"channels_first"`.
                         dilation_rate: tuple of integers,
                             dilation rates for the separable convolution.
                     # Returns
                         Output tensor.
                     # Raises
                         ValueError: if `data_format` is neither `"channels_last"` or
                         `"channels_first"`.
                     """
                     data_format = normalize_data_format(data_format)

                     image_shape = _preprocess_conv2d_image_shape(int_shape(x), data_format)
                     depthwise_kernel_shape = int_shape(depthwise_kernel)
                     if depthwise_kernel_shape is None:
                         # in case of a shared variable
                         depthwise_kernel_shape = depthwise_kernel.eval().shape
                     depthwise_kernel_shape = _preprocess_conv2d_depthwise_filter_shape(
                         depthwise_kernel_shape, data_format)

                     x = _preprocess_conv2d_input(x, data_format)
                     depthwise_kernel = _preprocess_conv2d_depthwise_kernel(
                         depthwise_kernel, depthwise_kernel_shape, data_format)
                     th_padding = _preprocess_padding(padding)

                     conv_out = T.nnet.conv2d(x, depthwise_kernel,
                                              border_mode=th_padding,
                                              subsample=strides,
                                              input_shape=image_shape,
                                              filter_shape=depthwise_kernel_shape,
                                              filter_dilation=dilation_rate,
                                              num_groups=image_shape[1])
                     conv_out = _postprocess_conv2d_output(
                         conv_out, x, padding, depthwise_kernel_shape, strides, data_format)
                     return conv_out


                 def conv3d(x, kernel, strides=(1, 1, 1),
                            padding='valid', data_format=None,
                            dilation_rate=(1, 1, 1)):
                     """3D convolution.
                     # Arguments
                         kernel: kernel tensor.
                         strides: strides tuple.
                         padding: string, "same" or "valid".
                         data_format: "channels_last" or "channels_first".
                             Whether to use Theano or TensorFlow data format
                         in inputs/kernels/outputs.
                     """
                     data_format = normalize_data_format(data_format)

                     volume_shape = _preprocess_conv3d_volume_shape(int_shape(x), data_format)
                     kernel_shape = int_shape(kernel)
                     if kernel_shape is None:
                         kernel_shape = kernel.eval().shape  # in case of a shared variable
                     kernel_shape = _preprocess_conv3d_filter_shape(kernel_shape, data_format)

                     x = _preprocess_conv3d_input(x, data_format)
                     kernel = _preprocess_conv3d_kernel(kernel, data_format)
                     th_padding = _preprocess_padding(padding)

                     conv_out = T.nnet.conv3d(x, kernel,
                                              border_mode=th_padding,
                                              subsample=strides,
                                              input_shape=volume_shape,
                                              filter_shape=kernel_shape,
                                              filter_dilation=dilation_rate)
                     conv_out = _postprocess_conv3d_output(conv_out, x, padding,
                                                           kernel_shape, strides, data_format)
                     return conv_out


                 def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),
                                      padding='valid', data_format=None):
                     """3D deconvolution (transposed convolution).
                     # Arguments
                         kernel: kernel tensor.
                         output_shape: desired dimensions of output.
                         strides: strides tuple.
                         padding: string, "same" or "valid".
                         data_format: "channels_last" or "channels_first".
                             Whether to use Theano or TensorFlow data format
                         in inputs/kernels/outputs.
                     # Raises
                         ValueError: if using an even kernel size with padding 'same'.
                     """
                     flip_filters = False
                     data_format = normalize_data_format(data_format)

                     if data_format == 'channels_last':
                         output_shape = (output_shape[0],
                                         output_shape[4],
                                         output_shape[1],
                                         output_shape[2],
                                         output_shape[3])

                     kernel_shape = int_shape(kernel)
                     if kernel_shape is None:
                         kernel_shape = kernel.eval().shape  # in case of a shared variable

                     if padding == 'same' and kernel_shape[0] % 2 == 0:
                         raise ValueError('In `Conv3DTranspose`, with padding mode `same`, '
                                          'even kernel sizes are not supported with Theano. '
                                          'You can set `kernel_size` to an odd number.')

                     kernel_shape = _preprocess_conv3d_filter_shape(kernel_shape, data_format)

                     x = _preprocess_conv3d_input(x, data_format)
                     kernel = _preprocess_conv3d_kernel(kernel, data_format)

                     th_padding = _preprocess_padding(padding)
                     op = T.nnet.abstract_conv.AbstractConv3d_gradInputs(imshp=None,
                                                                         kshp=kernel_shape,
                                                                         subsample=strides,
                                                                         border_mode=th_padding,
                                                                         filter_flip=not flip_filters)
                     conv_out = op(kernel, x, output_shape[2:])
                     conv_out = _postprocess_conv3d_output(conv_out, x, padding,
                                                           kernel_shape, strides, data_format)
                     return conv_out


                 def pool2d(x, pool_size, strides=(1, 1), padding='valid',
                            data_format=None, pool_mode='max'):
                     data_format = normalize_data_format(data_format)

                     assert pool_size[0] >= 1 and pool_size[1] >= 1

                     if padding == 'same':
                         odd_pad_w = pool_size[0] > 2 and pool_size[0] % 2 == 1
                         w_pad = pool_size[0] - 2 if odd_pad_w else pool_size[0] - 1
                         odd_pad_h = pool_size[1] > 2 and pool_size[1] % 2 == 1
                         h_pad = pool_size[1] - 2 if odd_pad_h else pool_size[1] - 1
                         pad = (w_pad, h_pad)
                     elif padding == 'valid':
                         pad = (0, 0)
                     else:
                         raise ValueError('Invalid border mode:', padding)

                     if data_format == 'channels_last':
                         x = x.dimshuffle((0, 3, 1, 2))

                     if pool_mode == 'max':
                         pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,
                                                 ignore_border=True,
                                                 pad=pad,
                                                 mode='max')
                     elif pool_mode == 'avg':
                         pool_out = pool.pool_2d(x, ws=pool_size, stride=strides,
                                                 ignore_border=True,
                                                 pad=pad,
                                                 mode='average_exc_pad')
                     else:
                         raise ValueError('Invalid pooling mode:', pool_mode)
                     if padding == 'same':
                         expected_width = (x.shape[2] + strides[0] - 1) // strides[0]
                         expected_height = (x.shape[3] + strides[1] - 1) // strides[1]
                         pool_out = pool_out[:, :,
                                             : expected_width,
                                             : expected_height]

                     if data_format == 'channels_last':
                         pool_out = pool_out.dimshuffle((0, 2, 3, 1))
                     return pool_out


                 def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',
                            data_format=None, pool_mode='max'):
                     data_format = normalize_data_format(data_format)

                     if padding == 'same':
                         w_pad = pool_size[0] - 2 if pool_size[0] % 2 == 1 else pool_size[0] - 1
                         h_pad = pool_size[1] - 2 if pool_size[1] % 2 == 1 else pool_size[1] - 1
                         d_pad = pool_size[2] - 2 if pool_size[2] % 2 == 1 else pool_size[2] - 1
                         pad = (w_pad, h_pad, d_pad)
                     elif padding == 'valid':
                         pad = (0, 0, 0)
                     else:
                         raise ValueError('Invalid padding:', padding)

                     if data_format == 'channels_last':
                         x = x.dimshuffle((0, 4, 1, 2, 3))

                     if pool_mode == 'max':
                         pool_out = pool.pool_3d(x, ws=pool_size, stride=strides,
                                                 ignore_border=True,
                                                 pad=pad,
                                                 mode='max')
                     elif pool_mode == 'avg':
                         pool_out = pool.pool_3d(x, ws=pool_size, stride=strides,
                                                 ignore_border=True,
                                                 pad=pad,
                                                 mode='average_exc_pad')
                     else:
                         raise ValueError('Invalid pooling mode:', pool_mode)

                     if padding == 'same':
                         expected_width = (x.shape[2] + strides[0] - 1) // strides[0]
                         expected_height = (x.shape[3] + strides[1] - 1) // strides[1]
                         expected_depth = (x.shape[4] + strides[2] - 1) // strides[2]

                         pool_out = pool_out[:, :,
                                             : expected_width,
                                             : expected_height,
                                             : expected_depth]

                     if data_format == 'channels_last':
                         pool_out = pool_out.dimshuffle((0, 2, 3, 4, 1))
                     return pool_out


                 def bias_add(x, bias, data_format=None):
                     data_format = normalize_data_format(data_format)
                     if ndim(bias) != 1 and ndim(bias) != ndim(x) - 1:
                         raise ValueError('Unexpected bias dimensions %d, '
                                          'expect to be 1 or %d dimensions'
                                          % (ndim(bias), ndim(x) - 1))
                     bias_shape = tuple(bias.shape)
                     if ndim(x) == 5:
                         if data_format == 'channels_first':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, bias_shape[0], 1, 1, 1))
                             else:
                                 x += reshape(bias, (1, bias_shape[3]) + bias_shape[:3])
                         elif data_format == 'channels_last':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, 1, 1, 1, bias_shape[0]))
                             else:
                                 x += reshape(bias, (1,) + bias_shape)
                     elif ndim(x) == 4:
                         if data_format == 'channels_first':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, bias_shape[0], 1, 1))
                             else:
                                 x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])
                         elif data_format == 'channels_last':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, 1, 1, bias_shape[0]))
                             else:
                                 x += reshape(bias, (1,) + bias_shape)
                     elif ndim(x) == 3:
                         if data_format == 'channels_first':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, bias_shape[0], 1))
                             else:
                                 x += reshape(bias, (1, bias_shape[1], bias_shape[0]))
                         elif data_format == 'channels_last':
                             if ndim(bias) == 1:
                                 x += reshape(bias, (1, 1, bias_shape[0]))
                             else:
                                 x += reshape(bias, (1,) + bias_shape)
                     else:
                         x += bias
                     return x


                 # RANDOMNESS


                 def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):
                     if dtype is None:
                         dtype = floatx()
                     if seed is None:
                         seed = np.random.randint(1, 10e6)
                     rng = RandomStreams(seed=seed)
                     return rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)


                 def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):
                     if dtype is None:
                         dtype = floatx()
                     if seed is None:
                         seed = np.random.randint(1, 10e6)
                     rng = RandomStreams(seed=seed)
                     return rng.uniform(shape, low=minval, high=maxval, dtype=dtype)


                 def random_binomial(shape, p=0.0, dtype=None, seed=None):
                     if dtype is None:
                         dtype = floatx()
                     if seed is None:
                         seed = np.random.randint(1, 10e6)
                     rng = RandomStreams(seed=seed)
                     return rng.binomial(shape, p=p, dtype=dtype)


                 def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):
                     if dtype is None:
                         dtype = floatx()
                     if seed is None:
                         seed = np.random.randint(1, 10e6)
                     rng = RandomStreams(seed=seed)

                     try:
                         return rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype,
                                           truncate=True)
                     except TypeError:
                         normal_t = rng.normal(size=shape, avg=mean, std=stddev, dtype=dtype)
                         # Poor man's truncated normal: we literally clip the tensor
                         return T.clip(normal_t, mean - 2 * stddev, mean + 2 * stddev)


                 # Theano implementation of CTC
                 # Used with permission from Shawn Tan
                 # https://github.com/shawntan/
                 # Note that TensorFlow's native CTC code is significantly
                 # faster than this


                 def ctc_interleave_blanks(Y):
                     Y_ = T.alloc(-1, Y.shape[0] * 2 + 1)
                     Y_ = T.set_subtensor(Y_[T.arange(Y.shape[0]) * 2 + 1], Y)
                     return Y_


                 def ctc_create_skip_idxs(Y):
                     skip_idxs = T.arange((Y.shape[0] - 3) // 2) * 2 + 1
                     non_repeats = T.neq(Y[skip_idxs], Y[skip_idxs + 2])
                     return skip_idxs[non_repeats.nonzero()]


                 def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):
                     active_skip_idxs = skip_idxs[(skip_idxs < active).nonzero()]
                     active_next = T.cast(T.minimum(
                         T.maximum(
                             active + 1,
                             T.max(T.concatenate([active_skip_idxs, [-1]])) + 2 + 1
                         ), log_p_curr.shape[0]), 'int32')

                     common_factor = T.max(log_p_prev[:active])
                     p_prev = T.exp(log_p_prev[:active] - common_factor)
                     _p_prev = zeros[:active_next]
                     # copy over
                     _p_prev = T.set_subtensor(_p_prev[:active], p_prev)
                     # previous transitions
                     _p_prev = T.inc_subtensor(_p_prev[1:], _p_prev[:-1])
                     # skip transitions
                     _p_prev = T.inc_subtensor(
                         _p_prev[active_skip_idxs + 2], p_prev[active_skip_idxs])
                     updated_log_p_prev = T.log(_p_prev) + common_factor

                     log_p_next = T.set_subtensor(
                         zeros[:active_next],
                         log_p_curr[:active_next] + updated_log_p_prev
                     )
                     return active_next, log_p_next


                 def ctc_path_probs(predict, Y, alpha=1e-4):
                     smoothed = (1 - alpha) * predict[:, Y] + alpha * np.float32(1.) / Y.shape[0]
                     L = T.log(smoothed)
                     zeros = T.zeros_like(L[0])
                     log_first = zeros

                     f_skip_idxs = ctc_create_skip_idxs(Y)
                     # there should be a shortcut to calculating this
                     b_skip_idxs = ctc_create_skip_idxs(Y[::-1])

                     def step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev):
                         f_active_next, log_f_next = ctc_update_log_p(
                             f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)
                         b_active_next, log_b_next = ctc_update_log_p(
                             b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)
                         return f_active_next, log_f_next, b_active_next, log_b_next

                     [f_active, log_f_probs, b_active, log_b_probs], _ = theano.scan(
                         step,
                         sequences=[L, L[::-1, ::-1]],
                         outputs_info=[np.int32(1), log_first, np.int32(1), log_first])

                     idxs = T.arange(L.shape[1]).dimshuffle('x', 0)
                     mask = ((idxs < f_active.dimshuffle(0, 'x')) &
                             (idxs < b_active.dimshuffle(0, 'x'))[::-1, ::-1])
                     log_probs = log_f_probs + log_b_probs[::-1, ::-1] - L
                     return log_probs, mask


                 def ctc_cost(predict, Y):
                     log_probs, mask = ctc_path_probs(predict, ctc_interleave_blanks(Y))
                     common_factor = T.max(log_probs)
                     total_log_prob = T.log(T.sum(T.exp(log_probs - common_factor)[mask.nonzero()]))
                     total_log_prob = total_log_prob + common_factor
                     return -total_log_prob


                 # batchifies original CTC code
                 def ctc_batch_cost(y_true, y_pred, input_length, label_length):
                     """Runs CTC loss algorithm on each batch element.
                     # Arguments
                         y_true: tensor (samples, max_string_length) containing the truth labels
                         y_pred: tensor (samples, time_steps, num_categories) containing the
                                 prediction, or output of the softmax
                         input_length: tensor (samples,1) containing the sequence length for
                                 each batch item in y_pred
                         label_length: tensor (samples,1) containing the sequence length for
                                 each batch item in y_true
                     # Returns
                         Tensor with shape (samples,1) containing the
                             CTC loss of each element
                     """

                     def ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step):
                         y_pred_step = y_pred_step[0: input_length_step[0]]
                         y_true_step = y_true_step[0:label_length_step[0]]
                         return ctc_cost(y_pred_step, y_true_step)

                     ret, _ = theano.scan(
                         fn=ctc_step,
                         outputs_info=None,
                         sequences=[y_true, y_pred, input_length, label_length]
                     )

                     ret = ret.dimshuffle('x', 0)
                     return ret


                 # HIGH ORDER FUNCTIONS

                 def map_fn(fn, elems, name=None, dtype=None):
                     """Map the function fn over the elements elems and return the outputs.
                     # Arguments
                         fn: Callable that will be called upon each element in elems
                         elems: tensor, at least 2 dimensional
                         name: A string name for the map node in the graph
                     # Returns
                         Tensor with first dimension equal to the elems and second depending on
                         fn
                     """
                     return theano.map(fn, elems, name=name)[0]


                 def foldl(fn, elems, initializer=None, name=None):
                     """Reduce elems using fn to combine them from left to right.
                     # Arguments
                         fn: Callable that will be called upon each element in elems and an
                             accumulator, for instance lambda acc, x: acc + x
                         elems: tensor
                         initializer: The first value used (elems[0] in case of None)
                         name: A string name for the foldl node in the graph
                     # Returns
                         Same type and shape as initializer
                     """
                     if initializer is None:
                         initializer = elems[0]
                         elems = elems[1:]

                     # We need to change the order of the arguments because theano accepts x as
                     # first parameter and accumulator as second
                     return theano.foldl(lambda x, acc: fn(acc, x),
                                         elems, initializer, name=name)[0]


                 def foldr(fn, elems, initializer=None, name=None):
                     """Reduce elems using fn to combine them from right to left.
                     # Arguments
                         fn: Callable that will be called upon each element in elems and an
                             accumulator, for instance lambda acc, x: acc + x
                         elems: tensor
                         initializer: The first value used (elems[-1] in case of None)
                         name: A string name for the foldr node in the graph
                     # Returns
                         Same type and shape as initializer
                     """
                     if initializer is None:
                         initializer = elems[-1]
                         elems = elems[:-1]

                     # We need to change the order of the arguments because theano accepts x as
                     # first parameter and accumulator as second
                     return theano.foldr(lambda x, acc: fn(acc, x),
                                         elems, initializer, name=name)[0]


                 def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):
                     data_format = normalize_data_format(data_format)

                     stride = strides[0]
                     kernel_shape = int_shape(kernel)
                     output_length, feature_dim, filters = kernel_shape

                     xs = []
                     for i in range(output_length):
                         slice_length = py_slice(i * stride,
                                                 i * stride + kernel_size[0])
                         xs.append(reshape(inputs[:, slice_length, :],
                                           (1, -1, feature_dim)))
                     x_aggregate = concatenate(xs, axis=0)
                     # Shape: `(output_length, batch_size, filters)`.
                     output = batch_dot(x_aggregate, kernel)
                     return permute_dimensions(output, (1, 0, 2))


                 def local_conv2d(inputs,
                                  kernel,
                                  kernel_size,
                                  strides,
                                  output_shape,
                                  data_format=None):
                     data_format = normalize_data_format(data_format)

                     stride_row, stride_col = strides
                     output_row, output_col = output_shape
                     kernel_shape = int_shape(kernel)
                     _, feature_dim, filters = kernel_shape

                     if data_format == 'channels_first':
                         output = []
                         for i in range(output_row):
                             for j in range(output_col):
                                 slice_row = py_slice(i * stride_row,
                                                      i * stride_row + kernel_size[0])
                                 slice_col = py_slice(j * stride_col,
                                                      j * stride_col + kernel_size[1])
                                 x_flatten = reshape(inputs[:, :, slice_row, slice_col],
                                                     (1, -1, feature_dim))
                                 output.append(dot(x_flatten,
                                                   kernel[i * output_col + j, :, :]))
                         output = concatenate(output, axis=0)
                         output = reshape(output,
                                          (output_row, output_col, -1, filters))
                         output = permute_dimensions(output, (2, 3, 0, 1))
                     else:
                         xs = []
                         for i in range(output_row):
                             for j in range(output_col):
                                 slice_row = py_slice(i * stride_row,
                                                      i * stride_row + kernel_size[0])
                                 slice_col = py_slice(j * stride_col,
                                                      j * stride_col + kernel_size[1])
                                 xs.append(reshape(inputs[:, slice_row, slice_col, :],
                                                   (1, -1, feature_dim)))

                         x_aggregate = concatenate(xs, axis=0)
                         output = batch_dot(x_aggregate, kernel)
                         output = reshape(output,
                                          (output_row, output_col, -1, filters))
                         output = permute_dimensions(output, (2, 0, 1, 3))
                     return output


                 def ctc_label_dense_to_sparse(labels, label_lengths):
                     raise NotImplementedError


                 def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1,
                                merge_repeated=False):
                     raise NotImplementedError
                     from __future__ import absolute_import
                     from __future__ import division
                     from __future__ import print_function

                     import tensorflow as tf
                     from tensorflow.python.framework import ops as tf_ops
                     from tensorflow.python.training import moving_averages
                     from tensorflow.python.ops import tensor_array_ops
                     from tensorflow.python.ops import control_flow_ops
                     from tensorflow.python.ops import functional_ops
                     from tensorflow.python.ops import ctc_ops as ctc
                     from tensorflow.python.client import device_lib
                     from tensorflow.core.protobuf import config_pb2

                     from collections import defaultdict

                     import numpy as np
                     from distutils.version import StrictVersion
                     import os

                     from .common import floatx
                     from .common import epsilon
                     from .common import normalize_data_format
                     from ..utils.generic_utils import transpose_shape
                     from ..utils.generic_utils import has_arg

                     # Legacy functions
                     from .common import set_image_dim_ordering
                     from .common import image_dim_ordering

                     py_all = all
                     py_any = any
                     py_sum = sum
                     py_slice = slice

                     # INTERNAL UTILS

                     # This is the default internal TF session used by Keras.
                     # It can be set manually via `set_session(sess)`.
                     _SESSION = None

                     # This dictionary holds a mapping {graph: learning_phase}.
                     # A learning phase is a bool tensor used to run Keras models in
                     # either train mode (learning_phase == 1) or test mode (learning_phase == 0).
                     _GRAPH_LEARNING_PHASES = {}

                     # This dictionary holds a mapping {graph: UID_DICT}.
                     # each UID_DICT is a dictionary mapping name prefixes to a current index,
                     # used for generating graph-specific string UIDs
                     # for various names (e.g. layer names).
                     _GRAPH_UID_DICTS = {}

                     # This boolean flag can be set to True to leave variable initialization
                     # up to the user.
                     # Change its value via `manual_variable_initialization(value)`.
                     _MANUAL_VAR_INIT = False

                     # This list holds the available devices.
                     # It is populated when `_get_available_gpus()` is called for the first time.
                     # We assume our devices don't change during our lifetime.
                     _LOCAL_DEVICES = None


                     def get_uid(prefix=''):
                         """Get the uid for the default graph.

                         # Arguments
                             prefix: An optional prefix of the graph.

                         # Returns
                             A unique identifier for the graph.
                         """
                         global _GRAPH_UID_DICTS
                         graph = tf.get_default_graph()
                         if graph not in _GRAPH_UID_DICTS:
                             _GRAPH_UID_DICTS[graph] = defaultdict(int)
                         _GRAPH_UID_DICTS[graph][prefix] += 1
                         return _GRAPH_UID_DICTS[graph][prefix]


                     def reset_uids():
                         """Resets graph identifiers.
                         """
                         global _GRAPH_UID_DICTS
                         _GRAPH_UID_DICTS = {}


                     def clear_session():
                         """Destroys the current TF graph and creates a new one.

                         Useful to avoid clutter from old models / layers.
                         """
                         global _SESSION
                         global _GRAPH_LEARNING_PHASES
                         tf.reset_default_graph()
                         reset_uids()
                         _SESSION = None
                         with tf.name_scope(''):
                             phase = tf.placeholder_with_default(
                                 False,
                                 shape=(),
                                 name='keras_learning_phase')
                         _GRAPH_LEARNING_PHASES = {}
                         _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase


                     def manual_variable_initialization(value):
                         """Sets the manual variable initialization flag.

                         This boolean flag determines whether
                         variables should be initialized
                         as they are instantiated (default), or if
                         the user should handle the initialization
                         (e.g. via `tf.initialize_all_variables()`).

                         # Arguments
                             value: Python boolean.
                         """
                         global _MANUAL_VAR_INIT
                         _MANUAL_VAR_INIT = value


                     def learning_phase():
                         """Returns the learning phase flag.

                         The learning phase flag is a bool tensor (0 = test, 1 = train)
                         to be passed as input to any Keras function
                         that uses a different behavior at train time and test time.

                         # Returns
                             Learning phase (scalar integer tensor or Python integer).
                         """
                         graph = tf.get_default_graph()
                         if graph not in _GRAPH_LEARNING_PHASES:
                             with tf.name_scope(''):
                                 phase = tf.placeholder_with_default(
                                     False,
                                     shape=(),
                                     name='keras_learning_phase')
                             _GRAPH_LEARNING_PHASES[graph] = phase
                         return _GRAPH_LEARNING_PHASES[graph]


                     def set_learning_phase(value):
                         """Sets the learning phase to a fixed value.

                         # Arguments
                             value: Learning phase value, either 0 or 1 (integers).

                         # Raises
                             ValueError: if `value` is neither `0` nor `1`.
                         """
                         global _GRAPH_LEARNING_PHASES
                         if value not in {0, 1}:
                             raise ValueError('Expected learning phase to be '
                                              '0 or 1.')
                         _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = value


                     def get_session():
                         """Returns the TF session to be used by the backend.

                         If a default TensorFlow session is available, we will return it.

                         Else, we will return the global Keras session.

                         If no global Keras session exists at this point:
                         we will create a new global session.

                         Note that you can manually set the global session
                         via `K.set_session(sess)`.

                         # Returns
                             A TensorFlow session.
                         """
                         global _SESSION

                         default_session = tf.get_default_session()

                         if default_session is not None:
                             session = default_session
                         else:
                             if _SESSION is None:
                                 if not os.environ.get('OMP_NUM_THREADS'):
                                     config = tf.ConfigProto(allow_soft_placement=True)
                                 else:
                                     num_thread = int(os.environ.get('OMP_NUM_THREADS'))
                                     config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,
                                                             inter_op_parallelism_threads=num_thread,
                                                             allow_soft_placement=True)
                                 _SESSION = tf.Session(config=config)
                             session = _SESSION
                         if not _MANUAL_VAR_INIT:
                             with session.graph.as_default():
                                 variables = tf.global_variables()
                                 candidate_vars = []
                                 for v in variables:
                                     if not getattr(v, '_keras_initialized', False):
                                         candidate_vars.append(v)
                                 if candidate_vars:
                                     # This step is expensive, so we only run it on variables
                                     # not already marked as initialized.
                                     is_initialized = session.run(
                                         [tf.is_variable_initialized(v) for v in candidate_vars])
                                     uninitialized_vars = []
                                     for flag, v in zip(is_initialized, candidate_vars):
                                         if not flag:
                                             uninitialized_vars.append(v)
                                         v._keras_initialized = True
                                     if uninitialized_vars:
                                         session.run(tf.variables_initializer(uninitialized_vars))
                         # hack for list_devices() function.
                         # list_devices() function is not available under tensorflow r1.3.
                         if not hasattr(session, 'list_devices'):
                             session.list_devices = lambda: device_lib.list_local_devices()
                         return session


                     def set_session(session):
                         """Sets the global TensorFlow session.

                         # Arguments
                             session: A TF Session.
                         """
                         global _SESSION
                         _SESSION = session


                     # DEVICE MANIPULATION AND PROBING

                     class _TfDeviceCaptureOp(object):
                         """Class for capturing the TF device scope."""

                         def __init__(self):
                             self.device = None

                         def _set_device(self, device):
                             """This method captures TF's explicit device scope setting."""
                             self.device = device


                     def _get_current_tf_device():
                         """Return explicit device of current context, otherwise returns `None`.

                         # Returns
                             If the current device scope is explicitly set, it returns a string with
                             the device (`CPU` or `GPU`). If the scope is not explicitly set, it will
                             return `None`.
                         """
                         g = tf.get_default_graph()
                         op = _TfDeviceCaptureOp()
                         g._apply_device_functions(op)
                         return op.device


                     def _is_current_explicit_device(device_type):
                         """Check if the current device is explicitly set on the device type specified.

                         # Arguments
                             device_type: A string containing `GPU` or `CPU` (case-insensitive).

                         # Returns
                             A boolean indicating if the current device
                             scope is explicitly set on the device type.

                         # Raises
                             ValueError: If the `device_type` string indicates an unsupported device.
                         """
                         device_type = device_type.upper()
                         if device_type not in ['CPU', 'GPU']:
                             raise ValueError('`device_type` should be either "CPU" or "GPU".')
                         device = _get_current_tf_device()
                         return (device is not None and device.device_type == device_type.upper())


                     def _get_available_gpus():
                         """Get a list of available gpu devices (formatted as strings).

                         # Returns
                             A list of available GPU devices.
                         """
                         global _LOCAL_DEVICES
                         if _LOCAL_DEVICES is None:
                             _LOCAL_DEVICES = get_session().list_devices()
                         return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']


                     def _has_nchw_support():
                         """Check whether the current scope supports NCHW ops.

                         TensorFlow does not support NCHW on CPU.
                         Therefore we check if we are not explicitly put on
                         CPU, and have GPUs available.
                         In this case there will be soft-placing on the GPU device.

                         # Returns
                             bool: if the current scope device placement would support nchw
                         """
                         explicitly_on_cpu = _is_current_explicit_device('CPU')
                         gpus_available = len(_get_available_gpus()) > 0
                         return (not explicitly_on_cpu and gpus_available)


                     # VARIABLE MANIPULATION

                     def _to_tensor(x, dtype):
                         """Convert the input `x` to a tensor of type `dtype`.

                         # Arguments
                             x: An object to be converted (numpy array, list, tensors).
                             dtype: The destination type.

                         # Returns
                             A tensor.
                         """
                         return tf.convert_to_tensor(x, dtype=dtype)


                     def is_sparse(tensor):
                         """Returns whether a tensor is a sparse tensor.

                         # Arguments
                             tensor: A tensor instance.

                         # Returns
                             A boolean.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> a = K.placeholder((2, 2), sparse=False)
                             >>> print(K.is_sparse(a))
                             False
                             >>> b = K.placeholder((2, 2), sparse=True)
                             >>> print(K.is_sparse(b))
                             True
                         ```
                         """
                         return isinstance(tensor, tf.SparseTensor)


                     def to_dense(tensor):
                         """Converts a sparse tensor into a dense tensor and returns it.

                         # Arguments
                             tensor: A tensor instance (potentially sparse).

                         # Returns
                             A dense tensor.

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> b = K.placeholder((2, 2), sparse=True)
                             >>> print(K.is_sparse(b))
                             True
                             >>> c = K.to_dense(b)
                             >>> print(K.is_sparse(c))
                             False
                         ```
                         """
                         if is_sparse(tensor):
                             return tf.sparse_tensor_to_dense(tensor)
                         else:
                             return tensor


                     name_scope = tf.name_scope


                     def variable(value, dtype=None, name=None, constraint=None):
                         """Instantiates a variable and returns it.

                         # Arguments
                             value: Numpy array, initial value of the tensor.
                             dtype: Tensor type.
                             name: Optional name string for the tensor.
                             constraint: Optional projection function to be
                                 applied to the variable after an optimizer update.

                         # Returns
                             A variable instance (with Keras metadata included).

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> val = np.array([[1, 2], [3, 4]])
                             >>> kvar = K.variable(value=val, dtype='float64', name='example_var')
                             >>> K.dtype(kvar)
                             'float64'
                             >>> print(kvar)
                             example_var
                             >>> K.eval(kvar)
                             array([[ 1.,  2.],
                                    [ 3.,  4.]])
                         ```
                         """
                         if dtype is None:
                             dtype = floatx()
                         if hasattr(value, 'tocoo'):
                             sparse_coo = value.tocoo()
                             indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),
                                                       np.expand_dims(sparse_coo.col, 1)), 1)
                             v = tf.SparseTensor(indices=indices,
                                                 values=sparse_coo.data,
                                                 dense_shape=sparse_coo.shape)
                             v._keras_shape = sparse_coo.shape
                             v._uses_learning_phase = False
                             return v
                         v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)
                         if isinstance(value, np.ndarray):
                             v._keras_shape = value.shape
                         elif hasattr(value, 'get_shape'):
                             v._keras_shape = int_shape(value)
                         v._uses_learning_phase = False
                         # TODO: move to Variable constructor when supported in public release.
                         try:
                             v.constraint = constraint
                         except AttributeError:
                             v._constraint = constraint
                         return v


                     def constant(value, dtype=None, shape=None, name=None):
                         """Creates a constant tensor.

                         # Arguments
                             value: A constant value (or list)
                             dtype: The type of the elements of the resulting tensor.
                             shape: Optional dimensions of resulting tensor.
                             name: Optional name for the tensor.

                         # Returns
                             A Constant Tensor.
                         """
                         if dtype is None:
                             dtype = floatx()
                         return tf.constant(value, dtype=dtype, shape=shape, name=name)


                     def is_keras_tensor(x):
                         """Returns whether `x` is a Keras tensor.

                         A "Keras tensor" is a tensor that was returned by a Keras layer,
                         (`Layer` class) or by `Input`.

                         # Arguments
                             x: A candidate tensor.

                         # Returns
                             A boolean: Whether the argument is a Keras tensor.

                         # Raises
                             ValueError: In case `x` is not a symbolic tensor.

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> from keras.layers import Input, Dense
                             >>> np_var = numpy.array([1, 2])
                             >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.
                             ValueError
                             >>> k_var = tf.placeholder('float32', shape=(1,1))
                             >>> # A variable indirectly created outside of keras is not a Keras tensor.
                             >>> K.is_keras_tensor(k_var)
                             False
                             >>> keras_var = K.variable(np_var)
                             >>> # A variable created with the keras backend is not a Keras tensor.
                             >>> K.is_keras_tensor(keras_var)
                             False
                             >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))
                             >>> # A placeholder is not a Keras tensor.
                             >>> K.is_keras_tensor(keras_placeholder)
                             False
                             >>> keras_input = Input([10])
                             >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.
                             True
                             >>> keras_layer_output = Dense(10)(keras_input)
                             >>> # Any Keras layer output is a Keras tensor.
                             >>> K.is_keras_tensor(keras_layer_output)
                             True
                         ```
                         """
                         if not is_tensor(x):
                             raise ValueError('Unexpectedly found an instance of type `' +
                                              str(type(x)) + '`. '
                                              'Expected a symbolic tensor instance.')
                         return hasattr(x, '_keras_history')


                     def is_tensor(x):
                         return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)


                     def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):
                         """Instantiates a placeholder tensor and returns it.

                         # Arguments
                             shape: Shape of the placeholder
                                 (integer tuple, may include `None` entries).
                             ndim: Number of axes of the tensor.
                                 At least one of {`shape`, `ndim`} must be specified.
                                 If both are specified, `shape` is used.
                             dtype: Placeholder type.
                             sparse: Boolean, whether the placeholder should have a sparse type.
                             name: Optional name string for the placeholder.

                         # Returns
                             Tensor instance (with Keras metadata included).

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> input_ph = K.placeholder(shape=(2, 4, 5))
                             >>> input_ph._keras_shape
                             (2, 4, 5)
                             >>> input_ph
                             <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>
                         ```
                         """
                         if dtype is None:
                             dtype = floatx()
                         if not shape:
                             if ndim:
                                 shape = tuple([None for _ in range(ndim)])
                         if sparse:
                             x = tf.sparse_placeholder(dtype, shape=shape, name=name)
                         else:
                             x = tf.placeholder(dtype, shape=shape, name=name)
                         x._keras_shape = shape
                         x._uses_learning_phase = False
                         return x


                     def is_placeholder(x):
                         """Returns whether `x` is a placeholder.

                         # Arguments
                             x: A candidate placeholder.

                         # Returns
                             Boolean.
                         """
                         try:
                             return x.op.type == 'Placeholder'
                         except AttributeError:
                             return False


                     def shape(x):
                         """Returns the symbolic shape of a tensor or variable.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A symbolic shape (which is itself a tensor).

                         # Examples
                         ```python
                             # TensorFlow example
                             >>> from keras import backend as K
                             >>> tf_session = K.get_session()
                             >>> val = np.array([[1, 2], [3, 4]])
                             >>> kvar = K.variable(value=val)
                             >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))
                             >>> K.shape(kvar)
                             <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>
                             >>> K.shape(inputs)
                             <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>
                             # To get integer shape (Instead, you can use K.int_shape(x))
                             >>> K.shape(kvar).eval(session=tf_session)
                             array([2, 2], dtype=int32)
                             >>> K.shape(inputs).eval(session=tf_session)
                             array([2, 4, 5], dtype=int32)
                         ```
                         """
                         return tf.shape(x)


                     def int_shape(x):
                         """Returns the shape of tensor or variable as a tuple of int or None entries.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tuple of integers (or None entries).

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> inputs = K.placeholder(shape=(2, 4, 5))
                             >>> K.int_shape(inputs)
                             (2, 4, 5)
                             >>> val = np.array([[1, 2], [3, 4]])
                             >>> kvar = K.variable(value=val)
                             >>> K.int_shape(kvar)
                             (2, 2)
                         ```

                         {{np_implementation}}
                         """
                         if hasattr(x, '_keras_shape'):
                             return x._keras_shape
                         try:
                             return tuple(x.get_shape().as_list())
                         except ValueError:
                             return None


                     def ndim(x):
                         """Returns the number of axes in a tensor, as an integer.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             Integer (scalar), number of axes.

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> inputs = K.placeholder(shape=(2, 4, 5))
                             >>> val = np.array([[1, 2], [3, 4]])
                             >>> kvar = K.variable(value=val)
                             >>> K.ndim(inputs)
                             3
                             >>> K.ndim(kvar)
                             2
                         ```

                         {{np_implementation}}
                         """
                         dims = x.get_shape()._dims
                         if dims is not None:
                             return len(dims)
                         return None


                     def dtype(x):
                         """Returns the dtype of a Keras tensor or variable, as a string.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             String, dtype of `x`.

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> K.dtype(K.placeholder(shape=(2,4,5)))
                             'float32'
                             >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
                             'float32'
                             >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
                             'float64'
                             # Keras variable
                             >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))
                             >>> K.dtype(kvar)
                             'float32_ref'
                             >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
                             >>> K.dtype(kvar)
                             'float32_ref'
                         ```
                         {{np_implementation}}
                         """
                         return x.dtype.base_dtype.name


                     def eval(x):
                         """Evaluates the value of a variable.

                         # Arguments
                             x: A variable.

                         # Returns
                             A Numpy array.

                         # Examples
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
                             >>> K.eval(kvar)
                             array([[ 1.,  2.],
                                    [ 3.,  4.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         return to_dense(x).eval(session=get_session())


                     def zeros(shape, dtype=None, name=None):
                         """Instantiates an all-zeros variable and returns it.

                         # Arguments
                             shape: Tuple of integers, shape of returned Keras variable
                             dtype: String, data type of returned Keras variable
                             name: String, name of returned Keras variable

                         # Returns
                             A variable (including Keras metadata), filled with `0.0`.
                             Note that if `shape` was symbolic, we cannot return a variable,
                             and will return a dynamically-shaped tensor instead.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.zeros((3,4))
                             >>> K.eval(kvar)
                             array([[ 0.,  0.,  0.,  0.],
                                    [ 0.,  0.,  0.,  0.],
                                    [ 0.,  0.,  0.,  0.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         tf_dtype = tf.as_dtype(dtype)
                         v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)
                         if py_all(v.get_shape().as_list()):
                             return variable(v, dtype=dtype, name=name)
                         return v


                     def ones(shape, dtype=None, name=None):
                         """Instantiates an all-ones variable and returns it.

                         # Arguments
                             shape: Tuple of integers, shape of returned Keras variable.
                             dtype: String, data type of returned Keras variable.
                             name: String, name of returned Keras variable.

                         # Returns
                             A Keras variable, filled with `1.0`.
                             Note that if `shape` was symbolic, we cannot return a variable,
                             and will return a dynamically-shaped tensor instead.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.ones((3,4))
                             >>> K.eval(kvar)
                             array([[ 1.,  1.,  1.,  1.],
                                    [ 1.,  1.,  1.,  1.],
                                    [ 1.,  1.,  1.,  1.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         tf_dtype = tf.as_dtype(dtype)
                         v = tf.ones(shape=shape, dtype=tf_dtype, name=name)
                         if py_all(v.get_shape().as_list()):
                             return variable(v, dtype=dtype, name=name)
                         return v


                     def eye(size, dtype=None, name=None):
                         """Instantiate an identity matrix and returns it.

                         # Arguments
                             size: Tuple, number of rows and columns. If Integer, number of rows.
                             dtype: String, data type of returned Keras variable.
                             name: String, name of returned Keras variable.

                         # Returns
                             A Keras variable, an identity matrix.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> K.eval(K.eye(3))
                             array([[ 1.,  0.,  0.],
                                    [ 0.,  1.,  0.],
                                    [ 0.,  0.,  1.]], dtype=float32)
                             >>> K.eval(K.eye((2, 3)))
                             array([[1., 0., 0.],
                                    [0., 1., 0.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         tf_dtype = tf.as_dtype(dtype)
                         if isinstance(size, (list, tuple)):
                             n, m = size
                         else:
                             n, m = size, size
                         return variable(tf.eye(n, m, dtype=tf_dtype), dtype, name)


                     def zeros_like(x, dtype=None, name=None):
                         """Instantiates an all-zeros variable of the same shape as another tensor.

                         # Arguments
                             x: Keras variable or Keras tensor.
                             dtype: String, dtype of returned Keras variable.
                                  None uses the dtype of x.
                             name: String, name for the variable to create.

                         # Returns
                             A Keras variable with the shape of x filled with zeros.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.variable(np.random.random((2,3)))
                             >>> kvar_zeros = K.zeros_like(kvar)
                             >>> K.eval(kvar_zeros)
                             array([[ 0.,  0.,  0.],
                                    [ 0.,  0.,  0.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         return tf.zeros_like(x, dtype=dtype, name=name)


                     def ones_like(x, dtype=None, name=None):
                         """Instantiates an all-ones variable of the same shape as another tensor.

                         # Arguments
                             x: Keras variable or tensor.
                             dtype: String, dtype of returned Keras variable.
                                  None uses the dtype of x.
                             name: String, name for the variable to create.

                         # Returns
                             A Keras variable with the shape of x filled with ones.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.variable(np.random.random((2,3)))
                             >>> kvar_ones = K.ones_like(kvar)
                             >>> K.eval(kvar_ones)
                             array([[ 1.,  1.,  1.],
                                    [ 1.,  1.,  1.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         return tf.ones_like(x, dtype=dtype, name=name)


                     def identity(x, name=None):
                         """Returns a tensor with the same content as the input tensor.

                         # Arguments
                             x: The input tensor.
                             name: String, name for the variable to create.

                         # Returns
                             A tensor of the same shape, type and content.
                         """
                         return tf.identity(x, name)


                     def random_uniform_variable(shape, low, high, dtype=None,
                                                 name=None, seed=None):
                         """Instantiates a variable with values drawn from a uniform distribution.

                         # Arguments
                             shape: Tuple of integers, shape of returned Keras variable.
                             low: Float, lower boundary of the output interval.
                             high: Float, upper boundary of the output interval.
                             dtype: String, dtype of returned Keras variable.
                             name: String, name of returned Keras variable.
                             seed: Integer, random seed.

                         # Returns
                             A Keras variable, filled with drawn samples.

                         # Example
                         ```python
                             # TensorFlow example
                             >>> kvar = K.random_uniform_variable((2,3), 0, 1)
                             >>> kvar
                             <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>
                             >>> K.eval(kvar)
                             array([[ 0.10940075,  0.10047495,  0.476143  ],
                                    [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         tf_dtype = tf.as_dtype(dtype)
                         if seed is None:
                             # ensure that randomness is conditioned by the Numpy RNG
                             seed = np.random.randint(10e8)
                         value = tf.random_uniform_initializer(
                             low, high, dtype=tf_dtype, seed=seed)(shape)
                         return variable(value, dtype=dtype, name=name)


                     def random_normal_variable(shape, mean, scale, dtype=None,
                                                name=None, seed=None):
                         """Instantiates a variable with values drawn from a normal distribution.

                         # Arguments
                             shape: Tuple of integers, shape of returned Keras variable.
                             mean: Float, mean of the normal distribution.
                             scale: Float, standard deviation of the normal distribution.
                             dtype: String, dtype of returned Keras variable.
                             name: String, name of returned Keras variable.
                             seed: Integer, random seed.

                         # Returns
                             A Keras variable, filled with drawn samples.

                         # Example
                         ```python
                             # TensorFlow example
                             >>> kvar = K.random_normal_variable((2,3), 0, 1)
                             >>> kvar
                             <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>
                             >>> K.eval(kvar)
                             array([[ 1.19591331,  0.68685907, -0.63814116],
                                    [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if dtype is None:
                             dtype = floatx()
                         tf_dtype = tf.as_dtype(dtype)
                         if seed is None:
                             # ensure that randomness is conditioned by the Numpy RNG
                             seed = np.random.randint(10e8)
                         value = tf.random_normal_initializer(
                             mean, scale, dtype=tf_dtype, seed=seed)(shape)
                         return variable(value, dtype=dtype, name=name)


                     def count_params(x):
                         """Returns the static number of elements in a Keras variable or tensor.

                         # Arguments
                             x: Keras variable or tensor.

                         # Returns
                             Integer, the number of elements in `x`, i.e., the product of the
                             array's static dimensions.

                         # Example
                         ```python
                             >>> kvar = K.zeros((2,3))
                             >>> K.count_params(kvar)
                             6
                             >>> K.eval(kvar)
                             array([[ 0.,  0.,  0.],
                                    [ 0.,  0.,  0.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         return np.prod(int_shape(x))


                     def cast(x, dtype):
                         """Casts a tensor to a different dtype and returns it.

                         You can cast a Keras variable but it still returns a Keras tensor.

                         # Arguments
                             x: Keras tensor (or variable).
                             dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).

                         # Returns
                             Keras tensor with dtype `dtype`.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> input = K.placeholder((2, 3), dtype='float32')
                             >>> input
                             <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
                             # It doesn't work in-place as below.
                             >>> K.cast(input, dtype='float16')
                             <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>
                             >>> input
                             <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>
                             # you need to assign it.
                             >>> input = K.cast(input, dtype='float16')
                             >>> input
                             <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>
                         ```
                         """
                         return tf.cast(x, dtype)


                     # UPDATES OPS


                     def update(x, new_x):
                         """Update the value of `x` to `new_x`.

                         # Arguments
                             x: A `Variable`.
                             new_x: A tensor of same shape as `x`.

                         # Returns
                             The variable `x` updated.
                         """
                         return tf.assign(x, new_x)


                     def update_add(x, increment):
                         """Update the value of `x` by adding `increment`.

                         # Arguments
                             x: A `Variable`.
                             increment: A tensor of same shape as `x`.

                         # Returns
                             The variable `x` updated.
                         """
                         return tf.assign_add(x, increment)


                     def update_sub(x, decrement):
                         """Update the value of `x` by subtracting `decrement`.

                         # Arguments
                             x: A `Variable`.
                             decrement: A tensor of same shape as `x`.

                         # Returns
                             The variable `x` updated.
                         """
                         return tf.assign_sub(x, decrement)


                     def moving_average_update(x, value, momentum):
                         """Compute the moving average of a variable.

                         # Arguments
                             x: A `Variable`.
                             value: A tensor with the same shape as `x`.
                             momentum: The moving average momentum.

                         # Returns
                             An operation to update the variable.
                         """
                         if value.dtype != x.dtype:
                             value = tf.cast(value, x.dtype)
                         return moving_averages.assign_moving_average(
                             x, value, momentum, zero_debias=True)


                     # LINEAR ALGEBRA

                     def dot(x, y):
                         """Multiplies 2 tensors (and/or variables) and returns a *tensor*.

                         When attempting to multiply a nD tensor
                         with a nD tensor, it reproduces the Theano behavior.
                         (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A tensor, dot product of `x` and `y`.

                         # Examples
                         ```python
                             # dot product between tensors
                             >>> x = K.placeholder(shape=(2, 3))
                             >>> y = K.placeholder(shape=(3, 4))
                             >>> xy = K.dot(x, y)
                             >>> xy
                             <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>
                         ```

                         ```python
                             # dot product between tensors
                             >>> x = K.placeholder(shape=(32, 28, 3))
                             >>> y = K.placeholder(shape=(3, 4))
                             >>> xy = K.dot(x, y)
                             >>> xy
                             <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>
                         ```

                         ```python
                             # Theano-like behavior example
                             >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
                             >>> y = K.ones((4, 3, 5))
                             >>> xy = K.dot(x, y)
                             >>> K.int_shape(xy)
                             (2, 4, 5)
                         ```
                         {{np_implementation}}
                         """
                         if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):
                             x_shape = []
                             for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):
                                 if i is not None:
                                     x_shape.append(i)
                                 else:
                                     x_shape.append(s)
                             x_shape = tuple(x_shape)
                             y_shape = []
                             for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):
                                 if i is not None:
                                     y_shape.append(i)
                                 else:
                                     y_shape.append(s)
                             y_shape = tuple(y_shape)
                             y_permute_dim = list(range(ndim(y)))
                             y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim
                             xt = tf.reshape(x, [-1, x_shape[-1]])
                             yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])
                             return tf.reshape(tf.matmul(xt, yt),
                                               x_shape[:-1] + y_shape[:-2] + y_shape[-1:])
                         if is_sparse(x):
                             out = tf.sparse_tensor_dense_matmul(x, y)
                         else:
                             out = tf.matmul(x, y)
                         return out


                     def batch_dot(x, y, axes=None):
                         """Batchwise dot product.

                         `batch_dot` is used to compute dot product of `x` and `y` when
                         `x` and `y` are data in batches, i.e. in a shape of
                         `(batch_size, :)`.
                         `batch_dot` results in a tensor or variable with less dimensions
                         than the input. If the number of dimensions is reduced to 1,
                         we use `expand_dims` to make sure that ndim is at least 2.

                         # Arguments
                             x: Keras tensor or variable with `ndim >= 2`.
                             y: Keras tensor or variable with `ndim >= 2`.
                             axes: int or tuple(int, int). Target dimensions to be reduced.

                         # Returns
                             A tensor with shape equal to the concatenation of `x`'s shape
                             (less the dimension that was summed over) and `y`'s shape
                             (less the batch dimension and the dimension that was summed over).
                             If the final rank is 1, we reshape it to `(batch_size, 1)`.

                         # Examples
                             Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`
                             `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal
                             of `x.dot(y.T)`, although we never have to calculate the off-diagonal
                             elements.

                             Pseudocode:
                             ```
                             inner_products = []
                             for xi, yi in zip(x, y):
                                 inner_products.append(xi.dot(yi))
                             result = stack(inner_products)
                             ```

                             Shape inference:
                             Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.
                             If `axes` is (1, 2), to find the output shape of resultant tensor,
                                 loop through each dimension in `x`'s shape and `y`'s shape:

                             * `x.shape[0]` : 100 : append to output shape
                             * `x.shape[1]` : 20 : do not append to output shape,
                                 dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)
                             * `y.shape[0]` : 100 : do not append to output shape,
                                 always ignore first dimension of `y`
                             * `y.shape[1]` : 30 : append to output shape
                             * `y.shape[2]` : 20 : do not append to output shape,
                                 dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)
                             `output_shape` = `(100, 30)`

                         ```python
                             >>> x_batch = K.ones(shape=(32, 20, 1))
                             >>> y_batch = K.ones(shape=(32, 30, 20))
                             >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=(1, 2))
                             >>> K.int_shape(xy_batch_dot)
                             (32, 1, 30)
                         ```

                         {{np_implementation}}
                         """
                         x_shape = int_shape(x)
                         y_shape = int_shape(y)

                         x_ndim = len(x_shape)
                         y_ndim = len(y_shape)

                         if x_ndim < 2 or y_ndim < 2:
                             raise ValueError('Can not do batch_dot on inputs '
                                              'with rank < 2. '
                                              'Received inputs with shapes ' +
                                              str(x_shape) + ' and ' +
                                              str(y_shape) + '.')

                         x_batch_size = x_shape[0]
                         y_batch_size = y_shape[0]

                         if x_batch_size is not None and y_batch_size is not None:
                             if x_batch_size != y_batch_size:
                                 raise ValueError('Can not do batch_dot on inputs '
                                                  'with different batch sizes. '
                                                  'Received inputs with shapes ' +
                                                  str(x_shape) + ' and ' +
                                                  str(y_shape) + '.')

                         if isinstance(axes, int):
                             axes = [axes, axes]

                         if axes is None:
                             if y_ndim == 2:
                                 axes = [x_ndim - 1, y_ndim - 1]
                             else:
                                 axes = [x_ndim - 1, y_ndim - 2]

                         if py_any([isinstance(a, (list, tuple)) for a in axes]):
                             raise ValueError('Multiple target dimensions are not supported. ' +
                                              'Expected: None, int, (int, int), ' +
                                              'Provided: ' + str(axes))

                         # if tuple, convert to list.
                         axes = list(axes)

                         # convert negative indices.
                         if axes[0] < 0:
                             axes[0] += x_ndim
                         if axes[1] < 0:
                             axes[1] += y_ndim

                         # sanity checks
                         if 0 in axes:
                             raise ValueError('Can not perform batch_dot over axis 0.'
                                              'If your inputs are not batched,'
                                              ' add a dummy batch dimension to your '
                                              'inputs using K.expand_dims(x, 0)')

                         a0, a1 = axes
                         d1 = x_shape[a0]
                         d2 = y_shape[a1]

                         if d1 is not None and d2 is not None and d1 != d2:
                             raise ValueError('Can not do batch_dot on inputs with shapes ' +
                                              str(x_shape) + ' and ' + str(y_shape) +
                                              ' with axes=' + str(axes) + '. x.shape[%d] != '
                                              'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))

                         # backup ndims. Need them later.
                         orig_x_ndim = x_ndim
                         orig_y_ndim = y_ndim

                         # if rank is 2, expand to 3.
                         if x_ndim == 2:
                             x = tf.expand_dims(x, 1)
                             a0 += 1
                             x_ndim += 1
                         if y_ndim == 2:
                             y = tf.expand_dims(y, 2)
                             y_ndim += 1

                         # bring x's dimension to be reduced to last axis.
                         if a0 != x_ndim - 1:
                             pattern = list(range(x_ndim))
                             for i in range(a0, x_ndim - 1):
                                 pattern[i] = pattern[i + 1]
                             pattern[-1] = a0
                             x = tf.transpose(x, pattern)

                         # bring y's dimension to be reduced to axis 1.
                         if a1 != 1:
                             pattern = list(range(y_ndim))
                             for i in range(a1, 1, -1):
                                 pattern[i] = pattern[i - 1]
                             pattern[1] = a1
                             y = tf.transpose(y, pattern)

                         # normalize both inputs to rank 3.
                         if x_ndim > 3:
                             # squash middle dimensions of x.
                             x_shape = shape(x)
                             x_mid_dims = x_shape[1:-1]
                             x_squashed_dim = tf.reduce_prod(x_mid_dims)
                             x_squashed_shape = tf.stack([x_shape[0], x_squashed_dim, x_shape[-1]])
                             x = tf.reshape(x, x_squashed_shape)
                             x_squashed = True
                         else:
                             x_squashed = False

                         if y_ndim > 3:
                             # squash trailing dimensions of y.
                             y_shape = shape(y)
                             y_trail_dims = y_shape[2:]
                             y_squashed_dim = tf.reduce_prod(y_trail_dims)
                             y_squashed_shape = tf.stack([y_shape[0], y_shape[1], y_squashed_dim])
                             y = tf.reshape(y, y_squashed_shape)
                             y_squashed = True
                         else:
                             y_squashed = False

                         result = tf.matmul(x, y)

                         # if inputs were squashed, we have to reshape the matmul output.
                         output_shape = tf.shape(result)
                         do_reshape = False

                         if x_squashed:
                             output_shape = tf.concat([output_shape[:1],
                                                       x_mid_dims,
                                                       output_shape[-1:]], 0)
                             do_reshape = True

                         if y_squashed:
                             output_shape = tf.concat([output_shape[:-1], y_trail_dims], 0)
                             do_reshape = True

                         if do_reshape:
                             result = tf.reshape(result, output_shape)

                         # if the inputs were originally rank 2, we remove the added 1 dim.
                         if orig_x_ndim == 2:
                             result = tf.squeeze(result, 1)
                         elif orig_y_ndim == 2:
                             result = tf.squeeze(result, -1)

                         return result


                     def transpose(x):
                         """Transposes a tensor and returns it.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.

                         # Examples
                         ```python
                             >>> var = K.variable([[1, 2, 3], [4, 5, 6]])
                             >>> K.eval(var)
                             array([[ 1.,  2.,  3.],
                                    [ 4.,  5.,  6.]], dtype=float32)
                             >>> var_transposed = K.transpose(var)
                             >>> K.eval(var_transposed)
                             array([[ 1.,  4.],
                                    [ 2.,  5.],
                                    [ 3.,  6.]], dtype=float32)
                         ```

                         ```python
                             >>> inputs = K.placeholder((2, 3))
                             >>> inputs
                             <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>
                             >>> input_transposed = K.transpose(inputs)
                             >>> input_transposed
                             <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>

                         ```
                         {{np_implementation}}
                         """
                         return tf.transpose(x)


                     def gather(reference, indices):
                         """Retrieves the elements of indices `indices` in the tensor `reference`.

                         # Arguments
                             reference: A tensor.
                             indices: An integer tensor of indices.

                         # Returns
                             A tensor of same type as `reference`.

                         {{np_implementation}}
                         """
                         return tf.nn.embedding_lookup(reference, indices)


                     # ELEMENT-WISE OPERATIONS


                     def max(x, axis=None, keepdims=False):
                         """Maximum value in a tensor.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to find maximum values. If `None` (default), finds the
                                 maximum over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with maximum values of `x`.

                         {{np_implementation}}
                         """
                         return tf.reduce_max(x, axis, keepdims)


                     def min(x, axis=None, keepdims=False):
                         """Minimum value in a tensor.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to find minimum values. If `None` (default), finds the
                                 minimum over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with miminum values of `x`.

                         {{np_implementation}}
                         """
                         return tf.reduce_min(x, axis, keepdims)


                     def sum(x, axis=None, keepdims=False):
                         """Sum of the values in a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to sum over. If `None` (default), sums over all
                                 dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with sum of `x`.

                         {{np_implementation}}
                         """
                         return tf.reduce_sum(x, axis, keepdims)


                     def prod(x, axis=None, keepdims=False):
                         """Multiplies the values in a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the product. If `None` (default), computes
                                 the product over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with the product of elements of `x`.

                         {{np_implementation}}
                         """
                         return tf.reduce_prod(x, axis, keepdims)


                     def cumsum(x, axis=0):
                         """Cumulative sum of the values in a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer, the axis to compute the sum.

                         # Returns
                             A tensor of the cumulative sum of values of `x` along `axis`.
                         {{np_implementation}}
                         """
                         return tf.cumsum(x, axis=axis)


                     def cumprod(x, axis=0):
                         """Cumulative product of the values in a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer, the axis to compute the product.

                         # Returns
                             A tensor of the cumulative product of values of `x` along `axis`.
                         {{np_implementation}}
                         """
                         return tf.cumprod(x, axis=axis)


                     def var(x, axis=None, keepdims=False):
                         """Variance of a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the variance. If `None` (default), computes
                                 the variance over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with the variance of elements of `x`.
                         {{np_implementation}}
                         """
                         if x.dtype.base_dtype == tf.bool:
                             x = tf.cast(x, floatx())
                         m = tf.reduce_mean(x, axis, True)
                         devs_squared = tf.square(x - m)
                         return tf.reduce_mean(devs_squared,
                                               axis,
                                               keepdims)


                     def std(x, axis=None, keepdims=False):
                         """Standard deviation of a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the standard deviation. If `None` (default),
                                 computes the standard deviation over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`,
                                 the reduced dimension is retained with length 1.

                         # Returns
                             A tensor with the standard deviation of elements of `x`.
                         {{np_implementation}}
                         """
                         return tf.sqrt(var(x, axis=axis, keepdims=keepdims))


                     def mean(x, axis=None, keepdims=False):
                         """Mean of a tensor, alongside the specified axis.

                         # Arguments
                             x: A tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the mean. If `None` (default), computes
                                 the mean over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1 for each entry in `axis`. If `keepdims` is `True`,
                                 the reduced dimensions are retained with length 1.

                         # Returns
                             A tensor with the mean of elements of `x`.
                         {{np_implementation}}
                         """
                         if x.dtype.base_dtype == tf.bool:
                             x = tf.cast(x, floatx())
                         return tf.reduce_mean(x, axis, keepdims)


                     def any(x, axis=None, keepdims=False):
                         """Bitwise reduction (logical OR).

                         # Arguments
                             x: Tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the logical or. If `None` (default), computes
                                 the logical or over all dimensions.
                             keepdims: whether the drop or broadcast the reduction axes.

                         # Returns
                             A uint8 tensor (0s and 1s).
                         {{np_implementation}}
                         """
                         x = tf.cast(x, tf.bool)
                         return tf.reduce_any(x, axis, keepdims)


                     def all(x, axis=None, keepdims=False):
                         """Bitwise reduction (logical AND).

                         # Arguments
                             x: Tensor or variable.
                             axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the logical and. If `None` (default), computes
                                 the logical and over all dimensions.
                             keepdims: whether the drop or broadcast the reduction axes.

                         # Returns
                             A uint8 tensor (0s and 1s).
                         {{np_implementation}}
                         """
                         x = tf.cast(x, tf.bool)
                         return tf.reduce_all(x, axis, keepdims)


                     def argmax(x, axis=-1):
                         """Returns the index of the maximum value along an axis.

                         # Arguments
                             x: Tensor or variable.
                             axis: axis along which to perform the reduction.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         return tf.argmax(x, axis)


                     def argmin(x, axis=-1):
                         """Returns the index of the minimum value along an axis.

                         # Arguments
                             x: Tensor or variable.
                             axis: axis along which to perform the reduction.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         return tf.argmin(x, axis)


                     def square(x):
                         """Element-wise square.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.square(x)


                     def abs(x):
                         """Element-wise absolute value.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.abs(x)


                     def sqrt(x):
                         """Element-wise square root.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         zero = _to_tensor(0., x.dtype.base_dtype)
                         inf = _to_tensor(np.inf, x.dtype.base_dtype)
                         x = tf.clip_by_value(x, zero, inf)
                         return tf.sqrt(x)


                     def exp(x):
                         """Element-wise exponential.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.exp(x)


                     def log(x):
                         """Element-wise log.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.log(x)


                     def logsumexp(x, axis=None, keepdims=False):
                         """Computes log(sum(exp(elements across dimensions of a tensor))).

                         This function is more numerically stable than log(sum(exp(x))).
                         It avoids overflows caused by taking the exp of large inputs and
                         underflows caused by taking the log of small inputs.

                         # Arguments
                             x: A tensor or variable.
                             axis: axis: An integer or list of integers in [-rank(x), rank(x)),
                                 the axes to compute the logsumexp. If `None` (default), computes
                                 the logsumexp over all dimensions.
                             keepdims: A boolean, whether to keep the dimensions or not.
                                 If `keepdims` is `False`, the rank of the tensor is reduced
                                 by 1. If `keepdims` is `True`, the reduced dimension is
                                 retained with length 1.

                         # Returns
                             The reduced tensor.
                         {{np_implementation}}
                         """
                         return tf.reduce_logsumexp(x, axis, keepdims)


                     def round(x):
                         """Element-wise rounding to the closest integer.

                         In case of tie, the rounding mode used is "half to even".

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.round(x)


                     def sign(x):
                         """Element-wise sign.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.sign(x)


                     def pow(x, a):
                         """Element-wise exponentiation.

                         # Arguments
                             x: Tensor or variable.
                             a: Python integer.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         return tf.pow(x, a)


                     def clip(x, min_value, max_value):
                         """Element-wise value clipping.

                         # Arguments
                             x: Tensor or variable.
                             min_value: Python float, integer or tensor.
                             max_value: Python float, integer or tensor.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         if (isinstance(min_value, (int, float)) and
                                 isinstance(max_value, (int, float))):
                             if max_value < min_value:
                                 max_value = min_value
                         if min_value is None:
                             min_value = -np.inf
                         if max_value is None:
                             max_value = np.inf
                         return tf.clip_by_value(x, min_value, max_value)


                     def equal(x, y):
                         """Element-wise equality between two tensors.

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.equal(x, y)


                     def not_equal(x, y):
                         """Element-wise inequality between two tensors.

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.not_equal(x, y)


                     def greater(x, y):
                         """Element-wise truth value of (x > y).

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.greater(x, y)


                     def greater_equal(x, y):
                         """Element-wise truth value of (x >= y).

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.greater_equal(x, y)


                     def less(x, y):
                         """Element-wise truth value of (x < y).

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.less(x, y)


                     def less_equal(x, y):
                         """Element-wise truth value of (x <= y).

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A bool tensor.

                         {{np_implementation}}
                         """
                         return tf.less_equal(x, y)


                     def maximum(x, y):
                         """Element-wise maximum of two tensors.

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.maximum(x, y)


                     def minimum(x, y):
                         """Element-wise minimum of two tensors.

                         # Arguments
                             x: Tensor or variable.
                             y: Tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.minimum(x, y)


                     def sin(x):
                         """Computes sin of x element-wise.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.sin(x)


                     def cos(x):
                         """Computes cos of x element-wise.

                         # Arguments
                             x: Tensor or variable.

                         # Returns
                             A tensor.
                         """
                         return tf.cos(x)


                     def _regular_normalize_batch_in_training(x, gamma, beta,
                                                              reduction_axes, epsilon=1e-3):
                         """Non-fused version of `normalize_batch_in_training`.

                         # Arguments
                             x: Input tensor or variable.
                             gamma: Tensor by which to scale the input.
                             beta: Tensor with which to center the input.
                             reduction_axes: iterable of integers,
                                 axes over which to normalize.
                             epsilon: Fuzz factor.

                         # Returns
                             A tuple length of 3, `(normalized_tensor, mean, variance)`.
                         """
                         mean, var = tf.nn.moments(x, reduction_axes,
                                                   None, None, False)
                         normed = tf.nn.batch_normalization(x, mean, var,
                                                            beta, gamma,
                                                            epsilon)
                         return normed, mean, var


                     def _broadcast_normalize_batch_in_training(x, gamma, beta,
                                                                reduction_axes, epsilon=1e-3):
                         """Non-fused, broadcast version of `normalize_batch_in_training`.

                         # Arguments
                             x: Input tensor or variable.
                             gamma: Tensor by which to scale the input.
                             beta: Tensor with which to center the input.
                             reduction_axes: iterable of integers,
                                 axes over which to normalize.
                             epsilon: Fuzz factor.

                         # Returns
                             A tuple length of 3, `(normalized_tensor, mean, variance)`.
                         """
                         mean, var = tf.nn.moments(x, reduction_axes,
                                                   None, None, False)
                         target_shape = []
                         for axis in range(ndim(x)):
                             if axis in reduction_axes:
                                 target_shape.append(1)
                             else:
                                 target_shape.append(tf.shape(x)[axis])
                         target_shape = tf.stack(target_shape)

                         broadcast_mean = tf.reshape(mean, target_shape)
                         broadcast_var = tf.reshape(var, target_shape)
                         if gamma is None:
                             broadcast_gamma = None
                         else:
                             broadcast_gamma = tf.reshape(gamma, target_shape)
                         if beta is None:
                             broadcast_beta = None
                         else:
                             broadcast_beta = tf.reshape(beta, target_shape)

                         normed = tf.nn.batch_normalization(
                             x,
                             broadcast_mean,
                             broadcast_var,
                             broadcast_beta,
                             broadcast_gamma,
                             epsilon)
                         return normed, mean, var


                     def _fused_normalize_batch_in_training(x, gamma, beta, reduction_axes,
                                                            epsilon=1e-3):
                         """Fused version of `normalize_batch_in_training`.

                         # Arguments
                             x: Input tensor or variable.
                             gamma: Tensor by which to scale the input.
                             beta: Tensor with which to center the input.
                             reduction_axes: iterable of integers,
                                 axes over which to normalize.
                             epsilon: Fuzz factor.

                         # Returns
                             A tuple length of 3, `(normalized_tensor, mean, variance)`.
                         """
                         if list(reduction_axes) == [0, 1, 2]:
                             normalization_axis = 3
                             tf_data_format = 'NHWC'
                         else:
                             normalization_axis = 1
                             tf_data_format = 'NCHW'

                         if gamma is None:
                             gamma = tf.constant(1.0,
                                                 dtype=x.dtype,
                                                 shape=[x.get_shape()[normalization_axis]])
                         if beta is None:
                             beta = tf.constant(0.0,
                                                dtype=x.dtype,
                                                shape=[x.get_shape()[normalization_axis]])

                         if gamma.dtype != tf.float32:
                             gamma = tf.cast(gamma, tf.float32)
                         if beta.dtype != tf.float32:
                             beta = tf.cast(beta, tf.float32)

                         return tf.nn.fused_batch_norm(
                             x,
                             gamma,
                             beta,
                             epsilon=epsilon,
                             data_format=tf_data_format)


                     def normalize_batch_in_training(x, gamma, beta,
                                                     reduction_axes, epsilon=1e-3):
                         """Computes mean and std for batch then apply batch_normalization on batch.

                         # Arguments
                             x: Input tensor or variable.
                             gamma: Tensor by which to scale the input.
                             beta: Tensor with which to center the input.
                             reduction_axes: iterable of integers,
                                 axes over which to normalize.
                             epsilon: Fuzz factor.

                         # Returns
                             A tuple length of 3, `(normalized_tensor, mean, variance)`.
                         """
                         if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:
                             if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:
                                 return _broadcast_normalize_batch_in_training(x, gamma, beta,
                                                                               reduction_axes,
                                                                               epsilon=epsilon)
                             return _fused_normalize_batch_in_training(
                                 x, gamma, beta, reduction_axes,
                                 epsilon=epsilon)
                         else:
                             if sorted(reduction_axes) == list(range(ndim(x)))[:-1]:
                                 return _regular_normalize_batch_in_training(x, gamma, beta,
                                                                             reduction_axes,
                                                                             epsilon=epsilon)
                             else:
                                 return _broadcast_normalize_batch_in_training(x, gamma, beta,
                                                                               reduction_axes,
                                                                               epsilon=epsilon)


                     def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):
                         """Applies batch normalization on x given mean, var, beta and gamma.

                         I.e. returns:
                         `output = (x - mean) / sqrt(var + epsilon) * gamma + beta`

                         # Arguments
                             x: Input tensor or variable.
                             mean: Mean of batch.
                             var: Variance of batch.
                             beta: Tensor with which to center the input.
                             gamma: Tensor by which to scale the input.
                             axis: Integer, the axis that should be normalized.
                                 (typically the features axis).
                             epsilon: Fuzz factor.

                         # Returns
                             A tensor.
                         """
                         if ndim(x) == 4:
                             # The CPU implementation of FusedBatchNorm only support NHWC
                             if axis == 1 or axis == -3:
                                 tf_data_format = 'NCHW'
                             elif axis == 3 or axis == -1:
                                 tf_data_format = 'NHWC'
                             else:
                                 tf_data_format = None

                             if (tf_data_format == 'NHWC'
                                     or tf_data_format == 'NCHW'
                                     and _has_nchw_support()):
                                 # The mean / var / beta / gamma may be processed by broadcast
                                 # so it may have extra axes with 1,
                                 # it is not needed and should be removed
                                 if ndim(mean) > 1:
                                     mean = tf.reshape(mean, [-1])
                                 if ndim(var) > 1:
                                     var = tf.reshape(var, [-1])
                                 if beta is None:
                                     beta = zeros_like(mean)
                                 elif ndim(beta) > 1:
                                     beta = tf.reshape(beta, [-1])
                                 if gamma is None:
                                     gamma = ones_like(mean)
                                 elif ndim(gamma) > 1:
                                     gamma = tf.reshape(gamma, [-1])

                                 if gamma.dtype != tf.float32:
                                     gamma = tf.cast(gamma, tf.float32)
                                 if beta.dtype != tf.float32:
                                     beta = tf.cast(beta, tf.float32)
                                 if mean.dtype != tf.float32:
                                     mean = tf.cast(mean, tf.float32)
                                 if var.dtype != tf.float32:
                                     var = tf.cast(var, tf.float32)

                                 y, _, _ = tf.nn.fused_batch_norm(
                                     x,
                                     gamma,
                                     beta,
                                     epsilon=epsilon,
                                     mean=mean,
                                     variance=var,
                                     data_format=tf_data_format,
                                     is_training=False
                                 )
                                 return y
                         # default
                         return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)


                     # SHAPE OPERATIONS

                     def concatenate(tensors, axis=-1):
                         """Concatenates a list of tensors alongside the specified axis.

                         # Arguments
                             tensors: list of tensors to concatenate.
                             axis: concatenation axis.

                         # Returns
                             A tensor.
                         """
                         if axis < 0:
                             rank = ndim(tensors[0])
                             if rank:
                                 axis %= rank
                             else:
                                 axis = 0

                         if py_all([is_sparse(x) for x in tensors]):
                             return tf.sparse_concat(axis, tensors)
                         else:
                             return tf.concat([to_dense(x) for x in tensors], axis)


                     def reshape(x, shape):
                         """Reshapes a tensor to the specified shape.

                         # Arguments
                             x: Tensor or variable.
                             shape: Target shape tuple.

                         # Returns
                             A tensor.
                         """
                         return tf.reshape(x, shape)


                     def permute_dimensions(x, pattern):
                         """Permutes axes in a tensor.

                         # Arguments
                             x: Tensor or variable.
                             pattern: A tuple of
                                 dimension indices, e.g. `(0, 2, 1)`.

                         # Returns
                             A tensor.
                         """
                         return tf.transpose(x, perm=pattern)


                     def resize_images(x,
                                       height_factor,
                                       width_factor,
                                       data_format,
                                       interpolation='nearest'):
                         """Resizes the images contained in a 4D tensor.

                         # Arguments
                             x: Tensor or variable to resize.
                             height_factor: Positive integer.
                             width_factor: Positive integer.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             interpolation: A string, one of `nearest` or `bilinear`.

                         # Returns
                             A tensor.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.
                         """
                         if data_format == 'channels_first':
                             rows, cols = 2, 3
                         else:
                             rows, cols = 1, 2

                         original_shape = int_shape(x)
                         new_shape = tf.shape(x)[rows:cols + 1]
                         new_shape *= tf.constant(np.array([height_factor, width_factor], dtype='int32'))

                         if data_format == 'channels_first':
                             x = permute_dimensions(x, [0, 2, 3, 1])
                         if interpolation == 'nearest':
                             x = tf.image.resize_nearest_neighbor(x, new_shape)
                         elif interpolation == 'bilinear':
                             x = tf.image.resize_bilinear(x, new_shape)
                         else:
                             raise ValueError('interpolation should be one '
                                              'of "nearest" or "bilinear".')
                         if data_format == 'channels_first':
                             x = permute_dimensions(x, [0, 3, 1, 2])

                         if original_shape[rows] is None:
                             new_height = None
                         else:
                             new_height = original_shape[rows] * height_factor

                         if original_shape[cols] is None:
                             new_width = None
                         else:
                             new_width = original_shape[cols] * width_factor

                         output_shape = (None, new_height, new_width, None)
                         x.set_shape(transpose_shape(output_shape, data_format, spatial_axes=(1, 2)))
                         return x


                     def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):
                         """Resizes the volume contained in a 5D tensor.

                         # Arguments
                             x: Tensor or variable to resize.
                             depth_factor: Positive integer.
                             height_factor: Positive integer.
                             width_factor: Positive integer.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             A tensor.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.
                         """
                         if data_format == 'channels_first':
                             output = repeat_elements(x, depth_factor, axis=2)
                             output = repeat_elements(output, height_factor, axis=3)
                             output = repeat_elements(output, width_factor, axis=4)
                             return output
                         elif data_format == 'channels_last':
                             output = repeat_elements(x, depth_factor, axis=1)
                             output = repeat_elements(output, height_factor, axis=2)
                             output = repeat_elements(output, width_factor, axis=3)
                             return output
                         else:
                             raise ValueError('Unknown data_format: ' + str(data_format))


                     def repeat_elements(x, rep, axis):
                         """Repeats the elements of a tensor along an axis, like `np.repeat`.

                         If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output
                         will have shape `(s1, s2 * rep, s3)`.

                         # Arguments
                             x: Tensor or variable.
                             rep: Python integer, number of times to repeat.
                             axis: Axis along which to repeat.

                         # Returns
                             A tensor.
                         """
                         x_shape = x.get_shape().as_list()
                         # For static axis
                         if x_shape[axis] is not None:
                             # slices along the repeat axis
                             splits = tf.split(value=x, num_or_size_splits=x_shape[axis], axis=axis)
                             # repeat each slice the given number of reps
                             x_rep = [s for s in splits for _ in range(rep)]
                             return concatenate(x_rep, axis)

                         # Here we use tf.tile to mimic behavior of np.repeat so that
                         # we can handle dynamic shapes (that include None).
                         # To do that, we need an auxiliary axis to repeat elements along
                         # it and then merge them along the desired axis.

                         # Repeating
                         auxiliary_axis = axis + 1
                         x_shape = tf.shape(x)
                         x_rep = tf.expand_dims(x, axis=auxiliary_axis)
                         reps = np.ones(len(x.get_shape()) + 1)
                         reps[auxiliary_axis] = rep
                         x_rep = tf.tile(x_rep, reps)

                         # Merging
                         reps = np.delete(reps, auxiliary_axis)
                         reps[axis] = rep
                         reps = tf.constant(reps, dtype='int32')
                         x_shape = x_shape * reps
                         x_rep = tf.reshape(x_rep, x_shape)

                         # Fix shape representation
                         x_shape = x.get_shape().as_list()
                         x_rep.set_shape(x_shape)
                         x_rep._keras_shape = tuple(x_shape)
                         return x_rep


                     def repeat(x, n):
                         """Repeats a 2D tensor.

                         if `x` has shape (samples, dim) and `n` is `2`,
                         the output will have shape `(samples, 2, dim)`.

                         # Arguments
                             x: Tensor or variable.
                             n: Python integer, number of times to repeat.

                         # Returns
                             A tensor.
                         """
                         assert ndim(x) == 2
                         x = tf.expand_dims(x, 1)
                         pattern = tf.stack([1, n, 1])
                         return tf.tile(x, pattern)


                     def arange(start, stop=None, step=1, dtype='int32'):
                         """Creates a 1D tensor containing a sequence of integers.

                         The function arguments use the same convention as
                         Theano's arange: if only one argument is provided,
                         it is in fact the "stop" argument and "start" is 0.

                         The default type of the returned tensor is `'int32'` to
                         match TensorFlow's default.

                         # Arguments
                             start: Start value.
                             stop: Stop value.
                             step: Difference between two successive values.
                             dtype: Integer dtype to use.

                         # Returns
                             An integer tensor.

                         """
                         # Match the behavior of numpy and Theano by returning an empty sequence.
                         if stop is None:
                             try:
                                 if start < 0:
                                     start = 0
                             except TypeError:
                                 # Handle case where start is a tensor
                                 start = tf.cond(start < 0,
                                                 true_fn=lambda: tf.constant(0, dtype=start.dtype),
                                                 false_fn=lambda: start)

                         result = tf.range(start, limit=stop, delta=step, name='arange')
                         if dtype != 'int32':
                             result = cast(result, dtype)
                         return result


                     def tile(x, n):
                         """Creates a tensor by tiling `x` by `n`.

                         # Arguments
                             x: A tensor or variable
                             n: A list of integer. The length must be the same as the number of
                                 dimensions in `x`.

                         # Returns
                             A tiled tensor.

                         # Example
                         ```python
                             >>> from keras import backend as K
                             >>> kvar = K.variable(np.random.random((2, 3)))
                             >>> kvar_tile = K.tile(K.eye(2), (2, 3))
                             >>> K.eval(kvar_tile)
                             array([[1., 0., 1., 0., 1., 0.],
                                    [0., 1., 0., 1., 0., 1.],
                                    [1., 0., 1., 0., 1., 0.],
                                    [0., 1., 0., 1., 0., 1.]], dtype=float32)
                         ```
                         {{np_implementation}}
                         """
                         if isinstance(n, int):
                             n = (n,)
                         elif isinstance(n, list):
                             n = tuple(n)

                         shape = int_shape(x)
                         if len(n) < len(shape):  # Padding the axis
                             n = tuple([1 for _ in range(len(shape) - len(n))]) + n
                         elif len(n) != len(shape):
                             raise NotImplementedError

                         return tf.tile(x, n)


                     def flatten(x):
                         """Flatten a tensor.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor, reshaped into 1-D
                         """
                         return tf.reshape(x, [-1])


                     def batch_flatten(x):
                         """Turn a nD tensor into a 2D tensor with same 0th dimension.

                         In other words, it flattens each data samples of a batch.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.
                         """
                         x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))
                         return x


                     def expand_dims(x, axis=-1):
                         """Adds a 1-sized dimension at index "axis".

                         # Arguments
                             x: A tensor or variable.
                             axis: Position where to add a new axis.

                         # Returns
                             A tensor with expanded dimensions.
                         """
                         return tf.expand_dims(x, axis)


                     def squeeze(x, axis):
                         """Removes a 1-dimension from the tensor at index "axis".

                         # Arguments
                             x: A tensor or variable.
                             axis: Axis to drop.

                         # Returns
                             A tensor with the same data as `x` but reduced dimensions.
                         """
                         return tf.squeeze(x, [axis])


                     def temporal_padding(x, padding=(1, 1)):
                         """Pads the middle dimension of a 3D tensor.

                         # Arguments
                             x: Tensor or variable.
                             padding: Tuple of 2 integers, how many zeros to
                                 add at the start and end of dim 1.

                         # Returns
                             A padded 3D tensor.
                         """
                         assert len(padding) == 2
                         pattern = [[0, 0], [padding[0], padding[1]], [0, 0]]
                         return tf.pad(x, pattern)


                     def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):
                         """Pads the 2nd and 3rd dimensions of a 4D tensor.

                         # Arguments
                             x: Tensor or variable.
                             padding: Tuple of 2 tuples, padding pattern.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             A padded 4D tensor.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.
                         """
                         assert len(padding) == 2
                         assert len(padding[0]) == 2
                         assert len(padding[1]) == 2
                         data_format = normalize_data_format(data_format)

                         pattern = [[0, 0],
                                    list(padding[0]),
                                    list(padding[1]),
                                    [0, 0]]
                         pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2))
                         return tf.pad(x, pattern)


                     def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):
                         """Pads 5D tensor with zeros along the depth, height, width dimensions.

                         Pads these dimensions with respectively
                         "padding[0]", "padding[1]" and "padding[2]" zeros left and right.

                         For 'channels_last' data_format,
                         the 2nd, 3rd and 4th dimension will be padded.
                         For 'channels_first' data_format,
                         the 3rd, 4th and 5th dimension will be padded.

                         # Arguments
                             x: Tensor or variable.
                             padding: Tuple of 3 tuples, padding pattern.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             A padded 5D tensor.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.

                         """
                         assert len(padding) == 3
                         assert len(padding[0]) == 2
                         assert len(padding[1]) == 2
                         assert len(padding[2]) == 2
                         data_format = normalize_data_format(data_format)

                         pattern = [
                             [0, 0],
                             [padding[0][0], padding[0][1]],
                             [padding[1][0], padding[1][1]],
                             [padding[2][0], padding[2][1]],
                             [0, 0]
                         ]
                         pattern = transpose_shape(pattern, data_format, spatial_axes=(1, 2, 3))

                         return tf.pad(x, pattern)


                     def stack(x, axis=0):
                         """Stacks a list of rank `R` tensors into a rank `R+1` tensor.

                         # Arguments
                             x: List of tensors.
                             axis: Axis along which to perform stacking.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.stack(x, axis=axis)


                     def one_hot(indices, num_classes):
                         """Computes the one-hot representation of an integer tensor.

                         # Arguments
                             indices: nD integer tensor of shape
                                 `(batch_size, dim1, dim2, ... dim(n-1))`
                             num_classes: Integer, number of classes to consider.

                         # Returns
                             (n + 1)D one hot representation of the input
                             with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`
                         """
                         return tf.one_hot(indices, depth=num_classes, axis=-1)


                     def reverse(x, axes):
                         """Reverses a tensor along the specified axes.

                         # Arguments
                             x: Tensor to reverse.
                             axes: Integer or iterable of integers.
                                 Axes to reverse.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         if isinstance(axes, int):
                             axes = [axes]
                         return tf.reverse(x, axes)


                     def slice(x, start, size):
                         """Extracts a slice from a tensor.

                         # Arguments
                             x: Input tensor.
                             start: Integer list/tuple or tensor
                                 indicating the start indices of the slice
                                 along each axis.
                             size: Integer list/tuple or tensor
                                 indicating how many dimensions to slice
                                 along each axis.

                         # Returns
                             A sliced tensor:
                             ```python
                             new_x = x[start[0]: start[0] + size[0], ..., start[-1]: start[-1] + size[-1]]
                             ```

                         # Raises
                             ValueError: if the dimension and the size of indices mismatches.

                         {{np_implementation}}
                         """
                         x_shape = int_shape(x)
                         if (x_shape is not None) and (x_shape[0] is not None):
                             len_start = int_shape(start)[0] if is_tensor(start) else len(start)
                             len_size = int_shape(size)[0] if is_tensor(size) else len(size)
                             if not (len(int_shape(x)) == len_start == len_size):
                                 raise ValueError('The dimension and the size of indices should match.')
                         return tf.slice(x, start, size)


                     # VALUE MANIPULATION


                     def get_value(x):
                         """Returns the value of a variable.

                         # Arguments
                             x: input variable.

                         # Returns
                             A Numpy array.
                         """
                         return x.eval(session=get_session())


                     def batch_get_value(ops):
                         """Returns the value of more than one tensor variable.

                         # Arguments
                             ops: list of ops to run.

                         # Returns
                             A list of Numpy arrays.
                         """
                         if ops:
                             return get_session().run(ops)
                         else:
                             return []


                     def set_value(x, value):
                         """Sets the value of a variable, from a Numpy array.

                         # Arguments
                             x: Tensor to set to a new value.
                             value: Value to set the tensor to, as a Numpy array
                                 (of the same shape).
                         """
                         value = np.asarray(value, dtype=dtype(x))
                         tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])
                         if hasattr(x, '_assign_placeholder'):
                             assign_placeholder = x._assign_placeholder
                             assign_op = x._assign_op
                         else:
                             assign_placeholder = tf.placeholder(tf_dtype, shape=value.shape)
                             assign_op = x.assign(assign_placeholder)
                             x._assign_placeholder = assign_placeholder
                             x._assign_op = assign_op
                         get_session().run(assign_op, feed_dict={assign_placeholder: value})


                     def batch_set_value(tuples):
                         """Sets the values of many tensor variables at once.

                         # Arguments
                             tuples: a list of tuples `(tensor, value)`.
                                 `value` should be a Numpy array.
                         """
                         if tuples:
                             assign_ops = []
                             feed_dict = {}
                             for x, value in tuples:
                                 value = np.asarray(value, dtype=dtype(x))
                                 tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])
                                 if hasattr(x, '_assign_placeholder'):
                                     assign_placeholder = x._assign_placeholder
                                     assign_op = x._assign_op
                                 else:
                                     assign_placeholder = tf.placeholder(tf_dtype,
                                                                         shape=value.shape)
                                     assign_op = x.assign(assign_placeholder)
                                     x._assign_placeholder = assign_placeholder
                                     x._assign_op = assign_op
                                 assign_ops.append(assign_op)
                                 feed_dict[assign_placeholder] = value
                             get_session().run(assign_ops, feed_dict=feed_dict)


                     def get_variable_shape(x):
                         """Returns the shape of a variable.

                         # Arguments
                             x: A variable.

                         # Returns
                             A tuple of integers.
                         """
                         return int_shape(x)


                     def print_tensor(x, message=''):
                         """Prints `message` and the tensor value when evaluated.

                          Note that `print_tensor` returns a new tensor identical to `x`
                          which should be used in the following code. Otherwise the
                          print operation is not taken into account during evaluation.

                          # Example
                          ```python
                              >>> x = K.print_tensor(x, message="x is: ")
                          ```

                         # Arguments
                             x: Tensor to print.
                             message: Message to print jointly with the tensor.

                         # Returns
                             The same tensor `x`, unchanged.
                         """
                         return tf.Print(x, [x], message)


                     # GRAPH MANIPULATION

                     class Function(object):
                         """Runs a computation graph.

                         It's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.
                         In particular additional operations via `fetches` argument and additional
                         tensor substitutions via `feed_dict` arguments. Note that given
                         substitutions are merged with substitutions from `inputs`. Even though
                         `feed_dict` is passed once in the constructor (called in `model.compile()`)
                         we can modify the values in the dictionary. Through this feed_dict we can
                         provide additional substitutions besides Keras inputs.

                         # Arguments
                             inputs: Feed placeholders to the computation graph.
                             outputs: Output tensors to fetch.
                             updates: Additional update ops to be run at function call.
                             name: a name to help users identify what this function does.
                             session_kwargs: arguments to `tf.Session.run()`:
                                 `fetches`, `feed_dict`,
                                 `options`, `run_metadata`
                         """

                         def __init__(self, inputs, outputs,
                                      updates=None,
                                      name=None,
                                      **session_kwargs):
                             updates = updates or []
                             if not isinstance(inputs, (list, tuple)):
                                 raise TypeError('`inputs` to a TensorFlow backend function '
                                                 'should be a list or tuple.')
                             if not isinstance(outputs, (list, tuple)):
                                 raise TypeError('`outputs` of a TensorFlow backend function '
                                                 'should be a list or tuple.')
                             if not isinstance(updates, (list, tuple)):
                                 raise TypeError('`updates` in a TensorFlow backend function '
                                                 'should be a list or tuple.')
                             self.inputs = list(inputs)
                             self.outputs = list(outputs)
                             with tf.control_dependencies(self.outputs):
                                 updates_ops = []
                                 for update in updates:
                                     if isinstance(update, tuple):
                                         p, new_p = update
                                         updates_ops.append(tf.assign(p, new_p))
                                     else:
                                         # assumed already an op
                                         updates_ops.append(update)
                                 self.updates_op = tf.group(*updates_ops)
                             self.name = name
                             # additional tensor substitutions
                             self.feed_dict = session_kwargs.pop('feed_dict', {})
                             # additional operations
                             self.fetches = session_kwargs.pop('fetches', [])
                             if not isinstance(self.fetches, list):
                                 self.fetches = [self.fetches]
                             # The main use case of `fetches` being passed to a model is the ability
                             # to run custom updates
                             # (since the outputs of fetches are never returned).
                             # This requires us to wrap fetches in `identity` ops.
                             self.fetches = [tf.identity(x) for x in self.fetches]
                             # self.session_kwargs is used for _legacy_call
                             self.session_kwargs = session_kwargs.copy()
                             self.run_options = session_kwargs.pop('options', None)
                             self.run_metadata = session_kwargs.pop('run_metadata', None)
                             if session_kwargs:
                                 raise ValueError('Some keys in session_kwargs are not '
                                                  'supported at this '
                                                  'time: %s', session_kwargs.keys())
                             self._callable_fn = None
                             self._feed_arrays = None
                             self._feed_symbols = None
                             self._symbol_vals = None
                             self._session = None

                         def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):
                             """Generates a callable that runs the graph.

                             # Arguments
                                 feed_arrays: List of input tensors to be fed
                                     Numpy arrays at runtime.
                                 feed_symbols: List of input tensors to be fed
                                     symbolic tensors at runtime.
                                 symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.
                                 session: Session to use to generate the callable.

                             # Returns
                                 Function that runs the graph according to the above options.
                             """
                             # Prepare callable options.
                             callable_opts = config_pb2.CallableOptions()
                             # Handle external-data feed.
                             for x in feed_arrays:
                                 callable_opts.feed.append(x.name)
                             if self.feed_dict:
                                 for key in sorted(self.feed_dict.keys()):
                                     callable_opts.feed.append(key.name)
                             # Handle symbolic feed.
                             for x, y in zip(feed_symbols, symbol_vals):
                                 connection = callable_opts.tensor_connection.add()
                                 if x.dtype != y.dtype:
                                     y = tf.cast(y, dtype=x.dtype)
                                 from_tensor = tf_ops._as_graph_element(y)
                                 if from_tensor is None:
                                     from_tensor = y
                                 connection.from_tensor = from_tensor.name  # Data tensor
                                 connection.to_tensor = x.name  # Placeholder
                             # Handle fetches.
                             for x in self.outputs + self.fetches:
                                 callable_opts.fetch.append(x.name)
                             # Handle updates.
                             callable_opts.target.append(self.updates_op.name)
                             # Handle run_options.
                             if self.run_options:
                                 callable_opts.run_options.CopyFrom(self.run_options)
                             # Create callable.
                             callable_fn = session._make_callable_from_options(callable_opts)
                             # Cache parameters corresponding to the generated callable, so that
                             # we can detect future mismatches and refresh the callable.
                             self._callable_fn = callable_fn
                             self._feed_arrays = feed_arrays
                             self._feed_symbols = feed_symbols
                             self._symbol_vals = symbol_vals
                             self._session = session

                         def _call(self, inputs):
                             if not isinstance(inputs, (list, tuple)):
                                 raise TypeError('`inputs` should be a list or tuple.')

                             session = get_session()
                             feed_arrays = []
                             array_vals = []
                             feed_symbols = []
                             symbol_vals = []
                             for tensor, value in zip(self.inputs, inputs):
                                 if value is None:
                                     continue
                                 if is_tensor(value):
                                     # Case: feeding symbolic tensor.
                                     feed_symbols.append(tensor)
                                     symbol_vals.append(value)
                                 else:
                                     feed_arrays.append(tensor)
                                     # We need to do array conversion and type casting
                                     # at this level, since
                                     # `callable_fn` only supports exact matches.
                                     array_vals.append(
                                         np.asarray(value,
                                                    dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))
                             if self.feed_dict:
                                 for key in sorted(self.feed_dict.keys()):
                                     array_vals.append(
                                         np.asarray(self.feed_dict[key],
                                                    dtype=tf.as_dtype(key.dtype).as_numpy_dtype))

                             # Refresh callable if anything has changed.
                             if (self._callable_fn is None or
                                     feed_arrays != self._feed_arrays or
                                     symbol_vals != self._symbol_vals or
                                     feed_symbols != self._feed_symbols or
                                     session != self._session):
                                 self._make_callable(feed_arrays,
                                                     feed_symbols,
                                                     symbol_vals,
                                                     session)
                             if self.run_metadata:
                                 fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
                             else:
                                 fetched = self._callable_fn(*array_vals)
                             return fetched[:len(self.outputs)]

                         def _legacy_call(self, inputs):
                             if not isinstance(inputs, (list, tuple)):
                                 raise TypeError('`inputs` should be a list or tuple.')
                             feed_dict = self.feed_dict.copy()
                             for tensor, value in zip(self.inputs, inputs):
                                 if is_sparse(tensor):
                                     sparse_coo = value.tocoo()
                                     indices = np.concatenate(
                                         (np.expand_dims(sparse_coo.row, 1),
                                          np.expand_dims(sparse_coo.col, 1)), 1)
                                     value = (indices, sparse_coo.data, sparse_coo.shape)
                                 feed_dict[tensor] = value
                             fetches = self.outputs + [self.updates_op] + self.fetches
                             session = get_session()
                             updated = session.run(fetches=fetches, feed_dict=feed_dict,
                                                   **self.session_kwargs)
                             return updated[:len(self.outputs)]

                         def __call__(self, inputs):
                             if hasattr(get_session(), '_make_callable_from_options'):
                                 if py_any(is_sparse(x) for x in self.inputs):
                                     if py_any(is_tensor(x) for x in inputs):
                                         raise ValueError(
                                             'Feeding from symbolic tensors is not '
                                             'supported with sparse inputs.')
                                     return self._legacy_call(inputs)

                                 # callable generated by Session._make_callable_from_options accepts
                                 # `run_metadata` keyword argument since TF 1.10
                                 if self.run_metadata:
                                     current_version = StrictVersion(tf.__version__.split('-')[0])
                                     if current_version < StrictVersion('1.10.0'):
                                         if py_any(is_tensor(x) for x in inputs):
                                             raise ValueError(
                                                 'In order to feed symbolic tensors '
                                                 'to a Keras model and set '
                                                 '`run_metadata`, you need tensorflow 1.10 or higher.')
                                         return self._legacy_call(inputs)

                                 return self._call(inputs)
                             else:
                                 if py_any(is_tensor(x) for x in inputs):
                                     raise ValueError(
                                         'In order to feed symbolic tensors to a Keras model '
                                         'in TensorFlow, you need tensorflow 1.8 or higher.')
                                 return self._legacy_call(inputs)


                     def function(inputs, outputs, updates=None, **kwargs):
                         """Instantiates a Keras function.

                         # Arguments
                             inputs: List of placeholder tensors.
                             outputs: List of output tensors.
                             updates: List of update ops.
                             **kwargs: Passed to `tf.Session.run`.

                         # Returns
                             Output values as Numpy arrays.

                         # Raises
                             ValueError: if invalid kwargs are passed in.
                         """
                         if kwargs:
                             for key in kwargs:
                                 session_has_key = has_arg(tf.Session.run, key, True)
                                 function_has_key = has_arg(Function.__init__, key, True)
                                 if not (session_has_key or function_has_key):
                                     raise ValueError('Invalid argument "%s" passed to K.function '
                                                      'with TensorFlow backend' % key)
                         return Function(inputs, outputs, updates=updates, **kwargs)


                     def gradients(loss, variables):
                         """Returns the gradients of `loss` w.r.t. `variables`.

                         # Arguments
                             loss: Scalar tensor to minimize.
                             variables: List of variables.

                         # Returns
                             A gradients tensor.
                         """
                         return tf.gradients(loss, variables, colocate_gradients_with_ops=True)


                     def stop_gradient(variables):
                         """Returns `variables` but with zero gradient w.r.t. every other variable.

                         # Arguments
                             variables: tensor or list of tensors to consider constant with respect
                                 to any other variable.

                         # Returns
                             A single tensor or a list of tensors (depending on the passed argument)
                                 that has constant gradient with respect to any other variable.
                         """
                         if isinstance(variables, (list, tuple)):
                             return map(tf.stop_gradient, variables)
                         else:
                             return tf.stop_gradient(variables)


                     # CONTROL FLOW

                     def rnn(step_function, inputs, initial_states,
                             go_backwards=False, mask=None, constants=None,
                             unroll=False, input_length=None):
                         """Iterates over the time dimension of a tensor.

                         # Arguments
                             step_function:
                                 Parameters:
                                     inputs: Tensor with shape (samples, ...) (no time dimension),
                                         representing input for the batch of samples at a certain
                                         time step.
                                     states: List of tensors.
                                 Returns:
                                     outputs: Tensor with shape (samples, ...) (no time dimension),
                                     new_states: List of tensors, same length and shapes
                                         as 'states'.
                             inputs: Tensor of temporal data of shape (samples, time, ...)
                                 (at least 3D).
                             initial_states: Tensor with shape (samples, ...) (no time dimension),
                                 containing the initial values for the states used in
                                 the step function.
                             go_backwards: Boolean. If True, do the iteration over the time
                                 dimension in reverse order and return the reversed sequence.
                             mask: Binary tensor with shape (samples, time),
                                 with a zero for every element that is masked.
                             constants: A list of constant values passed at each step.
                             unroll: Whether to unroll the RNN or to use a symbolic loop
                                 (`while_loop` or `scan` depending on backend).
                             input_length: Static number of timesteps in the input.

                         # Returns
                             A tuple, `(last_output, outputs, new_states)`.

                             last_output: The latest output of the rnn, of shape `(samples, ...)`
                             outputs: Tensor with shape `(samples, time, ...)` where each
                                 entry `outputs[s, t]` is the output of the step function
                                 at time `t` for sample `s`.
                             new_states: List of tensors, latest states returned by
                                 the step function, of shape `(samples, ...)`.

                         # Raises
                             ValueError: If input dimension is less than 3.
                             ValueError: If `unroll` is `True`
                                 but input timestep is not a fixed number.
                             ValueError: If `mask` is provided (not `None`)
                                 but states is not provided (`len(states)` == 0).

                         {{np_implementation}}
                         """
                         ndim = len(inputs.shape)
                         if ndim < 3:
                             raise ValueError('Input should be at least 3D.')

                         # Transpose to time-major, i.e.
                         # from (batch, time, ...) to (time, batch, ...)
                         axes = [1, 0] + list(range(2, ndim))
                         inputs = tf.transpose(inputs, (axes))

                         if mask is not None:
                             if mask.dtype != tf.bool:
                                 mask = tf.cast(mask, tf.bool)
                             if len(mask.shape) != 2:
                                 raise ValueError(
                                     'mask should have `shape=(samples, time)`, '
                                     'got {}'.format(mask.shape))
                             mask = tf.transpose(mask, [1, 0])

                             def get_matching_mask(mask_t, ref_tensor_t):
                                 # tf.where needs its condition tensor
                                 # to be the same shape as its two
                                 # result tensors
                                 ndim = len(ref_tensor_t.shape)
                                 for _ in range(ndim - 1):
                                     mask_t = expand_dims(mask_t)
                                 add_shape = tf.shape(ref_tensor_t)[1:]
                                 multiple = tf.concat([[1], add_shape], 0)
                                 return tf.tile(mask_t, multiple)

                         if constants is None:
                             constants = []

                         uses_learning_phase = [False]

                         if unroll:
                             if not inputs.shape[0]:
                                 raise ValueError('Unrolling requires a '
                                                  'fixed number of timesteps.')
                             states = initial_states
                             successive_states = []
                             successive_outputs = []

                             input_list = tf.unstack(inputs)
                             if go_backwards:
                                 input_list.reverse()

                             if mask is not None:
                                 mask_list = tf.unstack(mask)
                                 if go_backwards:
                                     mask_list.reverse()

                                 for inp, mask_t in zip(input_list, mask_list):
                                     output, new_states = step_function(inp, states + constants)
                                     if getattr(output, '_uses_learning_phase', False):
                                         uses_learning_phase[0] = True

                                     if not successive_outputs:
                                         prev_output = zeros_like(output)
                                     else:
                                         prev_output = successive_outputs[-1]

                                     output_mask_t = get_matching_mask(mask_t, output)
                                     output = tf.where(output_mask_t, output, prev_output)

                                     return_states = []
                                     for state, new_state in zip(states, new_states):
                                         state_mask_t = get_matching_mask(mask_t, new_state)
                                         return_states.append(tf.where(state_mask_t,
                                                                       new_state,
                                                                       state))
                                     states = return_states
                                     successive_outputs.append(output)
                                     successive_states.append(states)
                                 last_output = successive_outputs[-1]
                                 new_states = successive_states[-1]
                                 outputs = tf.stack(successive_outputs)
                             else:
                                 for inp in input_list:
                                     output, states = step_function(inp, states + constants)
                                     if getattr(output, '_uses_learning_phase', False):
                                         uses_learning_phase[0] = True
                                     successive_outputs.append(output)
                                     successive_states.append(states)
                                 last_output = successive_outputs[-1]
                                 new_states = successive_states[-1]
                                 outputs = tf.stack(successive_outputs)

                         else:
                             if go_backwards:
                                 inputs = reverse(inputs, 0)

                             states = tuple(initial_states)

                             time_steps = tf.shape(inputs)[0]
                             output, _ = step_function(inputs[0], initial_states + constants)
                             output_ta = tensor_array_ops.TensorArray(
                                 dtype=output.dtype,
                                 size=time_steps,
                                 tensor_array_name='output_ta')
                             initial_output = zeros_like(output)
                             input_ta = tensor_array_ops.TensorArray(
                                 dtype=inputs.dtype,
                                 size=time_steps,
                                 tensor_array_name='input_ta')
                             input_ta = input_ta.unstack(inputs)
                             time = tf.constant(0, dtype='int32', name='time')
                             while_loop_kwargs = {
                                 'cond': lambda time, *_: time < time_steps,
                                 'parallel_iterations': 32,
                                 'swap_memory': True,
                                 'maximum_iterations': input_length}

                             if mask is not None:
                                 if go_backwards:
                                     mask = reverse(mask, 0)

                                 mask_ta = tensor_array_ops.TensorArray(
                                     dtype=tf.bool,
                                     size=time_steps,
                                     tensor_array_name='mask_ta')
                                 mask_ta = mask_ta.unstack(mask)

                                 def _step(time, output_ta_t, output_tm1, *states):
                                     """RNN step function.

                                     # Arguments
                                         time: Current timestep value.
                                         output_ta_t: TensorArray.
                                         output_tm1: output Tensor from previous timestep
                                         *states: List of states.

                                     # Returns
                                         Tuple: `(time + 1,output_ta_t) + tuple(new_states)`
                                     """
                                     current_input = input_ta.read(time)
                                     mask_t = mask_ta.read(time)
                                     output, new_states = step_function(current_input,
                                                                        tuple(states) +
                                                                        tuple(constants))
                                     if getattr(output, '_uses_learning_phase', False):
                                         uses_learning_phase[0] = True
                                     for state, new_state in zip(states, new_states):
                                         new_state.set_shape(state.shape)

                                     output_mask_t = get_matching_mask(mask_t, output)
                                     output = tf.where(output_mask_t, output, output_tm1)

                                     new_states = [tf.where(get_matching_mask(mask_t, new_states[i]),
                                                            new_states[i],
                                                            states[i]) for i in range(len(states))]

                                     output_ta_t = output_ta_t.write(time, output)
                                     return (time + 1, output_ta_t, output) + tuple(new_states)

                                 final_outputs = control_flow_ops.while_loop(
                                     body=_step,
                                     loop_vars=(time, output_ta, initial_output) + states,
                                     **while_loop_kwargs)
                                 new_states = final_outputs[3:]  # skip output_tm1
                             else:
                                 def _step(time, output_ta_t, *states):
                                     """RNN step function.

                                     # Arguments
                                         time: Current timestep value.
                                         output_ta_t: TensorArray.
                                         *states: List of states.

                                     # Returns
                                         Tuple: `(time + 1,output_ta_t) + tuple(new_states)`
                                     """
                                     current_input = input_ta.read(time)
                                     output, new_states = step_function(current_input,
                                                                        tuple(states) +
                                                                        tuple(constants))
                                     if getattr(output, '_uses_learning_phase', False):
                                         uses_learning_phase[0] = True
                                     for state, new_state in zip(states, new_states):
                                         new_state.set_shape(state.shape)
                                     output_ta_t = output_ta_t.write(time, output)
                                     return (time + 1, output_ta_t) + tuple(new_states)

                                 final_outputs = control_flow_ops.while_loop(
                                     body=_step,
                                     loop_vars=(time, output_ta) + states,
                                     **while_loop_kwargs)
                                 new_states = final_outputs[2:]

                             last_time = final_outputs[0]
                             output_ta = final_outputs[1]
                             outputs = output_ta.stack()
                             last_output = output_ta.read(last_time - 1)

                         axes = [1, 0] + list(range(2, len(outputs.shape)))
                         outputs = tf.transpose(outputs, axes)
                         last_output._uses_learning_phase = uses_learning_phase[0]
                         return last_output, outputs, new_states


                     def switch(condition, then_expression, else_expression):
                         """Switches between two operations depending on a scalar value.

                         Note that both `then_expression` and `else_expression`
                         should be symbolic tensors of the *same shape*.

                         # Arguments
                             condition: tensor (`int` or `bool`).
                             then_expression: either a tensor, or a callable that returns a tensor.
                             else_expression: either a tensor, or a callable that returns a tensor.

                         # Returns
                             The selected tensor.

                         # Raises
                             ValueError: If rank of `condition` is greater than rank of expressions.

                         {{np_implementation}}
                         """
                         if condition.dtype != tf.bool:
                             condition = tf.cast(condition, 'bool')
                         cond_ndim = ndim(condition)
                         if not cond_ndim:
                             if not callable(then_expression):
                                 def then_expression_fn():
                                     return then_expression
                             else:
                                 then_expression_fn = then_expression
                             if not callable(else_expression):
                                 def else_expression_fn():
                                     return else_expression
                             else:
                                 else_expression_fn = else_expression
                             x = tf.cond(condition,
                                         then_expression_fn,
                                         else_expression_fn)
                         else:
                             # tf.where needs its condition tensor
                             # to be the same shape as its two
                             # result tensors
                             if callable(then_expression):
                                 then_expression = then_expression()
                             if callable(else_expression):
                                 else_expression = else_expression()
                             expr_ndim = ndim(then_expression)
                             if cond_ndim > expr_ndim:
                                 raise ValueError('Rank of `condition` should be less than or'
                                                  ' equal to rank of `then_expression` and '
                                                  '`else_expression`. ndim(condition)=' +
                                                  str(cond_ndim) + ', ndim(then_expression)'
                                                  '=' + str(expr_ndim))
                             if cond_ndim > 1:
                                 ndim_diff = expr_ndim - cond_ndim
                                 cond_shape = tf.concat([tf.shape(condition), [1] * ndim_diff], axis=0)
                                 condition = tf.reshape(condition, cond_shape)
                                 expr_shape = tf.shape(then_expression)
                                 shape_diff = expr_shape - cond_shape
                                 zero_expr_shape = tf.ones_like(expr_shape)
                                 tile_shape = tf.where(shape_diff > 0, expr_shape, zero_expr_shape)
                                 condition = tf.tile(condition, tile_shape)
                             x = tf.where(condition, then_expression, else_expression)
                         return x


                     def in_train_phase(x, alt, training=None):
                         """Selects `x` in train phase, and `alt` otherwise.

                         Note that `alt` should have the *same shape* as `x`.

                         # Arguments
                             x: What to return in train phase
                                 (tensor or callable that returns a tensor).
                             alt: What to return otherwise
                                 (tensor or callable that returns a tensor).
                             training: Optional scalar tensor
                                 (or Python boolean, or Python integer)
                                 specifying the learning phase.

                         # Returns
                             Either `x` or `alt` based on the `training` flag.
                             the `training` flag defaults to `K.learning_phase()`.
                         """
                         if training is None:
                             training = learning_phase()
                             uses_learning_phase = True
                         else:
                             uses_learning_phase = False

                         if training is 1 or training is True:
                             if callable(x):
                                 return x()
                             else:
                                 return x

                         elif training is 0 or training is False:
                             if callable(alt):
                                 return alt()
                             else:
                                 return alt

                         # else: assume learning phase is a placeholder tensor.
                         x = switch(training, x, alt)
                         if uses_learning_phase:
                             x._uses_learning_phase = True
                         return x


                     def in_test_phase(x, alt, training=None):
                         """Selects `x` in test phase, and `alt` otherwise.

                         Note that `alt` should have the *same shape* as `x`.

                         # Arguments
                             x: What to return in test phase
                                 (tensor or callable that returns a tensor).
                             alt: What to return otherwise
                                 (tensor or callable that returns a tensor).
                             training: Optional scalar tensor
                                 (or Python boolean, or Python integer)
                                 specifying the learning phase.

                         # Returns
                             Either `x` or `alt` based on `K.learning_phase`.
                         """
                         return in_train_phase(alt, x, training=training)


                     # NN OPERATIONS

                     def relu(x, alpha=0., max_value=None, threshold=0.):
                         """Rectified linear unit.

                         With default values, it returns element-wise `max(x, 0)`.

                         Otherwise, it follows:
                         `f(x) = max_value` for `x >= max_value`,
                         `f(x) = x` for `threshold <= x < max_value`,
                         `f(x) = alpha * (x - threshold)` otherwise.

                         # Arguments
                             x: A tensor or variable.
                             alpha: A scalar, slope of negative section (default=`0.`).
                             max_value: float. Saturation threshold.
                             threshold: float. Threshold value for thresholded activation.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """

                         if alpha != 0.:
                             if max_value is None and threshold == 0.:
                                 return tf.nn.leaky_relu(x, alpha=alpha)

                             if threshold != 0.:
                                 negative_part = tf.nn.relu(-x + threshold)
                             else:
                                 negative_part = tf.nn.relu(-x)

                         clip_max = max_value is not None

                         if threshold != 0:
                             # computes x for x > threshold else 0
                             x = x * tf.cast(tf.greater(x, threshold), floatx())
                         elif max_value == 6:
                             # if no threshold, then can use nn.relu6 native TF op for performance
                             x = tf.nn.relu6(x)
                             clip_max = False
                         else:
                             x = tf.nn.relu(x)

                         if clip_max:
                             max_value = _to_tensor(max_value, x.dtype.base_dtype)
                             zero = _to_tensor(0., x.dtype.base_dtype)
                             x = tf.clip_by_value(x, zero, max_value)

                         if alpha != 0:
                             alpha = _to_tensor(alpha, x.dtype.base_dtype)
                             x -= alpha * negative_part
                         return x


                     def elu(x, alpha=1.):
                         """Exponential linear unit.

                         # Arguments
                             x: A tensor or variable to compute the activation function for.
                             alpha: A scalar, slope of negative section.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         res = tf.nn.elu(x)
                         if alpha == 1:
                             return res
                         else:
                             return tf.where(x > 0, res, alpha * res)


                     def softmax(x, axis=-1):
                         """Softmax of a tensor.

                         # Arguments
                             x: A tensor or variable.
                             axis: The dimension softmax would be performed on.
                                 The default is -1 which indicates the last dimension.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.softmax(x, axis=axis)


                     def softplus(x):
                         """Softplus of a tensor.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.softplus(x)


                     def softsign(x):
                         """Softsign of a tensor.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.softsign(x)


                     def categorical_crossentropy(target, output, from_logits=False, axis=-1):
                         """Categorical crossentropy between an output tensor and a target tensor.

                         # Arguments
                             target: A tensor of the same shape as `output`.
                             output: A tensor resulting from a softmax
                                 (unless `from_logits` is True, in which
                                 case `output` is expected to be the logits).
                             from_logits: Boolean, whether `output` is the
                                 result of a softmax, or is a tensor of logits.
                             axis: Int specifying the channels axis. `axis=-1`
                                 corresponds to data format `channels_last`,
                                 and `axis=1` corresponds to data format
                                 `channels_first`.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: if `axis` is neither -1 nor one of
                                 the axes of `output`.
                         """
                         output_dimensions = list(range(len(output.get_shape())))
                         if axis != -1 and axis not in output_dimensions:
                             raise ValueError(
                                 '{}{}{}'.format(
                                     'Unexpected channels axis {}. '.format(axis),
                                     'Expected to be -1 or one of the axes of `output`, ',
                                     'which has {} dimensions.'.format(len(output.get_shape()))))
                         # Note: tf.nn.softmax_cross_entropy_with_logits
                         # expects logits, Keras expects probabilities.
                         if not from_logits:
                             # scale preds so that the class probas of each sample sum to 1
                             output /= tf.reduce_sum(output, axis, True)
                             # manual computation of crossentropy
                             _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
                             output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)
                             return - tf.reduce_sum(target * tf.log(output), axis)
                         else:
                             return tf.nn.softmax_cross_entropy_with_logits(labels=target,
                                                                            logits=output)


                     def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):
                         """Categorical crossentropy with integer targets.

                         # Arguments
                             target: An integer tensor.
                             output: A tensor resulting from a softmax
                                 (unless `from_logits` is True, in which
                                 case `output` is expected to be the logits).
                             from_logits: Boolean, whether `output` is the
                                 result of a softmax, or is a tensor of logits.
                             axis: Int specifying the channels axis. `axis=-1`
                                 corresponds to data format `channels_last`,
                                 and `axis=1` corresponds to data format
                                 `channels_first`.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: if `axis` is neither -1 nor one of
                                 the axes of `output`.
                         """
                         output_dimensions = list(range(len(output.get_shape())))
                         if axis != -1 and axis not in output_dimensions:
                             raise ValueError(
                                 '{}{}{}'.format(
                                     'Unexpected channels axis {}. '.format(axis),
                                     'Expected to be -1 or one of the axes of `output`, ',
                                     'which has {} dimensions.'.format(len(output.get_shape()))))
                         # If the channels are not in the last axis, move them to be there:
                         if axis != -1 and axis != output_dimensions[-1]:
                             permutation = output_dimensions[:axis] + output_dimensions[axis + 1:]
                             permutation += [axis]
                             output = tf.transpose(output, perm=permutation)

                         # Note: tf.nn.sparse_softmax_cross_entropy_with_logits
                         # expects logits, Keras expects probabilities.
                         if not from_logits:
                             _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
                             output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
                             output = tf.log(output)

                         output_shape = output.get_shape()
                         targets = cast(flatten(target), 'int64')
                         logits = tf.reshape(output, [-1, int(output_shape[-1])])
                         res = tf.nn.sparse_softmax_cross_entropy_with_logits(
                             labels=targets,
                             logits=logits)
                         if len(output_shape) >= 3:
                             # if our output includes timestep dimension
                             # or spatial dimensions we need to reshape
                             return tf.reshape(res, tf.shape(output)[:-1])
                         else:
                             return res


                     def binary_crossentropy(target, output, from_logits=False):
                         """Binary crossentropy between an output tensor and a target tensor.

                         # Arguments
                             target: A tensor with the same shape as `output`.
                             output: A tensor.
                             from_logits: Whether `output` is expected to be a logits tensor.
                                 By default, we consider that `output`
                                 encodes a probability distribution.

                         # Returns
                             A tensor.
                         """
                         # Note: tf.nn.sigmoid_cross_entropy_with_logits
                         # expects logits, Keras expects probabilities.
                         if not from_logits:
                             # transform back to logits
                             _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)
                             output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
                             output = tf.log(output / (1 - output))

                         return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,
                                                                        logits=output)


                     def sigmoid(x):
                         """Element-wise sigmoid.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.sigmoid(x)


                     def hard_sigmoid(x):
                         """Segment-wise linear approximation of sigmoid.

                         Faster than sigmoid.
                         Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.
                         In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         x = (0.2 * x) + 0.5
                         zero = _to_tensor(0., x.dtype.base_dtype)
                         one = _to_tensor(1., x.dtype.base_dtype)
                         x = tf.clip_by_value(x, zero, one)
                         return x


                     def tanh(x):
                         """Element-wise tanh.

                         # Arguments
                             x: A tensor or variable.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.tanh(x)


                     def dropout(x, level, noise_shape=None, seed=None):
                         """Sets entries in `x` to zero at random, while scaling the entire tensor.

                         # Arguments
                             x: tensor
                             level: fraction of the entries in the tensor
                                 that will be set to 0.
                             noise_shape: shape for randomly generated keep/drop flags,
                                 must be broadcastable to the shape of `x`
                             seed: random seed to ensure determinism.

                         # Returns
                             A tensor.
                         {{np_implementation}}
                         """
                         retain_prob = 1. - level
                         if seed is None:
                             seed = np.random.randint(10e6)
                         # the dummy 1. works around a TF bug
                         # (float32_ref vs. float32 incompatibility)
                         return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)


                     def l2_normalize(x, axis=None):
                         """Normalizes a tensor wrt the L2 norm alongside the specified axis.

                         # Arguments
                             x: Tensor or variable.
                             axis: axis along which to perform normalization.

                         # Returns
                             A tensor.

                         {{np_implementation}}
                         """
                         return tf.nn.l2_normalize(x, axis=axis)


                     def in_top_k(predictions, targets, k):
                         """Returns whether the `targets` are in the top `k` `predictions`.

                         # Arguments
                             predictions: A tensor of shape `(batch_size, classes)` and type `float32`.
                             targets: A 1D tensor of length `batch_size` and type `int32` or `int64`.
                             k: An `int`, number of top elements to consider.

                         # Returns
                             A 1D tensor of length `batch_size` and type `bool`.
                             `output[i]` is `True` if `predictions[i, targets[i]]` is within top-`k`
                             values of `predictions[i]`.
                         """
                         return tf.nn.in_top_k(predictions, targets, k)


                     # CONVOLUTIONS


                     def _preprocess_conv1d_input(x, data_format):
                         """Transpose and cast the input before the conv1d.

                         # Arguments
                             x: input tensor.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             A tensor.
                         """
                         # tensorflow doesn't support float64 for conv layer before 1.8.0
                         if (dtype(x) == 'float64' and
                                 StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
                             x = tf.cast(x, 'float32')
                         tf_data_format = 'NWC'  # to pass TF Conv2dNative operations
                         if data_format == 'channels_first':
                             if not _has_nchw_support():
                                 x = tf.transpose(x, (0, 2, 1))  # NCW -> NWC
                             else:
                                 tf_data_format = 'NCW'
                         return x, tf_data_format


                     def _preprocess_conv2d_input(x, data_format, force_transpose=False):
                         """Transpose and cast the input before the conv2d.

                         # Arguments
                             x: input tensor.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             force_transpose: boolean, whether force to transpose input from NCHW to NHWC
                                             if the `data_format` is `"channels_first"`.

                         # Returns
                             A tensor.
                         """
                         # tensorflow doesn't support float64 for conv layer before 1.8.0
                         if (dtype(x) == 'float64' and
                                 StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
                             x = tf.cast(x, 'float32')
                         tf_data_format = 'NHWC'
                         if data_format == 'channels_first':
                             if not _has_nchw_support() or force_transpose:
                                 x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC
                             else:
                                 tf_data_format = 'NCHW'
                         return x, tf_data_format


                     def _preprocess_conv3d_input(x, data_format):
                         """Transpose and cast the input before the conv3d.

                         # Arguments
                             x: input tensor.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             A tensor.
                         """
                         # tensorflow doesn't support float64 for conv layer before 1.8.0
                         if (dtype(x) == 'float64' and
                                 StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.8.0')):
                             x = tf.cast(x, 'float32')
                         tf_data_format = 'NDHWC'
                         if data_format == 'channels_first':
                             if not _has_nchw_support():
                                 x = tf.transpose(x, (0, 2, 3, 4, 1))
                             else:
                                 tf_data_format = 'NCDHW'
                         return x, tf_data_format


                     def _preprocess_padding(padding):
                         """Convert keras' padding to tensorflow's padding.

                         # Arguments
                             padding: string, `"same"` or `"valid"`.

                         # Returns
                             a string, `"SAME"` or `"VALID"`.

                         # Raises
                             ValueError: if `padding` is invalid.
                         """
                         if padding == 'same':
                             padding = 'SAME'
                         elif padding == 'valid':
                             padding = 'VALID'
                         else:
                             raise ValueError('Invalid padding: ' + str(padding))
                         return padding


                     def conv1d(x, kernel, strides=1, padding='valid',
                                data_format=None, dilation_rate=1):
                         """1D convolution.

                         # Arguments
                             x: Tensor or variable.
                             kernel: kernel tensor.
                             strides: stride integer.
                             padding: string, `"same"`, `"causal"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             dilation_rate: integer dilate rate.

                         # Returns
                             A tensor, result of 1D convolution.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         kernel_shape = kernel.get_shape().as_list()
                         if padding == 'causal':
                             if data_format != 'channels_last':
                                 raise ValueError('When using causal padding in `conv1d`, '
                                                  '`data_format` must be "channels_last" '
                                                  '(temporal data).')
                             # causal (dilated) convolution:
                             left_pad = dilation_rate * (kernel_shape[0] - 1)
                             x = temporal_padding(x, (left_pad, 0))
                             padding = 'valid'
                         padding = _preprocess_padding(padding)
                         x, tf_data_format = _preprocess_conv1d_input(x, data_format)
                         x = tf.nn.convolution(
                             input=x,
                             filter=kernel,
                             dilation_rate=(dilation_rate,),
                             strides=(strides,),
                             padding=padding,
                             data_format=tf_data_format)

                         if data_format == 'channels_first' and tf_data_format == 'NWC':
                             x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW
                         return x


                     def conv2d(x, kernel, strides=(1, 1), padding='valid',
                                data_format=None, dilation_rate=(1, 1)):
                         """2D convolution.

                         # Arguments
                             x: Tensor or variable.
                             kernel: kernel tensor.
                             strides: strides tuple.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                                 Whether to use Theano or TensorFlow/CNTK data format
                                 for inputs/kernels/outputs.
                             dilation_rate: tuple of 2 integers.

                         # Returns
                             A tensor, result of 2D convolution.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv2d_input(x, data_format)

                         padding = _preprocess_padding(padding)
                         x = tf.nn.convolution(
                             input=x,
                             filter=kernel,
                             dilation_rate=dilation_rate,
                             strides=strides,
                             padding=padding,
                             data_format=tf_data_format)

                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
                         return x


                     def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                                          padding='valid', data_format=None, dilation_rate=(1, 1)):
                         """2D deconvolution (i.e. transposed convolution).

                         # Arguments
                             x: Tensor or variable.
                             kernel: kernel tensor.
                             output_shape: 1D int tensor for the output shape.
                             strides: strides tuple.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                                 Whether to use Theano or TensorFlow/CNTK data format
                                 for inputs/kernels/outputs.
                             dilation_rate: tuple of 2 integers.

                         # Returns
                             A tensor, result of transposed 2D convolution.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)
                         if isinstance(output_shape, (tuple, list)):
                             output_shape = tf.stack(output_shape)

                         # tf.nn.atrous_conv2d_transpose input only supports NHWC format
                         if data_format == 'channels_first' and dilation_rate != (1, 1):
                             force_transpose = True
                         else:
                             force_transpose = False

                         x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)

                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             output_shape = (output_shape[0],
                                             output_shape[2],
                                             output_shape[3],
                                             output_shape[1])
                         if output_shape[0] is None:
                             output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])
                             output_shape = tf.stack(list(output_shape))

                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NHWC':
                             strides = (1,) + strides + (1,)
                         else:
                             strides = (1, 1) + strides

                         if dilation_rate == (1, 1):
                             x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
                                                        padding=padding,
                                                        data_format=tf_data_format)
                         else:
                             assert dilation_rate[0] == dilation_rate[1]
                             x = tf.nn.atrous_conv2d_transpose(
                                 x, kernel, output_shape, dilation_rate[0], padding)

                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
                         return x


                     def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
                                          padding='valid', data_format=None, dilation_rate=1):
                         """1D convolution with separable filters.

                         # Arguments
                             x: input tensor
                             depthwise_kernel: convolution kernel for the depthwise convolution.
                             pointwise_kernel: kernel for the 1x1 convolution.
                             strides: stride integer.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             dilation_rate: integer dilation rate.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)
                         if isinstance(strides, int):
                             strides = (strides,)
                         if isinstance(dilation_rate, int):
                             dilation_rate = (dilation_rate,)

                         x, tf_data_format = _preprocess_conv1d_input(x, data_format)
                         if tf_data_format == 'NWC':
                             tf_data_format = 'NHWC'
                         else:
                             tf_data_format = 'NCHW'
                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NHWC':
                             spatial_start_dim = 1
                             strides = (1,) + strides * 2 + (1,)
                         else:
                             spatial_start_dim = 2
                             strides = (1, 1) + strides * 2
                         x = tf.expand_dims(x, spatial_start_dim)
                         depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)
                         pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
                         dilation_rate = (1,) + dilation_rate

                         x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,
                                                    strides=strides,
                                                    padding=padding,
                                                    rate=dilation_rate,
                                                    data_format=tf_data_format)

                         x = tf.squeeze(x, [spatial_start_dim])

                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW

                         return x


                     def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),
                                          padding='valid', data_format=None, dilation_rate=(1, 1)):
                         """2D convolution with separable filters.

                         # Arguments
                             x: input tensor
                             depthwise_kernel: convolution kernel for the depthwise convolution.
                             pointwise_kernel: kernel for the 1x1 convolution.
                             strides: strides tuple (length 2).
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             dilation_rate: tuple of integers,
                                 dilation rates for the separable convolution.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv2d_input(x, data_format)
                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NHWC':
                             strides = (1,) + strides + (1,)
                         else:
                             strides = (1, 1) + strides

                         x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,
                                                    strides=strides,
                                                    padding=padding,
                                                    rate=dilation_rate,
                                                    data_format=tf_data_format)
                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
                         return x


                     def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',
                                          data_format=None, dilation_rate=(1, 1)):
                         """2D convolution with separable filters.

                         # Arguments
                             x: input tensor
                             depthwise_kernel: convolution kernel for the depthwise convolution.
                             strides: strides tuple (length 2).
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             dilation_rate: tuple of integers,
                                 dilation rates for the separable convolution.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv2d_input(x, data_format)
                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NHWC':
                             strides = (1,) + strides + (1,)
                         else:
                             strides = (1, 1) + strides

                         x = tf.nn.depthwise_conv2d(x, depthwise_kernel,
                                                    strides=strides,
                                                    padding=padding,
                                                    rate=dilation_rate,
                                                    data_format=tf_data_format)
                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
                         return x


                     def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',
                                data_format=None, dilation_rate=(1, 1, 1)):
                         """3D convolution.

                         # Arguments
                             x: Tensor or variable.
                             kernel: kernel tensor.
                             strides: strides tuple.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                                 Whether to use Theano or TensorFlow/CNTK data format
                                 for inputs/kernels/outputs.
                             dilation_rate: tuple of 3 integers.

                         # Returns
                             A tensor, result of 3D convolution.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv3d_input(x, data_format)
                         padding = _preprocess_padding(padding)
                         x = tf.nn.convolution(
                             input=x,
                             filter=kernel,
                             dilation_rate=dilation_rate,
                             strides=strides,
                             padding=padding,
                             data_format=tf_data_format)
                         if data_format == 'channels_first' and tf_data_format == 'NDHWC':
                             x = tf.transpose(x, (0, 4, 1, 2, 3))
                         return x


                     def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),
                                          padding='valid', data_format=None):
                         """3D deconvolution (i.e. transposed convolution).

                         # Arguments
                             x: input tensor.
                             kernel: kernel tensor.
                             output_shape: 1D int tensor for the output shape.
                             strides: strides tuple.
                             padding: string, "same" or "valid".
                             data_format: string, `"channels_last"` or `"channels_first"`.
                                 Whether to use Theano or TensorFlow/CNTK data format
                                 for inputs/kernels/outputs.

                         # Returns
                             A tensor, result of transposed 3D convolution.

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)
                         if isinstance(output_shape, (tuple, list)):
                             output_shape = tf.stack(output_shape)

                         x, tf_data_format = _preprocess_conv3d_input(x, data_format)

                         if data_format == 'channels_first' and tf_data_format == 'NDHWC':
                             output_shape = (output_shape[0],
                                             output_shape[2],
                                             output_shape[3],
                                             output_shape[4],
                                             output_shape[1])
                         if output_shape[0] is None:
                             output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])
                             output_shape = tf.stack(list(output_shape))

                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NDHWC':
                             strides = (1,) + strides + (1,)
                         else:
                             strides = (1, 1) + strides

                         x = tf.nn.conv3d_transpose(x, kernel, output_shape, strides,
                                                    padding=padding,
                                                    data_format=tf_data_format)
                         if data_format == 'channels_first' and tf_data_format == 'NDHWC':
                             x = tf.transpose(x, (0, 4, 1, 2, 3))
                         return x


                     def pool2d(x, pool_size, strides=(1, 1),
                                padding='valid', data_format=None,
                                pool_mode='max'):
                         """2D Pooling.

                         # Arguments
                             x: Tensor or variable.
                             pool_size: tuple of 2 integers.
                             strides: tuple of 2 integers.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             pool_mode: string, `"max"` or `"avg"`.

                         # Returns
                             A tensor, result of 2D pooling.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.
                             ValueError: if `pool_mode` is neither `"max"` or `"avg"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv2d_input(x, data_format)
                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NHWC':
                             strides = (1,) + strides + (1,)
                             pool_size = (1,) + pool_size + (1,)
                         else:
                             strides = (1, 1) + strides
                             pool_size = (1, 1) + pool_size

                         if pool_mode == 'max':
                             x = tf.nn.max_pool(x, pool_size, strides,
                                                padding=padding,
                                                data_format=tf_data_format)
                         elif pool_mode == 'avg':
                             x = tf.nn.avg_pool(x, pool_size, strides,
                                                padding=padding,
                                                data_format=tf_data_format)
                         else:
                             raise ValueError('Invalid pool_mode: ' + str(pool_mode))

                         if data_format == 'channels_first' and tf_data_format == 'NHWC':
                             x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
                         return x


                     def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',
                                data_format=None, pool_mode='max'):
                         """3D Pooling.

                         # Arguments
                             x: Tensor or variable.
                             pool_size: tuple of 3 integers.
                             strides: tuple of 3 integers.
                             padding: string, `"same"` or `"valid"`.
                             data_format: string, `"channels_last"` or `"channels_first"`.
                             pool_mode: string, `"max"` or `"avg"`.

                         # Returns
                             A tensor, result of 3D pooling.

                         # Raises
                             ValueError: if `data_format` is
                             neither `"channels_last"` or `"channels_first"`.
                             ValueError: if `pool_mode` is neither `"max"` or `"avg"`.
                         """
                         data_format = normalize_data_format(data_format)

                         x, tf_data_format = _preprocess_conv3d_input(x, data_format)
                         padding = _preprocess_padding(padding)
                         if tf_data_format == 'NDHWC':
                             strides = (1,) + strides + (1,)
                             pool_size = (1,) + pool_size + (1,)
                         else:
                             strides = (1, 1) + strides
                             pool_size = (1, 1) + pool_size

                         if pool_mode == 'max':
                             x = tf.nn.max_pool3d(x, pool_size, strides,
                                                  padding=padding,
                                                  data_format=tf_data_format)
                         elif pool_mode == 'avg':
                             x = tf.nn.avg_pool3d(x, pool_size, strides,
                                                  padding=padding,
                                                  data_format=tf_data_format)
                         else:
                             raise ValueError('Invalid pool_mode: ' + str(pool_mode))

                         if data_format == 'channels_first' and tf_data_format == 'NDHWC':
                             x = tf.transpose(x, (0, 4, 1, 2, 3))
                         return x


                     def bias_add(x, bias, data_format=None):
                         """Adds a bias vector to a tensor.

                         # Arguments
                             x: Tensor or variable.
                             bias: Bias tensor to add.
                             data_format: string, `"channels_last"` or `"channels_first"`.

                         # Returns
                             Output tensor.

                         # Raises
                             ValueError: In one of the two cases below:
                                         1. invalid `data_format` argument.
                                         2. invalid bias shape.
                                            the bias should be either a vector or
                                            a tensor with ndim(x) - 1 dimension
                         {{np_implementation}}
                         """
                         data_format = normalize_data_format(data_format)
                         bias_shape = int_shape(bias)
                         if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:
                             raise ValueError('Unexpected bias dimensions %d, '
                                              'expect to be 1 or %d dimensions'
                                              % (len(bias_shape), ndim(x)))
                         if ndim(x) == 5:
                             if len(bias_shape) == 1:
                                 new_shape = (1, 1, 1, 1, bias_shape[0])
                             else:
                                 new_shape = (1,) + bias_shape
                             new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1, 2, 3))
                             x += reshape(bias, new_shape)
                         elif ndim(x) == 4:
                             if data_format == 'channels_first':
                                 if len(bias_shape) == 1:
                                     if _has_nchw_support():
                                         x = tf.nn.bias_add(x, bias,
                                                            data_format='NCHW')
                                     else:
                                         x += reshape(bias, (1, bias_shape[0], 1, 1))
                                 else:
                                     x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])
                             elif data_format == 'channels_last':
                                 if len(bias_shape) == 1:
                                     x = tf.nn.bias_add(x, bias,
                                                        data_format='NHWC')
                                 else:
                                     x += reshape(bias, (1,) + bias_shape)
                         elif ndim(x) == 3:
                             if len(bias_shape) == 1:
                                 new_shape = (1, 1, bias_shape[0])
                             else:
                                 new_shape = (1,) + bias_shape
                             new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1,))
                             x += reshape(bias, new_shape)
                         else:
                             x = tf.nn.bias_add(x, bias)
                         return x


                     # RANDOMNESS

                     def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):
                         """Returns a tensor with normal distribution of values.

                         # Arguments
                             shape: A tuple of integers, the shape of tensor to create.
                             mean: A float, mean of the normal distribution to draw samples.
                             stddev: A float, standard deviation of the normal distribution
                                 to draw samples.
                             dtype: String, dtype of returned tensor.
                             seed: Integer, random seed.

                         # Returns
                             A tensor.
                         """
                         if dtype is None:
                             dtype = floatx()
                         if seed is None:
                             seed = np.random.randint(10e6)
                         return tf.random_normal(shape, mean=mean, stddev=stddev,
                                                 dtype=dtype, seed=seed)


                     def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):
                         """Returns a tensor with uniform distribution of values.

                         # Arguments
                             shape: A tuple of integers, the shape of tensor to create.
                             minval: A float, lower boundary of the uniform distribution
                                 to draw samples.
                             maxval: A float, upper boundary of the uniform distribution
                                 to draw samples.
                             dtype: String, dtype of returned tensor.
                             seed: Integer, random seed.

                         # Returns
                             A tensor.
                         """
                         if dtype is None:
                             dtype = floatx()
                         if seed is None:
                             seed = np.random.randint(10e6)
                         return tf.random_uniform(shape, minval=minval, maxval=maxval,
                                                  dtype=dtype, seed=seed)


                     def random_binomial(shape, p=0.0, dtype=None, seed=None):
                         """Returns a tensor with random binomial distribution of values.

                         # Arguments
                             shape: A tuple of integers, the shape of tensor to create.
                             p: A float, `0. <= p <= 1`, probability of binomial distribution.
                             dtype: String, dtype of returned tensor.
                             seed: Integer, random seed.

                         # Returns
                             A tensor.
                         """
                         if dtype is None:
                             dtype = floatx()
                         if seed is None:
                             seed = np.random.randint(10e6)
                         return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,
                                         tf.ones(shape, dtype=dtype),
                                         tf.zeros(shape, dtype=dtype))


                     def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):
                         """Returns a tensor with truncated random normal distribution of values.

                         The generated values follow a normal distribution
                         with specified mean and standard deviation,
                         except that values whose magnitude is more than
                         two standard deviations from the mean are dropped and re-picked.

                         # Arguments
                             shape: A tuple of integers, the shape of tensor to create.
                             mean: Mean of the values.
                             stddev: Standard deviation of the values.
                             dtype: String, dtype of returned tensor.
                             seed: Integer, random seed.

                         # Returns
                             A tensor.
                         """
                         if dtype is None:
                             dtype = floatx()
                         if seed is None:
                             seed = np.random.randint(10e6)
                         return tf.truncated_normal(shape, mean, stddev, dtype=dtype, seed=seed)


                     # CTC
                     # TensorFlow has a native implementation, but it uses sparse tensors
                     # and therefore requires a wrapper for Keras. The functions below convert
                     # dense to sparse tensors and also wraps up the beam search code that is
                     # in TensorFlow's CTC implementation


                     def ctc_label_dense_to_sparse(labels, label_lengths):
                         """Converts CTC labels from dense to sparse.

                         # Arguments
                             labels: dense CTC labels.
                             label_lengths: length of the labels.

                         # Returns
                             A sparse tensor representation of the labels.
                         """
                         label_shape = tf.shape(labels)
                         num_batches_tns = tf.stack([label_shape[0]])
                         max_num_labels_tns = tf.stack([label_shape[1]])

                         def range_less_than(_, current_input):
                             return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(
                                 max_num_labels_tns, current_input)

                         init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)
                         dense_mask = functional_ops.scan(range_less_than, label_lengths,
                                                          initializer=init, parallel_iterations=1)
                         dense_mask = dense_mask[:, 0, :]

                         label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),
                                                  label_shape)
                         label_ind = tf.boolean_mask(label_array, dense_mask)

                         tmp = tf.tile(tf.range(label_shape[0]), max_num_labels_tns)
                         batch_array = tf.transpose(tf.reshape(tmp, reverse(label_shape, 0)))
                         batch_ind = tf.boolean_mask(batch_array, dense_mask)

                         indices = concatenate([batch_ind, label_ind], axis=0)
                         indices = tf.transpose(tf.reshape(indices, [2, -1]))

                         vals_sparse = tf.gather_nd(labels, indices)

                         indices = tf.cast(indices, tf.int64)
                         label_shape = tf.cast(label_shape, tf.int64)
                         return tf.SparseTensor(indices, vals_sparse, label_shape)


                     def ctc_batch_cost(y_true, y_pred, input_length, label_length):
                         """Runs CTC loss algorithm on each batch element.

                         # Arguments
                             y_true: tensor `(samples, max_string_length)`
                                 containing the truth labels.
                             y_pred: tensor `(samples, time_steps, num_categories)`
                                 containing the prediction, or output of the softmax.
                             input_length: tensor `(samples, 1)` containing the sequence length for
                                 each batch item in `y_pred`.
                             label_length: tensor `(samples, 1)` containing the sequence length for
                                 each batch item in `y_true`.

                         # Returns
                             Tensor with shape (samples,1) containing the
                                 CTC loss of each element.
                         """
                         label_length = tf.cast(tf.squeeze(label_length, axis=-1), tf.int32)
                         input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)
                         sparse_labels = tf.cast(
                             ctc_label_dense_to_sparse(y_true, label_length), tf.int32)
                         y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
                         return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,
                                                            labels=sparse_labels,
                                                            sequence_length=input_length), 1)


                     def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,
                                    top_paths=1, merge_repeated=False):
                         """Decodes the output of a softmax.

                         Can use either greedy search (also known as best path)
                         or a constrained dictionary search.

                         # Arguments
                             y_pred: tensor `(samples, time_steps, num_categories)`
                                 containing the prediction, or output of the softmax.
                             input_length: tensor `(samples, )` containing the sequence length for
                                 each batch item in `y_pred`.
                             greedy: perform much faster best-path search if `True`.
                                 This does not use a dictionary.
                             beam_width: if `greedy` is `False`: a beam search decoder will be used
                                 with a beam of this width.
                             top_paths: if `greedy` is `False`,
                                 how many of the most probable paths will be returned.
                             merge_repeated: if `greedy` is `False`,
                                 merge repeated classes in the output beams.

                         # Returns
                             Tuple:
                                 List: if `greedy` is `True`, returns a list of one element that
                                     contains the decoded sequence.
                                     If `False`, returns the `top_paths` most probable
                                     decoded sequences.
                                     Important: blank labels are returned as `-1`.
                                 Tensor `(top_paths, )` that contains
                                     the log probability of each decoded sequence.
                         """
                         y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
                         input_length = tf.cast(input_length, tf.int32)

                         if greedy:
                             (decoded, log_prob) = ctc.ctc_greedy_decoder(
                                 inputs=y_pred,
                                 sequence_length=input_length)
                         else:
                             (decoded, log_prob) = ctc.ctc_beam_search_decoder(
                                 inputs=y_pred,
                                 sequence_length=input_length, beam_width=beam_width,
                                 top_paths=top_paths, merge_repeated=merge_repeated)

                         decoded_dense = []
                         for st in decoded:
                             dense_tensor = tf.sparse.to_dense(st, default_value=-1)
                             decoded_dense.append(dense_tensor)
                         return (decoded_dense, log_prob)


                     # HIGH ORDER FUNCTIONS

                     def map_fn(fn, elems, name=None, dtype=None):
                         """Map the function fn over the elements elems and return the outputs.

                         # Arguments
                             fn: Callable that will be called upon each element in elems
                             elems: tensor
                             name: A string name for the map node in the graph
                             dtype: Output data type.

                         # Returns
                             Tensor with dtype `dtype`.
                         """
                         return tf.map_fn(fn, elems, name=name, dtype=dtype)


                     def foldl(fn, elems, initializer=None, name=None):
                         """Reduce elems using fn to combine them from left to right.

                         # Arguments
                             fn: Callable that will be called upon each element in elems and an
                                 accumulator, for instance `lambda acc, x: acc + x`
                             elems: tensor
                             initializer: The first value used (`elems[0]` in case of None)
                             name: A string name for the foldl node in the graph

                         # Returns
                             Tensor with same type and shape as `initializer`.
                         """
                         return tf.foldl(fn, elems, initializer=initializer, name=name)


                     def foldr(fn, elems, initializer=None, name=None):
                         """Reduce elems using fn to combine them from right to left.

                         # Arguments
                             fn: Callable that will be called upon each element in elems and an
                                 accumulator, for instance `lambda acc, x: acc + x`
                             elems: tensor
                             initializer: The first value used (`elems[-1]` in case of None)
                             name: A string name for the foldr node in the graph

                         # Returns
                             Tensor with same type and shape as `initializer`.
                         """
                         return tf.foldr(fn, elems, initializer=initializer, name=name)


                     def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):
                         """Apply 1D conv with un-shared weights.

                         # Arguments
                             inputs: 3D tensor with shape: (batch_size, steps, input_dim)
                             kernel: the unshared weight for convolution,
                                     with shape (output_length, feature_dim, filters)
                             kernel_size: a tuple of a single integer,
                                          specifying the length of the 1D convolution window
                             strides: a tuple of a single integer,
                                      specifying the stride length of the convolution
                             data_format: the data format, channels_first or channels_last

                         # Returns
                             the tensor after 1d conv with un-shared weights,
                             with shape (batch_size, output_length, filters)

                         # Raises
                             ValueError: If `data_format` is neither
                                 `"channels_last"` nor `"channels_first"`.
                         """
                         data_format = normalize_data_format(data_format)

                         stride = strides[0]
                         kernel_shape = int_shape(kernel)
                         output_length, feature_dim, filters = kernel_shape

                         xs = []
                         for i in range(output_length):
                             slice_length = py_slice(i * stride,
                                                     i * stride + kernel_size[0])
                             xs.append(reshape(inputs[:, slice_length, :],
                                               (1, -1, feature_dim)))
                         x_aggregate = concatenate(xs, axis=0)
                         # Shape: `(output_length, batch_size, filters)`.
                         output = batch_dot(x_aggregate, kernel)
                         return permute_dimensions(output, (1, 0, 2))


                     def local_conv2d(inputs,
                                      kernel,
                                      kernel_size,
                                      strides,
                                      output_shape,
                                      data_format=None):
                         """Apply 2D conv with un-shared weights.

                         # Arguments
                             inputs: 4D tensor with shape:
                                     (batch_size, filters, new_rows, new_cols)
                                     if data_format='channels_first'
                                     or 4D tensor with shape:
                                     (batch_size, new_rows, new_cols, filters)
                                     if data_format='channels_last'.
                             kernel: the unshared weight for convolution,
                                     with shape (output_items, feature_dim, filters)
                             kernel_size: a tuple of 2 integers, specifying the
                                          width and height of the 2D convolution window.
                             strides: a tuple of 2 integers, specifying the strides
                                      of the convolution along the width and height.
                             output_shape: a tuple with (output_row, output_col)
                             data_format: the data format, channels_first or channels_last

                         # Returns
                             A 4d tensor with shape:
                             (batch_size, filters, new_rows, new_cols)
                             if data_format='channels_first'
                             or 4D tensor with shape:
                             (batch_size, new_rows, new_cols, filters)
                             if data_format='channels_last'.

                         # Raises
                             ValueError: if `data_format` is neither
                                         `channels_last` or `channels_first`.
                         """
                         data_format = normalize_data_format(data_format)

                         stride_row, stride_col = strides
                         output_row, output_col = output_shape
                         kernel_shape = int_shape(kernel)
                         _, feature_dim, filters = kernel_shape

                         xs = []
                         for i in range(output_row):
                             for j in range(output_col):
                                 slice_row = py_slice(i * stride_row,
                                                      i * stride_row + kernel_size[0])
                                 slice_col = py_slice(j * stride_col,
                                                      j * stride_col + kernel_size[1])
                                 if data_format == 'channels_first':
                                     xs.append(reshape(inputs[:, :, slice_row, slice_col],
                                                       (1, -1, feature_dim)))
                                 else:
                                     xs.append(reshape(inputs[:, slice_row, slice_col, :],
                                                       (1, -1, feature_dim)))

                         x_aggregate = concatenate(xs, axis=0)
                         output = batch_dot(x_aggregate, kernel)
                         output = reshape(output,
                                          (output_row, output_col, -1, filters))

                         if data_format == 'channels_first':
                             output = permute_dimensions(output, (2, 3, 0, 1))
                         else:
                             output = permute_dimensions(output, (2, 0, 1, 3))
                         return output
